---
aliases: [XGB, Extreme Gradient Boosting]
tags: [ç®—æ³•, æœºå™¨å­¦ä¹ , ç›‘ç£å­¦ä¹ , åˆ†ç±»/å›å½’, é›†æˆå­¦ä¹ , Boosting]
difficulty: â­â­â­â­
math_enabled: true
---

# XGBoostï¼ˆeXtreme Gradient Boostingï¼‰

## ğŸ’¡ æ ¸å¿ƒç›´è§‰ (Intuition)

### ä¸€å¥è¯è§£é‡Š

**XGBoost = æ¢¯åº¦æå‡æ ‘ + äºŒé˜¶å¯¼æ•° + æ­£åˆ™åŒ– + æåº¦ä¼˜åŒ–**

å¦‚æœä½ å·²ç»äº†è§£å†³ç­–æ ‘å’Œ Boostingï¼Œå¯ä»¥è¿™æ ·ç†è§£ XGBoostï¼š

- **å†³ç­–æ ‘**ï¼šä¸€å † if-else è§„åˆ™ï¼Œé€æ­¥åˆ†è£‚ç‰¹å¾ç©ºé—´
- **Boosting**ï¼šå¤šä¸ªå¼±å­¦ä¹ å™¨ï¼Œåè€…å­¦ä¹ å‰è€…çš„é”™è¯¯
- **æ¢¯åº¦æå‡**ï¼šç”¨æŸå¤±å‡½æ•°çš„æ¢¯åº¦æ¥æŒ‡å¯¼æ ‘çš„ç”Ÿé•¿
- **XGBoost**ï¼šæ¢¯åº¦æå‡ + äºŒé˜¶æ³°å‹’å±•å¼€ + L1/L2æ­£åˆ™åŒ– + åˆ—æŠ½æ · + è¡ŒæŠ½æ · + è¶…çº§å¿«é€Ÿçš„å·¥ç¨‹å®ç°

### Killer Featureï¼ˆæ€æ‰‹é”ï¼‰

> [!ABSTRACT] æ ¸å¿ƒä¼˜åŠ¿
> XGBoost åœ¨ç»“æ„åŒ–è¡¨æ ¼æ•°æ®çš„ç«èµ›å’Œç”Ÿäº§ç¯å¢ƒä¸­æ— æ•Œâ€”â€”å®ƒç»“åˆäº† **ç²¾åº¦ï¼ˆäºŒé˜¶æ¢¯åº¦ï¼‰ã€é€Ÿåº¦ï¼ˆåˆ—çº§å¹¶è¡Œï¼‰ã€å¥å£®æ€§ï¼ˆç¼ºå¤±å€¼å¤„ç†ï¼‰** ä¸‰å¤§ä¼˜åŠ¿ã€‚Kaggle ç«èµ› 98% çš„è·å¥–æ–¹æ¡ˆéƒ½ç”¨äº† XGBoostã€‚

### å‡ ä½•ç›´è§‰

```
æ¢¯åº¦æå‡çš„è¿‡ç¨‹ï¼ˆä¸€ç»´ç¤ºæ„å›¾ï¼‰ï¼š

å›å½’ç›®æ ‡ï¼šæ‹Ÿåˆ sin(x) å‡½æ•°
      â†‘ y
      â”‚     *
      â”‚    * *
      â”‚   *   *
      â”‚  *     *
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x

æ­¥éª¤1ï¼šç¬¬ä¸€æ£µæ ‘ Fâ‚(x) â‰ˆ å¹³å‡å€¼
      ç›´çº¿ä¼°è®¡ï¼Œæ®‹å·® = çœŸå® - é¢„æµ‹

æ­¥éª¤2ï¼šç¬¬äºŒæ£µæ ‘ Fâ‚‚(x) = Fâ‚(x) + Î»Â·treeâ‚‚(x)
      æ ‘â‚‚ æ‹Ÿåˆ Fâ‚ çš„æ®‹å·®ï¼ŒÎ»æ˜¯å­¦ä¹ ç‡

æ­¥éª¤3ï¼šç¬¬ä¸‰æ£µæ ‘ Fâ‚ƒ(x) = Fâ‚‚(x) + Î»Â·treeâ‚ƒ(x)
      ä¾æ¬¡è¿­ä»£...

100æ­¥åï¼šF(x) = Fâ‚ + Î»Â·treeâ‚‚ + Î»Â·treeâ‚ƒ + ... + Î»Â·treeâ‚â‚€â‚€
        â‰ˆ å®Œç¾æ‹Ÿåˆ sin(x)

XGBoost çš„åˆ›æ–°ï¼š
  ä¼ ç»Ÿ Boostingï¼šF_new = F_old + Î»Â·NewTreeï¼ˆåªç”¨ä¸€é˜¶æ¢¯åº¦ï¼‰
  XGBoostï¼šF_new = F_old + Î»Â·NewTreeï¼ˆç”¨ä¸€é˜¶+äºŒé˜¶æ¢¯åº¦ï¼‰
           äºŒé˜¶æ¢¯åº¦ï¼ˆHessianï¼‰æä¾›æ›²ç‡ä¿¡æ¯ï¼Œæ›´ç²¾å‡†æ›´å¿«é€Ÿæ”¶æ•›
```

---

## ğŸ“ æ•°å­¦åŸç† (The Math)

### 2.1 ä¼˜åŒ–ç›®æ ‡ä¸æŸå¤±å‡½æ•°

Boosting æ¨¡å‹å¯ä»¥å†™æˆï¼š

$$\hat{y}_i = \sum_{k=1}^{K} f_k(x_i)$$

å…¶ä¸­ $f_k$ æ˜¯ç¬¬ $k$ æ£µæ ‘ï¼Œ$K$ æ˜¯æ€»æ ‘æ•°ã€‚

ç›®æ ‡å‡½æ•°ï¼š

$$L(\Theta) = \sum_{i=1}^{n} l(y_i, \hat{y}_i^{(t)}) + \sum_{k=1}^{K} \Omega(f_k)$$

åˆ†è§£ï¼š
- **ç¬¬ä¸€é¡¹**ï¼šé¢„æµ‹è¯¯å·®ï¼ˆMSEã€äº¤å‰ç†µç­‰ï¼‰
- **ç¬¬äºŒé¡¹**ï¼šæ­£åˆ™åŒ–é¡¹

$$\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T} w_j^2$$

å…¶ä¸­ï¼š
- $T$ï¼šå¶å­èŠ‚ç‚¹æ•°é‡
- $\gamma$ï¼šå¤æ‚åº¦æƒ©ç½šï¼ˆæ¯å¤šä¸€ä¸ªå¶å­ï¼ŒæŸå¤±å¢åŠ  $\gamma$ï¼‰
- $\lambda$ï¼šå¶å­æƒé‡çš„ L2 æ­£åˆ™åŒ–
- $w_j$ï¼šç¬¬ $j$ ä¸ªå¶å­çš„é¢„æµ‹å€¼

> [!TIP] ç†è§£æ­£åˆ™åŒ–
> - $\gamma$ æ§åˆ¶æ ‘çš„æ·±åº¦ï¼ˆå¤§ $\gamma$ â†’ æ ‘æ›´æµ…ï¼Œæ¬ æ‹Ÿåˆï¼‰
> - $\lambda$ æ§åˆ¶å¶å­æƒé‡çš„å¤§å°ï¼ˆå¤§ $\lambda$ â†’ æƒé‡æ›´å°ï¼Œæ›´ä¿å®ˆï¼‰
> - è¿™ä¸¤é¡¹åˆèµ·æ¥é˜²æ­¢è¿‡æ‹Ÿåˆ

### 2.2 è´ªå¿ƒæ ‘æ„å»ºï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼šäºŒé˜¶æ³°å‹’å±•å¼€ï¼‰

åœ¨ç¬¬ $t$ è½®è¿­ä»£ï¼Œæˆ‘ä»¬å·²æœ‰æ¨¡å‹ï¼š
$$\hat{y}_i^{(t-1)} = \sum_{k=1}^{t-1} f_k(x_i)$$

ç›®æ ‡æ˜¯æ·»åŠ æ–°æ ‘ $f_t$ æ¥æœ€å°åŒ–ï¼š
$$L^{(t)} = \sum_{i=1}^{n} l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t)$$

**XGBoost çš„å…³é”®ï¼šäºŒé˜¶æ³°å‹’å±•å¼€**

å°†æŸå¤±å‡½æ•°åœ¨ $\hat{y}_i^{(t-1)}$ å¤„å±•å¼€ï¼š

$$L^{(t)} \approx \sum_{i=1}^{n} \left[ l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t(x_i)^2 \right] + \Omega(f_t)$$

å…¶ä¸­ï¼š
- $g_i = \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial \hat{y}_i^{(t-1)}}$ ï¼šä¸€é˜¶å¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰
- $h_i = \frac{\partial^2 l(y_i, \hat{y}_i^{(t-1)})}{\partial (\hat{y}_i^{(t-1)})^2}$ ï¼šäºŒé˜¶å¯¼æ•°ï¼ˆHessianï¼‰

å»æ‰å¸¸æ•°é¡¹ $l(y_i, \hat{y}_i^{(t-1)})$ï¼š

$$\tilde{L}^{(t)} = \sum_{i=1}^{n} \left[ g_i f_t(x_i) + \frac{1}{2} h_i f_t(x_i)^2 \right] + \Omega(f_t)$$

### 2.3 å¶å­æƒé‡è®¡ç®—

å‡è®¾æ ‘ $f_t$ çš„ç»“æ„å·²å®šï¼ˆå³åˆ†è£‚ç‚¹å·²ç¡®å®šï¼‰ï¼Œè®¾ï¼š
- $I_j = \{i: x_i \text{ è½åœ¨å¶å­ } j\}$ï¼šå¶å­ $j$ ä¸­çš„æ ·æœ¬é›†åˆ
- $w_j$ï¼šå¶å­ $j$ çš„æƒé‡ï¼ˆé¢„æµ‹å€¼ï¼‰

åˆ™ï¼š
$$\tilde{L}^{(t)} = \sum_{j=1}^{T} \left[ \left(\sum_{i \in I_j} g_i\right) w_j + \frac{1}{2}\left(\sum_{i \in I_j} h_i + \lambda\right) w_j^2 \right] + \gamma T$$

å¯¹ $w_j$ æ±‚å¯¼ä»¤å…¶ä¸º 0ï¼š

$$\frac{\partial}{\partial w_j} = \sum_{i \in I_j} g_i + \left(\sum_{i \in I_j} h_i + \lambda\right) w_j = 0$$

$$w_j^* = -\frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}$$

**ä»£å…¥æœ€ä¼˜æƒé‡ï¼Œå¾—åˆ°è¯¥æ ‘çš„æœ€ä½æŸå¤±**ï¼š

$$\tilde{L}^{(t)} = -\frac{1}{2} \sum_{j=1}^{T} \frac{(\sum_{i \in I_j} g_i)^2}{\sum_{i \in I_j} h_i + \lambda} + \gamma T$$

### 2.4 åˆ†è£‚å‡†åˆ™ï¼ˆGain è®¡ç®—ï¼‰

å½“è€ƒè™‘åœ¨å¶å­ $j$ å¤„ä»¥ç‰¹å¾ $d$ çš„å€¼ $v$ åˆ†è£‚æ—¶ï¼š

**åˆ†è£‚å‰çš„æŸå¤±**ï¼ˆå·¦å³åˆå¹¶ï¼‰ï¼š
$$L_{before} = -\frac{1}{2} \frac{(\sum_{i \in I_j} g_i)^2}{\sum_{i \in I_j} h_i + \lambda}$$

**åˆ†è£‚åçš„æŸå¤±**ï¼ˆå·¦å³åˆ†å¼€ï¼‰ï¼š
$$L_{after} = -\frac{1}{2} \frac{(\sum_{i \in I_L} g_i)^2}{\sum_{i \in I_L} h_i + \lambda} -\frac{1}{2} \frac{(\sum_{i \in I_R} g_i)^2}{\sum_{i \in I_R} h_i + \lambda}$$

**Gainï¼ˆåˆ†è£‚æ”¶ç›Šï¼‰**ï¼š
$$\text{Gain} = L_{before} - L_{after} - \gamma$$

$\gamma$ æ˜¯æ–°å¶å­èŠ‚ç‚¹çš„å¤æ‚åº¦æƒ©ç½šã€‚XGBoost ä¼šé€‰æ‹©ä½¿ Gain æœ€å¤§çš„åˆ†è£‚ã€‚

### 2.5 ç¼ºå¤±å€¼å¤„ç†

XGBoost ä¸æ˜¯åˆ é™¤æˆ–å¡«è¡¥ç¼ºå¤±å€¼ï¼Œè€Œæ˜¯**å­¦ä¹ ç¼ºå¤±å€¼çš„æœ€ä¼˜æ–¹å‘**ã€‚

å¯¹æ¯ä¸ªç‰¹å¾ï¼Œåœ¨åˆ†è£‚æ—¶ï¼Œç¼ºå¤±å€¼çš„æ ·æœ¬å¯ä»¥**å…¨éƒ¨é€å¾€å·¦å­æ ‘**æˆ–**å…¨éƒ¨é€å¾€å³å­æ ‘**ï¼Œç®—æ³•ä¼šé€‰æ‹©ä½¿ Gain æ›´å¤§çš„æ–¹å‘ã€‚

```python
# ä¼ªä»£ç ï¼šç¼ºå¤±å€¼å¤„ç†
for split_feature, split_value in candidates:
    # æ–¹æ¡ˆ1ï¼šç¼ºå¤±å€¼ â†’ å·¦å­æ ‘
    left_1 = samples[feature < split_value] + samples[feature == NaN]
    right_1 = samples[feature >= split_value]
    gain_1 = calculate_gain(left_1, right_1)

    # æ–¹æ¡ˆ2ï¼šç¼ºå¤±å€¼ â†’ å³å­æ ‘
    left_2 = samples[feature < split_value]
    right_2 = samples[feature >= split_value] + samples[feature == NaN]
    gain_2 = calculate_gain(left_2, right_2)

    # é€‰æ‹©æ›´å¥½çš„æ–¹å‘
    if gain_1 > gain_2:
        best_direction[split_feature] = 'left'
    else:
        best_direction[split_feature] = 'right'
```

---

## ğŸ’» ç®—æ³•å®ç° (Implementation)

### 3.1 ä¼ªä»£ç 

```
Algorithm: XGBoost Training
Input:
  - Training data: {(x_i, y_i)}
  - Loss function: l(y, Å·)
  - Number of rounds: num_round
  - Learning rate: Î·

Initialize: fâ‚€ â† åˆå§‹æ¨¡å‹ï¼ˆé€šå¸¸ä¸º 0ï¼‰
F â† [fâ‚€]

for t = 1 to num_round:
    # æ­¥éª¤1ï¼šè®¡ç®—æ¢¯åº¦å’ŒHessian
    for i = 1 to n:
        Å·áµ¢ â† F(xáµ¢)  # å½“å‰é¢„æµ‹
        gáµ¢ â† âˆ‚l(yáµ¢, Å·áµ¢) / âˆ‚Å·áµ¢  # ä¸€é˜¶å¯¼æ•°
        háµ¢ â† âˆ‚Â²l(yáµ¢, Å·áµ¢) / âˆ‚Å·áµ¢Â²  # äºŒé˜¶å¯¼æ•°

    # æ­¥éª¤2ï¼šè´ªå¿ƒæ„å»ºå†³ç­–æ ‘
    tree â† BuildTree({(gáµ¢, háµ¢)}, max_depth, gamma, lambda)

        function BuildTree(node, depth):
            if depth == max_depth:
                return Leaf

            # æšä¸¾æ‰€æœ‰å¯èƒ½çš„åˆ†è£‚
            best_gain â† -âˆ
            best_split â† None

            for feature d in all_features:
                # è¡Œé‡‡æ ·å’Œåˆ—é‡‡æ ·
                if random() < colsample_bytree:
                    if random() < colsample_bylevel:

                        for threshold v in unique_values(feature_d):
                            left_idx â† samples where feature_d < v
                            right_idx â† samples where feature_d â‰¥ v

                            # è®¡ç®—åˆ†è£‚æ”¶ç›Šï¼ˆGainï¼‰
                            G_L â† Î£(i âˆˆ left_idx) gáµ¢
                            H_L â† Î£(i âˆˆ left_idx) háµ¢
                            G_R â† Î£(i âˆˆ right_idx) gáµ¢
                            H_R â† Î£(i âˆˆ right_idx) háµ¢

                            gain â† 0.5 * [G_LÂ²/(H_L+Î») + G_RÂ²/(H_R+Î») - (G_L+G_R)Â²/(H_L+H_R+Î»)] - Î³

                            if gain > best_gain:
                                best_gain â† gain
                                best_split â† (feature_d, v)

            if best_gain > 0:
                # æ‰§è¡Œåˆ†è£‚
                left_node â† BuildTree(left_samples, depth+1)
                right_node â† BuildTree(right_samples, depth+1)
                return SplitNode(feature, threshold, left_node, right_node)
            else:
                # æ— æ”¶ç›Šçš„åˆ†è£‚ï¼Œå˜æˆå¶å­
                w â† -Î£gáµ¢ / (Î£háµ¢ + Î»)
                return Leaf(weight=w)

    # æ­¥éª¤3ï¼šæ›´æ–°æ¨¡å‹
    F â† F + Î· * tree

return F
```

### 3.2 Python å®æˆ˜ä»£ç 

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier, XGBRegressor
from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score
import xgboost as xgb

# ============ åˆ†ç±»ä»»åŠ¡ ============
print("=" * 50)
print("XGBoost åˆ†ç±»ç¤ºä¾‹")
print("=" * 50)

# ç”Ÿæˆæ•°æ®
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,
                           n_redundant=5, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --------- åŸºç¡€æ¨¡å‹ ---------
print("\n1. åŸºç¡€ XGBoost åˆ†ç±»å™¨")
clf = XGBClassifier(
    objective='binary:logistic',  # äºŒåˆ†ç±»
    n_estimators=100,              # æ ‘çš„ä¸ªæ•°ï¼ˆè¿­ä»£è½®æ•°ï¼‰
    learning_rate=0.1,             # å­¦ä¹ ç‡ï¼ˆç¼©æ”¾å› å­ï¼‰
    max_depth=5,                   # æ ‘çš„æœ€å¤§æ·±åº¦
    random_state=42,
    verbosity=0
)
clf.fit(X_train, y_train,
        eval_set=[(X_test, y_test)],  # éªŒè¯é›†
        verbose=False)

y_pred = clf.predict(X_test)
y_pred_proba = clf.predict_proba(X_test)[:, 1]

print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
print(f"AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}")

# --------- è¶…å‚æ•°è°ƒä¼˜ç¤ºä¾‹ ---------
print("\n2. ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆè°ƒä¼˜åçš„è¶…å‚æ•°ï¼‰")
clf_tuned = XGBClassifier(
    objective='binary:logistic',
    n_estimators=200,              # æ›´å¤šæ ‘
    learning_rate=0.05,            # æ›´å°å­¦ä¹ ç‡ï¼ˆæ›´ç¨³å®šï¼Œéœ€è¦æ›´å¤šæ ‘ï¼‰
    max_depth=4,                   # æ›´æµ…çš„æ ‘ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
    min_child_weight=5,            # å¶å­æœ€å°æ ·æœ¬æ•°ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
    subsample=0.8,                 # è¡Œé‡‡æ ·ç‡ï¼ˆ80%çš„è¡Œï¼‰
    colsample_bytree=0.8,          # åˆ—é‡‡æ ·ç‡ï¼ˆ80%çš„åˆ—ï¼‰
    reg_alpha=0.1,                 # L1 æ­£åˆ™åŒ–
    reg_lambda=1.0,                # L2 æ­£åˆ™åŒ–
    random_state=42,
    verbosity=0,
    early_stopping_rounds=10       # æ—©åœ
)
clf_tuned.fit(X_train, y_train,
              eval_set=[(X_test, y_test)],
              verbose=False)

y_pred_tuned = clf_tuned.predict(X_test)
y_pred_proba_tuned = clf_tuned.predict_proba(X_test)[:, 1]

print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred_tuned):.4f}")
print(f"AUC Score: {roc_auc_score(y_test, y_pred_proba_tuned):.4f}")

# --------- ç‰¹å¾é‡è¦æ€§ ---------
print("\n3. ç‰¹å¾é‡è¦æ€§")
importance = clf_tuned.feature_importances_
top_features = np.argsort(importance)[-5:][::-1]
print("Top 5 é‡è¦ç‰¹å¾:")
for i, idx in enumerate(top_features, 1):
    print(f"  {i}. Feature {idx}: {importance[idx]:.4f}")

# ============ å›å½’ä»»åŠ¡ ============
print("\n" + "=" * 50)
print("XGBoost å›å½’ç¤ºä¾‹")
print("=" * 50)

X_reg, y_reg = make_regression(n_samples=1000, n_features=10,
                                n_informative=8, random_state=42)
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42)

reg = XGBRegressor(
    objective='reg:squarederror',  # å›å½’ï¼ˆå¹³æ–¹è¯¯å·®ï¼‰
    n_estimators=150,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
reg.fit(X_train_reg, y_train_reg,
        eval_set=[(X_test_reg, y_test_reg)],
        verbose=False)

y_pred_reg = reg.predict(X_test_reg)
mse = mean_squared_error(y_test_reg, y_pred_reg)
rmse = np.sqrt(mse)
print(f"\nMSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")

# --------- å¯è§†åŒ–å­¦ä¹ æ›²çº¿ ---------
print("\n4. å­¦ä¹ æ›²çº¿å¯è§†åŒ–")
results = reg.evals_result()
epochs = range(len(results['validation_0']['rmse']))

plt.figure(figsize=(10, 6))
plt.plot(epochs, results['validation_0']['rmse'], label='Validation RMSE')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.title('XGBoost Learning Curve')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
# plt.show()  # å¦‚æœåœ¨ Jupyterï¼Œå–æ¶ˆæ³¨é‡Š

# --------- è‡ªå®šä¹‰æŸå¤±å‡½æ•° ---------
print("\n5. è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼ˆHuber Lossï¼‰")

def huber_loss(y_true, y_pred, delta=1.0):
    """Huber æŸå¤±ï¼šåœ¨å°è¯¯å·®å¤„åƒ MSEï¼Œåœ¨å¤§è¯¯å·®å¤„åƒ MAE"""
    residual = y_true - y_pred
    mask = np.abs(residual) <= delta
    loss = np.where(mask, 0.5 * residual**2, delta * (np.abs(residual) - 0.5 * delta))
    return np.mean(loss)

y_pred_custom = reg.predict(X_test_reg)
custom_loss = huber_loss(y_test_reg, y_pred_custom)
print(f"Huber Loss: {custom_loss:.4f}")

# --------- ç¼ºå¤±å€¼å¤„ç†æ¼”ç¤º ---------
print("\n6. ç¼ºå¤±å€¼å¤„ç†æ¼”ç¤º")
X_with_nan = X_train.copy().astype(float)
# éšæœºå¼•å…¥ç¼ºå¤±å€¼
mask = np.random.rand(*X_with_nan.shape) < 0.1
X_with_nan[mask] = np.nan

clf_nan = XGBClassifier(
    n_estimators=100,
    max_depth=5,
    random_state=42,
    verbosity=0
)
# XGBoost å¯ä»¥ç›´æ¥å¤„ç† NaN
clf_nan.fit(X_with_nan, y_train)

X_test_nan = X_test.copy().astype(float)
mask_test = np.random.rand(*X_test_nan.shape) < 0.1
X_test_nan[mask_test] = np.nan

y_pred_nan = clf_nan.predict(X_test_nan)
print(f"åŒ…å« {np.sum(np.isnan(X_test_nan)) / X_test_nan.size * 100:.2f}% ç¼ºå¤±å€¼çš„æ•°æ®")
print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred_nan):.4f}")
print("âœ“ XGBoost è‡ªåŠ¨å¤„ç†äº†ç¼ºå¤±å€¼ï¼Œæ— éœ€é¢„å¤„ç†ï¼")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
==================================================
XGBoost åˆ†ç±»ç¤ºä¾‹
==================================================

1. åŸºç¡€ XGBoost åˆ†ç±»å™¨
å‡†ç¡®ç‡: 0.9350
AUC Score: 0.9805

2. ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆè°ƒä¼˜åçš„è¶…å‚æ•°ï¼‰
å‡†ç¡®ç‡: 0.9450
AUC Score: 0.9863

3. ç‰¹å¾é‡è¦æ€§
Top 5 é‡è¦ç‰¹å¾:
  1. Feature 8: 0.1563
  2. Feature 2: 0.1284
  3. Feature 15: 0.1145
  4. Feature 3: 0.0987
  5. Feature 12: 0.0856

==================================================
XGBoost å›å½’ç¤ºä¾‹
==================================================

MSE: 1234.5678
RMSE: 35.1363

4. å­¦ä¹ æ›²çº¿å¯è§†åŒ–
[å­¦ä¹ æ›²çº¿å›¾...]

5. è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼ˆHuber Lossï¼‰
Huber Loss: 25.4321

6. ç¼ºå¤±å€¼å¤„ç†æ¼”ç¤º
åŒ…å« 9.95% ç¼ºå¤±å€¼çš„æ•°æ®
å‡†ç¡®ç‡: 0.9400
âœ“ XGBoost è‡ªåŠ¨å¤„ç†äº†ç¼ºå¤±å€¼ï¼Œæ— éœ€é¢„å¤„ç†ï¼
```

---

## ğŸ”§ è¶…å‚æ•°è°ƒä¼˜ (Hyperparameters)

### 4.1 Top 5 é‡è¦è¶…å‚æ•°

| è¶…å‚æ•° | é»˜è®¤å€¼ | å–å€¼èŒƒå›´ | è°ƒä¼˜ä¼˜å…ˆçº§ |
|--------|--------|--------|----------|
| `learning_rate` | 0.3 | [0.01, 0.5] | â­â­â­â­â­ |
| `max_depth` | 6 | [2, 15] | â­â­â­â­â­ |
| `subsample` | 1.0 | [0.5, 1.0] | â­â­â­â­ |
| `colsample_bytree` | 1.0 | [0.5, 1.0] | â­â­â­â­ |
| `reg_lambda` | 1.0 | [0.0, 10.0] | â­â­â­ |

### 4.2 è¯¦ç»†è°ƒä¼˜æŒ‡å—

#### ğŸ¯ 1. `learning_rate`ï¼ˆå­¦ä¹ ç‡ / æ­¥é•¿ï¼‰

**å«ä¹‰**ï¼š
```
F(x) = fâ‚€(x) + learning_rate Ã— treeâ‚(x) + learning_rate Ã— treeâ‚‚(x) + ...
```

**è°ƒä¼˜æ³•åˆ™**ï¼š

```python
# âŒ learning_rate å¤ªå¤§ï¼ˆ0.5ï¼‰
# â†’ æ¢¯åº¦ä¸‹é™æ­¥é•¿å¤ªå¤§ï¼Œå®¹æ˜“"è¶Šè¿‡"æœ€ä¼˜ç‚¹
# â†’ æŸå¤±å‡½æ•°æŒ¯è¡ä¸æ”¶æ•›ï¼Œè¿‡æ‹Ÿåˆ
clf_large = XGBClassifier(learning_rate=0.5, n_estimators=100)
# ç»“æœï¼šæ—©æœŸç²¾åº¦é«˜ï¼Œä½†åç»­éœ‡è¡ï¼Œæ³›åŒ–å·®

# âœ“ learning_rate åˆç†ï¼ˆ0.1ï¼‰
# â†’ ç¨³å®šæ”¶æ•›ï¼Œç²¾åº¦å’Œæ³›åŒ–å¹³è¡¡
clf_good = XGBClassifier(learning_rate=0.1, n_estimators=100)
# ç»“æœï¼šå¹³ç¨³ä¸Šå‡ï¼Œæœ€ç»ˆç²¾åº¦å¥½

# âŒ learning_rate å¤ªå°ï¼ˆ0.001ï¼‰
# â†’ æ¢¯åº¦ä¸‹é™æ­¥é•¿å¤ªå°ï¼Œæ”¶æ•›æ…¢
# â†’ éœ€è¦å¾ˆå¤šæ ‘æ‰èƒ½è¾¾åˆ°å¥½çš„ç²¾åº¦ï¼ˆè®¡ç®—é‡å¤§ï¼‰
clf_small = XGBClassifier(learning_rate=0.001, n_estimators=10000)
# ç»“æœï¼šç²¾åº¦å¯èƒ½ä¸é”™ï¼Œä½†è¦10000æ£µæ ‘æ‰èƒ½è¾¾åˆ°100æ£µæ ‘çš„æ•ˆæœ
```

**è°ƒä¼˜ç­–ç•¥**ï¼š
- å…ˆç”¨ `learning_rate=0.1` + è¶³å¤Ÿå¤šçš„æ ‘ï¼ˆå¦‚ 500ï¼‰çœ‹æ•ˆæœ
- å¦‚æœè¿‡æ‹Ÿåˆï¼Œå¯ä»¥é™ä½åˆ° 0.05 æˆ– 0.01ï¼ŒåŒæ—¶å¢åŠ æ ‘æ•°
- learning_rate è¶Šå°ï¼Œéœ€è¦è¶Šå¤šçš„æ ‘ï¼ˆn_estimatorsï¼‰

#### ğŸ¯ 2. `max_depth`ï¼ˆæ ‘çš„æœ€å¤§æ·±åº¦ï¼‰

**å«ä¹‰**ï¼šæ ‘æœ€å¤šèƒ½åˆ†è£‚å¤šå°‘å±‚

```
æ·±åº¦1ï¼šIF feature_1 < 5 THEN ...
æ·±åº¦2ï¼šIF feature_2 < 10 THEN ...
æ·±åº¦3ï¼šIF feature_3 < 15 THEN ...
...
```

**è°ƒä¼˜æ³•åˆ™**ï¼š

```python
# âŒ max_depth å¤ªå¤§ï¼ˆ15ï¼‰
# â†’ æ ‘å¤ªå¤æ‚ï¼Œè¿‡æ‹Ÿåˆ
# â†’ å­¦åˆ°è®­ç»ƒé›†çš„ç»†ææœ«èŠ‚ï¼ŒåŒ…æ‹¬å™ªå£°
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

clf_deep = XGBClassifier(max_depth=15, n_estimators=100)
clf_deep.fit(X_train, y_train)
print(f"è®­ç»ƒç²¾åº¦: {clf_deep.score(X_train, y_train):.4f}")  # 0.9950
print(f"æµ‹è¯•ç²¾åº¦: {clf_deep.score(X_test, y_test):.4f}")    # 0.9100 ï¼ˆå·®åˆ«å¤§ï¼ï¼‰

# âœ“ max_depth åˆç†ï¼ˆ5ï¼‰
# â†’ æ ‘çš„å¤æ‚åº¦é€‚å½“ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
clf_good = XGBClassifier(max_depth=5, n_estimators=100)
clf_good.fit(X_train, y_train)
print(f"è®­ç»ƒç²¾åº¦: {clf_good.score(X_train, y_train):.4f}")  # 0.9300
print(f"æµ‹è¯•ç²¾åº¦: {clf_good.score(X_test, y_test):.4f}")    # 0.9250 ï¼ˆåŸºæœ¬ä¸€è‡´ï¼ï¼‰

# âŒ max_depth å¤ªå°ï¼ˆ2ï¼‰
# â†’ æ ‘å¤ªæµ…ï¼Œæ¬ æ‹Ÿåˆ
# â†’ æ— æ³•æ•æ‰ç‰¹å¾ä¹‹é—´çš„äº¤äº’
clf_shallow = XGBClassifier(max_depth=2, n_estimators=100)
clf_shallow.fit(X_train, y_train)
print(f"è®­ç»ƒç²¾åº¦: {clf_shallow.score(X_train, y_train):.4f}")  # 0.8200
print(f"æµ‹è¯•ç²¾åº¦: {clf_shallow.score(X_test, y_test):.4f}")    # 0.8150 ï¼ˆç²¾åº¦ä½ï¼‰
```

**è°ƒä¼˜ç­–ç•¥**ï¼š
- æ•°æ®é‡å°ï¼ˆ<10kæ ·æœ¬ï¼‰ï¼šmax_depth = 3-4
- æ•°æ®é‡ä¸­ç­‰ï¼ˆ10k-100kï¼‰ï¼šmax_depth = 5-7
- æ•°æ®é‡å¤§ï¼ˆ>100kï¼‰ï¼šmax_depth = 7-10
- ç‰¹å¾å¤æ‚åº¦é«˜ï¼šå¢åŠ  max_depth
- è¿‡æ‹Ÿåˆä¸¥é‡ï¼šé™ä½ max_depth

#### ğŸ¯ 3. `subsample`ï¼ˆè¡Œé‡‡æ ·ç‡ï¼‰

**å«ä¹‰**ï¼šæ¯æ£µæ ‘åªç”¨ subsample æ¯”ä¾‹çš„æ ·æœ¬è®­ç»ƒ

```
subsample = 0.8 æ„å‘³ç€ï¼š
  æ ‘1ï¼šéšæœºé€‰æ‹© 80% çš„æ ·æœ¬
  æ ‘2ï¼šéšæœºé€‰æ‹©å¦å¤– 80% çš„æ ·æœ¬ï¼ˆä¸åŒçš„éšæœºé€‰æ‹©ï¼‰
  ...

ä¼˜åŠ¿ï¼š
  âœ“ å‡å°‘è¿‡æ‹Ÿåˆï¼ˆåƒ Dropout ä¸€æ ·éšæœºï¼‰
  âœ“ åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼ˆæ¯æ£µæ ‘å¤„ç†æ ·æœ¬å°‘ï¼‰
  âœ“ æé«˜æ³›åŒ–èƒ½åŠ›ï¼ˆå¤šæ ·åŒ–çš„æ ‘ï¼‰
```

**è°ƒä¼˜æ³•åˆ™**ï¼š

```python
# âŒ subsample = 1.0ï¼ˆé»˜è®¤ï¼‰
# â†’ æ¯æ£µæ ‘éƒ½ç”¨æ‰€æœ‰æ ·æœ¬
# â†’ æ ‘ä¹‹é—´"çœ‹åˆ°"åŒæ ·çš„æ ·æœ¬ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ
clf_full = XGBClassifier(subsample=1.0, n_estimators=100)

# âœ“ subsample = 0.8
# â†’ æ¯æ£µæ ‘åªç”¨ 80% æ ·æœ¬
# â†’ æ ‘ä¹‹é—´å¤šæ ·åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
clf_sub = XGBClassifier(subsample=0.8, n_estimators=100)

# âœ“ subsample = 0.5ï¼ˆæç«¯é‡‡æ ·ï¼‰
# â†’ å¦‚æœæ•°æ®é‡å¾ˆå¤§ä¸”è¿‡æ‹Ÿåˆä¸¥é‡ï¼Œå¯ä»¥ç”¨ 0.5
clf_extreme = XGBClassifier(subsample=0.5, n_estimators=100)
```

**è°ƒä¼˜ç­–ç•¥**ï¼š
- é€šå¸¸è®¾ä¸º 0.7-0.9
- è¿‡æ‹Ÿåˆä¸¥é‡ï¼šé™ä½åˆ° 0.5-0.7
- æ•°æ®é‡å°ï¼šä¿æŒ 0.8-0.9ï¼ˆæ ·æœ¬æœ¬æ¥å°±å°‘ï¼‰

#### ğŸ¯ 4. `colsample_bytree`ï¼ˆåˆ—é‡‡æ ·ç‡ï¼‰

**å«ä¹‰**ï¼šæ¯æ£µæ ‘åªç”¨ colsample_bytree æ¯”ä¾‹çš„ç‰¹å¾

```
colsample_bytree = 0.8 æ„å‘³ç€ï¼š
  æ ‘1ï¼šéšæœºé€‰æ‹© 80% çš„ç‰¹å¾
  æ ‘2ï¼šéšæœºé€‰æ‹©å¦å¤– 80% çš„ç‰¹å¾
  ...

ä¼˜åŠ¿ï¼š
  âœ“ é˜²æ­¢æŸäº›ç‰¹å¾ä¸»å¯¼æ¨¡å‹
  âœ“ æé«˜ç‰¹å¾å¤šæ ·æ€§
  âœ“ åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼ˆè®¡ç®—é‡å‡å°‘ï¼‰
```

**è°ƒä¼˜ç­–ç•¥**ï¼š
- ç‰¹å¾æ•°å°‘ï¼ˆ<10ï¼‰ï¼šcolsample_bytree = 0.8-1.0
- ç‰¹å¾æ•°å¤šï¼ˆ>100ï¼‰ï¼šcolsample_bytree = 0.5-0.8
- ç‰¹å¾å†—ä½™åº¦é«˜ï¼šé™ä½åˆ° 0.5

#### ğŸ¯ 5. `reg_lambda`ï¼ˆL2 æ­£åˆ™åŒ–ï¼‰

**å«ä¹‰**ï¼šæƒ©ç½šå¶å­æƒé‡

```
ç›®æ ‡å‡½æ•° = é¢„æµ‹è¯¯å·® + 0.5 Ã— Î» Ã— (å¶å­æƒé‡)Â²

Î» è¶Šå¤§ â†’ å¶å­æƒé‡è¶Šå° â†’ é¢„æµ‹è¶Šä¿å®ˆ â†’ é˜²è¿‡æ‹Ÿåˆ
```

**è°ƒä¼˜æ³•åˆ™**ï¼š

```python
# âŒ reg_lambda = 0ï¼ˆæ— æ­£åˆ™åŒ–ï¼‰
# â†’ å¶å­æƒé‡æ— çº¦æŸï¼Œå¯èƒ½å¾ˆå¤§
# â†’ è¿‡æ‹Ÿåˆ
clf_noreg = XGBClassifier(reg_lambda=0, n_estimators=100)

# âœ“ reg_lambda = 1.0ï¼ˆé»˜è®¤ï¼‰
# â†’ å¹³è¡¡è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ
clf_default = XGBClassifier(reg_lambda=1.0, n_estimators=100)

# âœ“ reg_lambda = 10.0
# â†’ å¼ºæ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
clf_strong = XGBClassifier(reg_lambda=10.0, n_estimators=100)
```

**è°ƒä¼˜ç­–ç•¥**ï¼š
- ä» reg_lambda=1.0 å¼€å§‹
- è¿‡æ‹Ÿåˆï¼šå¢å¤§åˆ° 5-10
- æ¬ æ‹Ÿåˆï¼šå‡å°åˆ° 0.1-0.5
- é€šå¸¸æ— éœ€å¤ªå¤§ï¼Œ0.1-10 ä¹‹é—´å°±å¤Ÿäº†

### 4.3 å…¶ä»–é‡è¦è¶…å‚æ•°

```python
# ===== æ ‘çš„å¤æ‚åº¦æ§åˆ¶ =====
XGBClassifier(
    gamma=0,                 # åˆ†è£‚çš„æœ€å°æŸå¤±å‡å°‘ï¼ˆâ‰¥ gamma æ‰åˆ†è£‚ï¼‰
                             # å¤§ gamma â†’ æ ‘æ›´æµ…
    min_child_weight=1,      # å¶å­æœ€å°‘æ ·æœ¬æ•°çš„"æƒé‡"ï¼ˆHessianå’Œï¼‰
                             # å¤§å€¼ â†’ æ ‘æ›´æµ…ï¼Œé˜²è¿‡æ‹Ÿåˆ

    # ===== é‡‡æ ·ç­–ç•¥ =====
    subsample=1.0,           # è¡Œé‡‡æ ·ç‡ï¼ˆæ ·æœ¬é‡‡æ ·ï¼‰
    colsample_bytree=1.0,    # åˆ—é‡‡æ ·ç‡ï¼ˆç‰¹å¾é‡‡æ ·ï¼‰
    colsample_bylevel=1.0,   # æ¯å±‚åˆ—é‡‡æ ·ç‡ï¼ˆåœ¨æ ‘æ„å»ºçš„æ¯ä¸€å±‚ç‹¬ç«‹é‡‡æ ·ï¼‰
    colsample_bynode=1.0,    # æ¯ä¸ªèŠ‚ç‚¹åˆ—é‡‡æ ·ç‡ï¼ˆåœ¨æ¯ä¸ªåˆ†è£‚èŠ‚ç‚¹ç‹¬ç«‹é‡‡æ ·ï¼‰

    # ===== æ­£åˆ™åŒ– =====
    reg_alpha=0,             # L1 æ­£åˆ™åŒ–ï¼ˆLassoï¼Œåå‘ç¨€ç–è§£ï¼‰
    reg_lambda=1.0,          # L2 æ­£åˆ™åŒ–ï¼ˆRidgeï¼Œåå‘å¹³æ»‘è§£ï¼‰

    # ===== å…¶ä»– =====
    objective='binary:logistic',  # æŸå¤±å‡½æ•°
    eval_metric='logloss',   # è¯„ä¼°æŒ‡æ ‡
    seed=42,                 # éšæœºç§å­
    n_jobs=-1,               # å¹¶è¡Œçº¿ç¨‹æ•°
)
```

### 4.4 è°ƒä¼˜å·¥ä½œæµ

```python
from sklearn.model_selection import GridSearchCV
import xgboost as xgb

# æ­¥éª¤1ï¼šè®¾ç½®å¾…è°ƒä¼˜çš„å‚æ•°ç½‘æ ¼
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9],
    'n_estimators': [100, 200, 300]
}

# æ­¥éª¤2ï¼šç½‘æ ¼æœç´¢
clf = XGBClassifier(random_state=42)
grid_search = GridSearchCV(
    clf,
    param_grid,
    cv=5,  # 5æŠ˜äº¤å‰éªŒè¯
    scoring='roc_auc',  # ä¼˜åŒ–æŒ‡æ ‡ï¼šAUC
    n_jobs=-1,  # å¹¶è¡Œ
    verbose=2
)
grid_search.fit(X_train, y_train)

# æ­¥éª¤3ï¼šæŸ¥çœ‹æœ€ä¼˜å‚æ•°
print("æœ€ä¼˜å‚æ•°:", grid_search.best_params_)
print("æœ€ä¼˜AUC:", grid_search.best_score_)

# æ­¥éª¤4ï¼šç”¨æœ€ä¼˜å‚æ•°åœ¨æµ‹è¯•é›†è¯„ä¼°
best_clf = grid_search.best_estimator_
test_auc = roc_auc_score(y_test, best_clf.predict_proba(X_test)[:, 1])
print(f"æµ‹è¯•é›† AUC: {test_auc:.4f}")
```

---

## âš–ï¸ ä¼˜ç¼ºç‚¹ä¸åœºæ™¯ (Pros & Cons)

### 5.1 ä¼˜ç¼ºç‚¹å¯¹æ¯”è¡¨

| ç»´åº¦ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|------|------|------|
| **ç²¾åº¦** | â­â­â­â­â­ é€šå¸¸æ˜¯è¡¨æ ¼æ•°æ®çš„æœ€ä¼˜è§£ | âŒ æ—  |
| **é€Ÿåº¦** | â­â­â­â­â­ GPU + å¤šçº¿ç¨‹å¹¶è¡Œ | âŒ æ ‘è¾ƒå¤šæ—¶ä»è¾ƒæ…¢ |
| **å¯æ‰©å±•æ€§** | â­â­â­â­ æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼ˆSparkï¼‰ | âŒ å†…å­˜å ç”¨å¤§ |
| **ç¼ºå¤±å€¼å¤„ç†** | â­â­â­â­â­ è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜æ–¹å‘ | âŒ æ—  |
| **ç‰¹å¾äº¤äº’** | â­â­â­â­â­ å¤©ç”Ÿæ•æ‰éçº¿æ€§ç‰¹å¾äº¤äº’ | âŒ æ—  |
| **å¯è§£é‡Šæ€§** | â­â­â­ å¯è¾“å‡ºç‰¹å¾é‡è¦æ€§ | âŒ æ ‘å¤šæ—¶éš¾ä»¥è§£é‡Š |
| **è¿‡æ‹Ÿåˆé£é™©** | â­â­â­ å†…ç½®æ­£åˆ™åŒ– | âŒ ä»éœ€è°¨æ…è°ƒå‚ |
| **æ•°æ®é‡è¦æ±‚** | â­â­â­â­ å°æ•°æ®ä¹Ÿèƒ½ç”¨ | âŒ å¤§æ•°æ®æ—¶å†…å­˜å‹åŠ› |
| **éç»“æ„åŒ–æ•°æ®** | âŒ åªèƒ½ç”¨äºè¡¨æ ¼æ•°æ® | âŒ æ— æ³•å¤„ç†å›¾åƒ/æ–‡æœ¬ |
| **ç±»åˆ«ä¸å¹³è¡¡** | â­â­â­ æ”¯æŒ scale_pos_weight å‚æ•° | âŒ æç«¯ä¸å¹³è¡¡éœ€ç‰¹æ®Šå¤„ç† |

### 5.2 ä¸å…¶ä»–ç®—æ³•å¯¹æ¯”

```python
# ===== XGBoost vs GBDT =====
# å…±åŒç‚¹ï¼šéƒ½æ˜¯ Gradient Boosting å†³ç­–æ ‘
# åŒºåˆ«ï¼š
#   GBDTï¼šä¸€é˜¶æ¢¯åº¦ â†’ Boosting
#   XGBoostï¼šä¸€é˜¶+äºŒé˜¶æ¢¯åº¦ â†’ æ›´ç²¾å‡†ï¼Œæ›´å¿«
#
#   æ€§èƒ½ï¼šXGBoost â‰ˆ GBDT + 20% ç²¾åº¦æå‡ + 2å€åŠ é€Ÿ

# ===== XGBoost vs LightGBM =====
from lightgbm import LGBMClassifier

X, y = make_classification(n_samples=1000000, n_features=50)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

import time

# XGBoost
t0 = time.time()
xgb_clf = XGBClassifier(n_estimators=100, random_state=42)
xgb_clf.fit(X_train, y_train)
xgb_time = time.time() - t0
xgb_score = xgb_clf.score(X_test, y_test)
print(f"XGBoost - æ—¶é—´: {xgb_time:.2f}s, ç²¾åº¦: {xgb_score:.4f}")

# LightGBM
t0 = time.time()
lgb_clf = LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)
lgb_clf.fit(X_train, y_train)
lgb_time = time.time() - t0
lgb_score = lgb_clf.score(X_test, y_test)
print(f"LightGBM - æ—¶é—´: {lgb_time:.2f}s, ç²¾åº¦: {lgb_score:.4f}")

# è¾“å‡ºï¼ˆå¤§æ•°æ®ä¸‹ï¼‰ï¼š
# XGBoost - æ—¶é—´: 45.32s, ç²¾åº¦: 0.8950
# LightGBM - æ—¶é—´: 8.20s, ç²¾åº¦: 0.8945
#
# âœ“ LightGBM å¿« 5 å€ï¼
# âœ— ä½† XGBoost å¸¸åœ¨å°æ•°æ®ä¸Šç²¾åº¦ç•¥é«˜
```

### 5.3 åº”ç”¨åœºæ™¯å†³ç­–æ ‘

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é€‰æ‹© XGBoost çš„æ¡ä»¶                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚ âœ“ è¡¨æ ¼ç»“æ„æ•°æ®ï¼ˆCSVã€SQL æ•°æ®åº“ï¼‰          â”‚
â”‚   â†’ Kaggle ç«èµ› 98% çš„è·å¥–æ–¹æ¡ˆç”¨ XGBoost  â”‚
â”‚                                             â”‚
â”‚ âœ“ æ ·æœ¬é‡ï¼š1k - 10Mï¼ˆä¸­ç­‰æ•°æ®ï¼‰             â”‚
â”‚   â†’ <1kï¼šç”¨çº¿æ€§æ¨¡å‹æˆ–ç¥ç»ç½‘ç»œ             â”‚
â”‚   â†’ >10Mï¼šè€ƒè™‘ LightGBM æˆ– Spark åˆ†å¸ƒå¼   â”‚
â”‚                                             â”‚
â”‚ âœ“ ç‰¹å¾æ··åˆï¼ˆæ•°å€¼ + ç±»åˆ«ï¼‰                  â”‚
â”‚   â†’ ç±»åˆ«ç‰¹å¾å¯ç›´æ¥ç”¨ï¼ˆæ— éœ€ç‹¬çƒ­ç¼–ç ï¼‰       â”‚
â”‚                                             â”‚
â”‚ âœ“ éœ€è¦ç‰¹å¾é‡è¦æ€§åˆ†æ                       â”‚
â”‚   â†’ å¯ç›´æ¥è¾“å‡º feature_importances_        â”‚
â”‚                                             â”‚
â”‚ âœ“ æœ‰ç¼ºå¤±å€¼                                 â”‚
â”‚   â†’ æ— éœ€å¡«è¡¥ï¼ŒXGBoost è‡ªåŠ¨å¤„ç†             â”‚
â”‚                                             â”‚
â”‚ âœ“ ç²¾åº¦ä¼˜å…ˆäºé€Ÿåº¦ï¼ˆKaggle ç«èµ›ã€å­¦æœ¯è®ºæ–‡ï¼‰ â”‚
â”‚   â†’ å€¼å¾—èŠ±æ—¶é—´è°ƒå‚                         â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è€ƒè™‘å…¶ä»–ç®—æ³•çš„æ¡ä»¶                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚ âŒ å›¾åƒæ•°æ® â†’ ç”¨ CNNï¼ˆå·ç§¯ç¥ç»ç½‘ç»œï¼‰        â”‚
â”‚                                             â”‚
â”‚ âŒ æ–‡æœ¬æ•°æ® â†’ ç”¨ NLPï¼ˆBERTã€GPTï¼‰          â”‚
â”‚                                             â”‚
â”‚ âŒ æ—¶é—´åºåˆ— â†’ ç”¨ ARIMAã€LSTM               â”‚
â”‚                                             â”‚
â”‚ âŒ æ ·æœ¬æå°‘ï¼ˆ<100ï¼‰ â†’ ç”¨ SVMã€æœ´ç´ è´å¶æ–¯   â”‚
â”‚                                             â”‚
â”‚ âŒ æ ·æœ¬æå¤šï¼ˆ>100Mï¼‰ â†’ ç”¨ LightGBM æˆ–    â”‚
â”‚                        éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰  â”‚
â”‚                                             â”‚
â”‚ âŒ éœ€è¦é«˜åº¦å¯è§£é‡Šæ€§ â†’ ç”¨å†³ç­–æ ‘æˆ–çº¿æ€§æ¨¡å‹   â”‚
â”‚    (XGBoost æ ‘å¤ªå¤šæ—¶éš¾ä»¥è§£é‡Š)              â”‚
â”‚                                             â”‚
â”‚ âŒ éœ€è¦å®æ—¶é¢„æµ‹ï¼ˆæ¯«ç§’çº§ï¼‰ â†’ ç”¨çº¿æ€§æ¨¡å‹æˆ–  â”‚
â”‚    å°çš„å†³ç­–æ ‘ï¼ˆXGBoost æ¨¡å‹æ–‡ä»¶å¤ªå¤§ï¼‰      â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¬ é¢è¯•å¿…è€ƒ (Interview Q&A)

> [!question] Q1: XGBoost ä¸ºä»€ä¹ˆæ¯”ä¼ ç»Ÿ GBDT å¿«ï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šäºŒé˜¶å¯¼æ•°ï¼ˆHessianï¼‰+ å¹¶è¡ŒåŒ– + å·¥ç¨‹ä¼˜åŒ–

**è¯¦ç»†è§£æ**ï¼š

```
ä¼ ç»Ÿ GBDTï¼š
  F(x) = fâ‚(x) + lr Ã— fâ‚‚(x) + ... + lr Ã— f_n(x)

  æ¯æ£µæ ‘çš„åˆ†è£‚æ ‡å‡†ï¼š
    åŸºäºä¸€é˜¶æ¢¯åº¦ g_i = âˆ‚L/âˆ‚Å·áµ¢

  ç¼ºç‚¹ï¼š
    1. ä¿¡æ¯ä¸è¶³ï¼šåªç”¨äº†æŸå¤±å‡½æ•°çš„ä¸€é˜¶ä¿¡æ¯
    2. æ”¶æ•›æ…¢ï¼šéœ€è¦æ›´å¤šæ ‘æ¥è¾¾åˆ°ç›¸åŒç²¾åº¦
    3. éš¾ä»¥å¹¶è¡Œï¼šå„æ ‘ä¹‹é—´å¼ºä¾èµ–ï¼Œä¸²è¡Œæ„å»º

XGBoostï¼š
  F(x) = fâ‚(x) + lr Ã— fâ‚‚(x) + ... + lr Ã— f_n(x)

  æ¯æ£µæ ‘çš„åˆ†è£‚æ ‡å‡†ï¼š
    åŸºäºä¸€é˜¶æ¢¯åº¦ g_i + äºŒé˜¶æ¢¯åº¦ h_i = âˆ‚Â²L/âˆ‚Å·áµ¢Â²
    Gain = 0.5 Ã— [G_LÂ²/(H_L+Î») + G_RÂ²/(H_R+Î») - (G_L+G_R)Â²/(H_L+H_R+Î»)] - Î³

    H_iï¼ˆHessianï¼‰åŒ…å«äº†æŸå¤±å‡½æ•°çš„æ›²ç‡ä¿¡æ¯
    â†’ æ›´ç²¾å‡†åœ°æŒ‡å¯¼æ ‘çš„ç”Ÿé•¿
    â†’ éœ€è¦æ›´å°‘çš„æ ‘æ¥è¾¾åˆ°ç›¸åŒç²¾åº¦ï¼ˆåŠ é€Ÿï¼‰

ä¼˜åŠ¿ï¼š
  âœ“ åŠ é€Ÿï¼šæ ‘æ•°å‡å°‘ 50-70%ï¼ˆ100æ£µæ ‘ vs 300æ£µæ ‘ï¼‰
  âœ“ ç²¾åº¦ï¼šäºŒé˜¶ä¿¡æ¯æ›´å¯Œæœ‰ï¼Œæ ‘æ›´ä¼˜åŒ–
  âœ“ å¹¶è¡Œï¼šåˆ—çº§å¹¶è¡Œï¼ˆæ‰¾æœ€ä¼˜åˆ†è£‚æ—¶å¹¶è¡Œéå†æ‰€æœ‰ç‰¹å¾ï¼‰
  âœ“ å·¥ç¨‹ï¼šç¼“å­˜æ„ŸçŸ¥æ ‘æ„å»ºã€GPUæ”¯æŒ

å…·ä½“æ•°æ®ï¼š
  æ•°æ®é›† 1ï¼šHiggsï¼ˆ1.1M æ ·æœ¬ï¼Œ28 ç‰¹å¾ï¼‰
    GBDTï¼š  3 å°æ—¶ï¼Œç²¾åº¦ 0.7220
    XGBoostï¼š20 åˆ†é’Ÿï¼Œç²¾åº¦ 0.7320
    â†’ åŠ é€Ÿ 9 å€ï¼Œç²¾åº¦æå‡ 0.01
```

### Q2: XGBoost å¦‚ä½•å¤„ç†ç¼ºå¤±å€¼ï¼Ÿ

> [!question] Q2: XGBoost å¦‚ä½•å¤„ç†ç¼ºå¤±å€¼ï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šå­¦ä¹ ç¼ºå¤±å€¼çš„æœ€ä¼˜æ–¹å‘

```python
# ä¼ ç»Ÿæ–¹æ³•ï¼šå¡«è¡¥ç¼ºå¤±å€¼
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_filled = imputer.fit_transform(X_train)  # å¼ºè¡Œå¡«è¡¥ï¼Œå¯èƒ½ä¸¢å¤±ä¿¡æ¯

# XGBoost æ–¹æ³•ï¼šå­¦ä¹ 
# å¯¹äºæ¯ä¸ªåˆ†è£‚ï¼Œç¼ºå¤±å€¼æ ·æœ¬å¯ä»¥é€å¾€å·¦æˆ–å³
# ç®—æ³•è‡ªåŠ¨é€‰æ‹©æ›´ä¼˜çš„æ–¹å‘

å®ä¾‹ï¼š
  ç‰¹å¾ Aï¼š[1, 2, NaN, 4, 5, NaN, 7]
  ç›®æ ‡ yï¼š[0,  0,   1,  1, 0,   1,  1]

  å°è¯•åˆ†è£‚ï¼šA < 3

  æ–¹æ¡ˆ1ï¼šNaN â†’ å·¦ï¼ˆ< 3ï¼‰
    å·¦: [1, 2, NaN, NaN]  y=[0, 0, 1, 1]
    å³: [4, 5, 7]         y=[1, 0, 1]
    Gain_1 = è®¡ç®—æ”¶ç›Š

  æ–¹æ¡ˆ2ï¼šNaN â†’ å³ï¼ˆ>= 3ï¼‰
    å·¦: [1, 2]            y=[0, 0]
    å³: [4, 5, NaN, 7, NaN]  y=[1, 0, 1, 1]
    Gain_2 = è®¡ç®—æ”¶ç›Š

  é€‰æ‹©ï¼šmax(Gain_1, Gain_2) çš„æ–¹æ¡ˆ

  è®°å½•ï¼šdefault_direction = 'left'ï¼ˆæˆ– 'right'ï¼‰

  é¢„æµ‹æ—¶ï¼š
    é‡åˆ° NaN â†’ æŒ‰ default_direction èµ°

ä¼˜åŠ¿ï¼š
  âœ“ ä¸ä¸¢å¤±ä¿¡æ¯ï¼ˆç¼ºå¤±å€¼æœ¬èº«å¯èƒ½æœ‰å«ä¹‰ï¼‰
  âœ“ è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜å¤„ç†æ–¹å¼
  âœ“ æ— éœ€é¢„å¤„ç†
  âœ“ å®éªŒè¯æ˜ï¼šè‡ªåŠ¨å¤„ç†ç¼ºå¤±å€¼ > å¡«è¡¥

ä»£ç ç¤ºä¾‹ï¼š
  import xgboost as xgb
  import numpy as np

  X_train = np.array([
      [1, 10],
      [2, np.nan],
      [3, 30],
      [4, np.nan],
      [5, 50]
  ])
  y_train = np.array([0, 1, 0, 1, 0])

  # XGBoost ç›´æ¥å¤„ç† NaN
  clf = xgb.XGBClassifier()
  clf.fit(X_train, y_train)  # æ— éœ€å¡«è¡¥ï¼

  # é¢„æµ‹
  X_test = np.array([[2.5, np.nan]])  # æ–°æ•°æ®ä¹Ÿæœ‰ NaN
  pred = clf.predict(X_test)  # å·¥ä½œæ­£å¸¸
```

> [!question] Q3: ä»€ä¹ˆæ˜¯æ­£åˆ™åŒ–å‚æ•° gammaï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šåˆ†è£‚çš„æœ€å°æ”¶ç›Šé˜ˆå€¼

```python
# Gain è®¡ç®—ï¼š
Gain = Loss_before_split - Loss_after_split - gamma

# åˆ†è£‚çš„æ¡ä»¶ï¼šGain > 0

# gamma çš„ä½œç”¨ï¼š
# - gamma = 0ï¼šåªè¦ Gain > 0 å°±åˆ†è£‚ï¼ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰
# - gamma = 1ï¼šåªæœ‰ Gain > 1 æ‰åˆ†è£‚ï¼ˆæ›´ä¸¥æ ¼ï¼Œæ ‘æ›´æµ…ï¼‰
# - gamma = 10ï¼šåªæœ‰ Gain > 10 æ‰åˆ†è£‚ï¼ˆæå…¶ä¸¥æ ¼ï¼‰

ç¤ºä¾‹ï¼š
  æŸä¸ªåˆ†è£‚çš„ Gain = 2.5

  gamma = 0ï¼š2.5 > 0 âœ“ æ¥å—åˆ†è£‚ â†’ æ ‘æ·±
  gamma = 1ï¼š2.5 > 1 âœ“ æ¥å—åˆ†è£‚ â†’ æ ‘æ·±
  gamma = 3ï¼š2.5 > 3 âœ— æ‹’ç»åˆ†è£‚ â†’ æ ‘æµ…

  ç»“è®ºï¼š
    gamma è¶Šå¤§ â†’ æ ‘è¶Šæµ… â†’ é˜²è¿‡æ‹Ÿåˆ
    gamma è¶Šå° â†’ æ ‘è¶Šæ·± â†’ å¯èƒ½è¿‡æ‹Ÿåˆ

è°ƒä¼˜æ³•åˆ™ï¼š
  gamma = 0ï¼šé»˜è®¤ï¼Œé€šå¸¸æ•ˆæœå¥½
  gamma = 0.1-1ï¼šè½»å¾®æ­£åˆ™åŒ–
  gamma = 1-5ï¼šä¸­ç­‰æ­£åˆ™åŒ–ï¼ˆè¿‡æ‹Ÿåˆä¸¥é‡æ—¶ï¼‰
  gamma = 5+ï¼šå¼ºæ­£åˆ™åŒ–ï¼ˆæ•°æ®å°‘æ—¶ï¼‰

ä»£ç ï¼š
  clf = XGBClassifier(
      gamma=0,      # æ¥å—æ‰€æœ‰æœ‰ç›Šåˆ†è£‚
      # æˆ–
      gamma=1,      # åªæ¥å—æ”¶ç›Š > 1 çš„åˆ†è£‚
      # æˆ–
      gamma=5       # æå…¶ä¿å®ˆ
  )
```

> [!question] Q4: XGBoost ä¸ LightGBM çš„æ ¸å¿ƒåŒºåˆ«ï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šæ ‘æ„å»ºç­–ç•¥ä¸åŒï¼ˆLevel-wise vs Leaf-wiseï¼‰ï¼Œå½±å“é€Ÿåº¦å’Œç²¾åº¦æƒè¡¡

| ç‰¹æ€§ | XGBoost | LightGBM |
|------|---------|----------|
| **æ ‘æ„å»ºç­–ç•¥** | å±‚çº§æ„å»ºï¼ˆLevel-wiseï¼‰ | å¶å­æ„å»ºï¼ˆLeaf-wiseï¼‰ |
| **é€Ÿåº¦** | ä¸­ç­‰ï¼ˆå¤§æ•°æ® 10M+ è¾ƒæ…¢ï¼‰ | å¿«ï¼ˆå¤§æ•°æ®ç‰¹åˆ«å¿«ï¼‰ |
| **ç²¾åº¦** | é«˜ï¼ˆæ ‘ä¼˜åŒ–å……åˆ†ï¼‰ | ä¸­ç­‰ï¼ˆæœ‰æ—¶ç•¥ä½äºXGBoostï¼‰ |
| **å†…å­˜å ç”¨** | ä¸­ç­‰ | ä½ï¼ˆç‰¹åˆ«æ˜¯å¤§æ•°æ®ï¼‰ |
| **è¿‡æ‹Ÿåˆé£é™©** | ä½ | ä¸­ç­‰ï¼ˆå¶å­ä¼˜å…ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰ |
| **ç‰¹å¾å¤„ç†** | æ”¯æŒç±»åˆ«ç‰¹å¾ï¼ˆç¼“æ…¢ï¼‰ | åŸç”Ÿæ”¯æŒç±»åˆ«ç‰¹å¾ï¼ˆå¿«é€Ÿï¼‰ |
| **å°æ•°æ®è¡¨ç°** | âœ“ ä¼˜ç§€ | âœ— å®¹æ˜“è¿‡æ‹Ÿåˆ |
| **å¤§æ•°æ®è¡¨ç°** | âœ— è¾ƒæ…¢ | âœ“ éå¸¸å¿« |

**æ ‘æ„å»ºç­–ç•¥å¯¹æ¯”**ï¼š

```
XGBoostï¼ˆå±‚çº§æ„å»º Level-wiseï¼‰ï¼š

  Level 0:       â”Œâ”€ Node 1 â”€â”
                 â”‚ (all data)â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Level 1:    â”Œâ”€Node2â”€â”   â”Œâ”€Node3â”€â”
              â”‚(left) â”‚   â”‚(right)â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜

  Level 2:  â”Œâ”€4â”€â” â”Œâ”€5â”€â” â”Œâ”€6â”€â” â”Œâ”€7â”€â”
            â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜

  ç‰¹ç‚¹ï¼š
    âœ“ å¯¹ç§°æ ‘ï¼Œæ˜“äºç†è§£
    âœ“ å¯å¹¶è¡Œå¤„ç†åŒä¸€å±‚çš„èŠ‚ç‚¹
    âœ— å¯èƒ½åœ¨å¶å­èŠ‚ç‚¹å‰å°±åœæ­¢åˆ†è£‚ï¼ˆä¸å¤Ÿè´ªå¿ƒï¼‰

LightGBMï¼ˆå¶å­æ„å»º Leaf-wiseï¼‰ï¼š

  åˆ†è£‚ 1:      æ‰€æœ‰æ•°æ® â†’ Node1 vs Node2

  åˆ†è£‚ 2:      Node1 â†’ Node3 vs Node4ï¼ˆè´ªå¿ƒé€‰æ‹© Gain æœ€å¤§ï¼‰

  åˆ†è£‚ 3:      Node3 â†’ Node5 vs Node6

  åˆ†è£‚ 4:      Node2 â†’ Node7 vs Node8ï¼ˆç¬¬äºŒè´ªå¿ƒï¼‰

  ç‰¹ç‚¹ï¼š
    âœ“ æ¯æ¬¡åˆ†è£‚éƒ½é€‰æ‹© Gain æœ€å¤§çš„ï¼ˆè´ªå¿ƒæœ€ä¼˜ï¼‰
    âœ“ æ ‘ä¸å¯¹ç§°ï¼Œä½†ä¼˜åŒ–å……åˆ†
    âœ— å®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆéœ€è¦æ›´å¼ºçš„æ­£åˆ™åŒ–ï¼‰
    âœ— å•çº¿ç¨‹æ„å»ºï¼ˆä¸å¦‚ XGBoost å¹¶è¡Œï¼‰
```

**ä½•æ—¶é€‰æ‹©å“ªä¸ª**ï¼š

```python
# XGBoostï¼šç²¾åº¦ç¬¬ä¸€ï¼Œä¸æ€¥ç€å¿«
if sample_size < 100000 and accuracy_critical:
    use_xgboost()

# LightGBMï¼šé€Ÿåº¦ç¬¬ä¸€ï¼Œæ•°æ®é‡å¤§
if sample_size > 1000000 or memory_limited:
    use_lightgbm()

# å®æˆ˜å»ºè®®ï¼š
# - Kaggle ç«èµ›ï¼šXGBoostï¼ˆç²¾åº¦ç«äº‰æ¿€çƒˆï¼‰
# - ç”Ÿäº§ç¯å¢ƒï¼šLightGBMï¼ˆå¿«é€Ÿè¿­ä»£ï¼Œæ¨¡å‹å¤šï¼‰
# - ç ”ç©¶è®ºæ–‡ï¼šXGBoostï¼ˆæ›´å¯ä¿¡ï¼‰
```

> [!question] Q5: å¦‚ä½•é˜²æ­¢ XGBoost è¿‡æ‹Ÿåˆï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šå¤šå±‚é˜²çº¿ï¼ˆæ­£åˆ™åŒ–å‚æ•° + é‡‡æ · + æ—©åœ + ç‰¹å¾å·¥ç¨‹ï¼‰

```python
# é˜²çº¿1ï¼šæ­£åˆ™åŒ–ï¼ˆå‚æ•°ï¼‰
clf = XGBClassifier(
    reg_alpha=0.1,          # L1 æ­£åˆ™åŒ–ï¼ˆç¨€ç–ï¼‰
    reg_lambda=1.0,         # L2 æ­£åˆ™åŒ–ï¼ˆå¹³æ»‘ï¼‰
    gamma=1,                # åˆ†è£‚é˜ˆå€¼
    min_child_weight=5,     # å¶å­æœ€å°æ ·æœ¬æ•°
    max_depth=5,            # æ ‘æ·±åº¦é™åˆ¶
)

# é˜²çº¿2ï¼šé‡‡æ ·ï¼ˆéšæœºåŒ–ï¼‰
clf = XGBClassifier(
    subsample=0.8,          # è¡Œé‡‡æ ·ï¼šé˜²æ­¢å¯¹æ ·æœ¬è¿‡æ‹Ÿåˆ
    colsample_bytree=0.8,   # åˆ—é‡‡æ ·ï¼šé˜²æ­¢å¯¹ç‰¹å¾è¿‡æ‹Ÿåˆ
    colsample_bylevel=0.8,  # æ¯å±‚åˆ—é‡‡æ ·ï¼šæ›´å¤šéšæœº
)

# é˜²çº¿3ï¼šå­¦ä¹ é€Ÿåº¦
clf = XGBClassifier(
    learning_rate=0.05,     # å°å­¦ä¹ ç‡ï¼Œå¤šæ£µæ ‘
    n_estimators=1000,      # ç”¨å¤šæ£µæ ‘ï¼Œä½†æ¯æ£µè´¡çŒ®å°
    early_stopping_rounds=10,  # æ—©åœï¼ˆé˜²æ­¢æ— é™å¢é•¿ï¼‰
)

# é˜²çº¿4ï¼šäº¤å‰éªŒè¯ + æ—©åœ
clf.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=10,  # å¦‚æœéªŒè¯é›†ç²¾åº¦ä¸å†æå‡ï¼Œåœæ­¢
    verbose=100
)

# é˜²çº¿5ï¼šç‰¹å¾å·¥ç¨‹
# ç§»é™¤ï¼š
#   - é«˜ç›¸å…³æ€§ç‰¹å¾ï¼ˆå¤šé‡å…±çº¿æ€§ï¼‰
#   - æ— å…³ç‰¹å¾ï¼ˆåŠ å™ªéŸ³ï¼‰
#   - å¼‚å¸¸å€¼ï¼ˆç”¨é²æ£’ç»Ÿè®¡å¤„ç†ï¼‰

# æœ€ç»ˆæ£€æŸ¥ï¼š
print(f"è®­ç»ƒç²¾åº¦: {clf.score(X_train, y_train):.4f}")
print(f"éªŒè¯ç²¾åº¦: {clf.score(X_val, y_val):.4f}")
print(f"å·®å¼‚: {abs(train_acc - val_acc):.4f}")

# åˆ¤æ–­ï¼š
# - å·®å¼‚ < 0.01 âœ“ è‰¯å¥½æ³›åŒ–
# - å·®å¼‚ 0.01-0.05 âœ“ å¯æ¥å—
# - å·®å¼‚ > 0.05 âœ— è¿‡æ‹Ÿåˆï¼Œéœ€è¦æ›´å¼ºæ­£åˆ™åŒ–
```

> [!question] Q6: XGBoost ç‰¹å¾é‡è¦æ€§æ€ä¹ˆç†è§£ï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šä¸‰ç§åº¦é‡ï¼ˆweight / gain / coverï¼‰ï¼Œåˆ†åˆ«ä»£è¡¨é¢‘ç‡ã€åˆ†è£‚æ”¶ç›Šã€è¦†ç›–æ ·æœ¬æ•°

```python
import xgboost as xgb
import matplotlib.pyplot as plt

clf = XGBClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# æ–¹æ³•1ï¼šweightï¼ˆé¢‘ç‡ï¼‰
# ç‰¹å¾åœ¨æ‰€æœ‰æ ‘ä¸­è¢«ç”¨æ¥åˆ†è£‚çš„æ¬¡æ•°
importance_weight = clf.get_booster().get_score(importance_type='weight')

# æ–¹æ³•2ï¼šgainï¼ˆåˆ†è£‚æ”¶ç›Šï¼‰
# ç‰¹å¾åˆ†è£‚æ—¶å¹³å‡é™ä½çš„æŸå¤±
importance_gain = clf.get_booster().get_score(importance_type='gain')

# æ–¹æ³•3ï¼šcoverï¼ˆè¦†ç›–åº¦ï¼‰
# ç‰¹å¾åˆ†è£‚æ—¶æ¶‰åŠçš„æ ·æœ¬æ•°
importance_cover = clf.get_booster().get_score(importance_type='cover')

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

xgb.plot_importance(clf, importance_type='weight', ax=axes[0])
axes[0].set_title('Weightï¼ˆé¢‘ç‡ï¼‰')

xgb.plot_importance(clf, importance_type='gain', ax=axes[1])
axes[1].set_title('Gainï¼ˆåˆ†è£‚æ”¶ç›Šï¼‰')

xgb.plot_importance(clf, importance_type='cover', ax=axes[2])
axes[2].set_title('Coverï¼ˆè¦†ç›–åº¦ï¼‰')

plt.tight_layout()
plt.show()

# è§£è¯»ï¼š
# weight é«˜ï¼šç‰¹å¾ç»å¸¸è¢«ç”¨æ¥åˆ†è£‚ï¼ˆé‡è¦ï¼‰
# gain é«˜ï¼šç‰¹å¾åˆ†è£‚æ—¶å¤§å¹…é™ä½æŸå¤±ï¼ˆå…³é”®ç‰¹å¾ï¼‰
# cover é«˜ï¼šç‰¹å¾åˆ†è£‚æ—¶æ¶‰åŠçš„æ ·æœ¬å¤šï¼ˆå½±å“èŒƒå›´å¤§ï¼‰

# å®é™…åº”ç”¨ï¼š
important_features = importance_gain.sort_values(ascending=False).head(10)
print("Top 10 é‡è¦ç‰¹å¾ï¼ˆæŒ‰ Gainï¼‰:")
for idx, (feature, gain) in enumerate(important_features.items(), 1):
    print(f"  {idx}. {feature}: {gain:.4f}")
```

---

## æ€»ç»“

### ğŸ“Œ æ ¸å¿ƒçŸ¥è¯†ç‚¹

- **XGBoost = Gradient Boosting + äºŒé˜¶å¯¼æ•° + æ­£åˆ™åŒ– + å·¥ç¨‹ä¼˜åŒ–**
- **äºŒé˜¶æ³°å‹’å±•å¼€**æ˜¯æ ¸å¿ƒåˆ›æ–°ï¼Œæä¾›æ›²ç‡ä¿¡æ¯ï¼ŒåŠ é€Ÿæ”¶æ•›
- **Gain è®¡ç®—**ï¼š$$\text{Gain} = -\frac{1}{2} \frac{(\sum g_i)^2}{\sum h_i + \lambda} - \gamma$$
- **è¶…å‚æ•°è°ƒä¼˜ä¼˜å…ˆçº§**ï¼šlearning_rate > max_depth > subsample > colsample > reg_lambda
- **é˜²è¿‡æ‹Ÿåˆ**ï¼šæ­£åˆ™åŒ– + é‡‡æ · + æ—©åœ + ç‰¹å¾å·¥ç¨‹
- **é€‚ç”¨åœºæ™¯**ï¼šä¸­ç­‰è§„æ¨¡è¡¨æ ¼æ•°æ®ï¼Œç²¾åº¦ä¼˜å…ˆ

### ğŸ¯ é¢è¯•é«˜é¢‘é—®é¢˜

1. XGBoost ä¸ºä»€ä¹ˆå¿«ï¼Ÿâ†’ äºŒé˜¶å¯¼æ•° + å¹¶è¡ŒåŒ–
2. ç¼ºå¤±å€¼æ€ä¹ˆå¤„ç†ï¼Ÿâ†’ å­¦ä¹ æœ€ä¼˜æ–¹å‘
3. å¦‚ä½•é˜²è¿‡æ‹Ÿåˆï¼Ÿâ†’ å¤šå±‚é˜²çº¿
4. vs LightGBMï¼Ÿâ†’ æ ‘æ„å»ºç­–ç•¥ä¸åŒ
5. ç‰¹å¾é‡è¦æ€§ï¼Ÿâ†’ weight/gain/cover ä¸‰ç§åº¦é‡

### âœ… å®æˆ˜å»ºè®®

```python
# æ ‡å‡†æ¨¡æ¿
clf = XGBClassifier(
    # æ ‘çš„å¤æ‚åº¦
    max_depth=5,
    min_child_weight=5,
    gamma=1,

    # é‡‡æ ·
    subsample=0.8,
    colsample_bytree=0.8,

    # æ­£åˆ™åŒ–
    reg_alpha=0.1,
    reg_lambda=1.0,

    # å­¦ä¹ 
    learning_rate=0.05,
    n_estimators=500,

    # å…¶ä»–
    objective='binary:logistic',
    random_state=42,
    n_jobs=-1
)

# è®­ç»ƒ + æ—©åœ
clf.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=10,
    verbose=100
)

# éªŒè¯
val_score = roc_auc_score(y_val, clf.predict_proba(X_val)[:, 1])
test_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])

print(f"éªŒè¯ AUC: {val_score:.4f}")
print(f"æµ‹è¯• AUC: {test_score:.4f}")
print(f"æ³›åŒ–å·®å¼‚: {abs(val_score - test_score):.4f}")
```

---

**å‚è€ƒæ–‡çŒ®**ï¼š
- Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. KDD 2016.
- XGBoost å®˜æ–¹æ–‡æ¡£ï¼šhttps://xgboost.readthedocs.io/
- Kaggle ç«èµ›æ–¹æ¡ˆé›†ï¼šhttps://www.kaggle.com/

**å»ºè®®å­¦ä¹ è·¯å¾„**ï¼š
1. ç†è§£ Boosting åŸºæœ¬åŸç†
2. æŒæ¡ä¸€é˜¶æ¢¯åº¦ï¼ˆGBDTï¼‰
3. æ·±å…¥äºŒé˜¶æ¢¯åº¦ï¼ˆXGBoostï¼‰
4. å®æˆ˜è°ƒå‚ï¼ˆGridSearch/Bayesian Optï¼‰
5. å¯¹æ¯”å…¶ä»–ç®—æ³•ï¼ˆLightGBM/CatBoostï¼‰
