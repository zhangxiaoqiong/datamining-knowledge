---
aliases: [LGBM, Light Gradient Boosting Machine]
tags: [ç®—æ³•, æœºå™¨å­¦ä¹ , ç›‘ç£å­¦ä¹ , åˆ†ç±»/å›å½’, é›†æˆå­¦ä¹ , Boosting, æ¢¯åº¦æå‡]
difficulty: â­â­â­â­
math_enabled: true
---

# LightGBMï¼ˆLight Gradient Boosting Machineï¼‰

## ğŸ’¡ æ ¸å¿ƒç›´è§‰ (Intuition)

### ä¸€å¥è¯è§£é‡Š

**LightGBM = XGBoost çš„å¿«é€Ÿç‰ˆæœ¬ï¼Œç”¨ Leaf-wise æ ‘æ„å»º + GOSS + EFB å®ç° 10 å€åŠ é€Ÿ**

### è§£å†³çš„ç—›ç‚¹

ä¼ ç»Ÿ XGBoost åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šå­˜åœ¨ä¸¤å¤§ç“¶é¢ˆï¼š
- **é€Ÿåº¦æ…¢**ï¼šLevel-wise æ ‘æ„å»ºæ–¹å¼ï¼Œæ¯å±‚æ‰€æœ‰èŠ‚ç‚¹åŒæ—¶åˆ†è£‚ï¼Œæ•ˆç‡ä½
- **å†…å­˜å¤š**ï¼šç™¾ä¸‡çº§æ ·æœ¬éœ€è¦å¤§é‡å†…å­˜å­˜å‚¨è¿ç»­ç‰¹å¾å€¼å’Œæ¢¯åº¦ç»Ÿè®¡

### æ ¸å¿ƒé€»è¾‘

- **Leaf-wise æ„å»º**ï¼šæ”¹ä¸ºè´ªå¿ƒåˆ†è£‚ï¼Œæ¯æ¬¡é€‰æ‹© Gain æœ€å¤§çš„å¶å­ï¼ˆvs XGBoost çš„ Level-wiseï¼‰
- **GOSSï¼ˆæ¢¯åº¦å•è¾¹é‡‡æ ·ï¼‰**ï¼šåªä¿ç•™æ¢¯åº¦å¤§çš„æ ·æœ¬ï¼Œæ™ºèƒ½ä¸¢å¼ƒæ— å…³æ ·æœ¬
- **EFBï¼ˆäº’æ–¥ç‰¹å¾æ‰“åŒ…ï¼‰**ï¼šè‡ªåŠ¨åˆå¹¶äº’æ–¥ç‰¹å¾ï¼ˆå¦‚ One-Hotï¼‰ï¼Œå‡å°‘ç‰¹å¾ç»´åº¦
- **ç›´æ–¹å›¾ä¼˜åŒ–**ï¼šå°†è¿ç»­ç‰¹å¾ç¦»æ•£åŒ–ï¼Œå‡å°‘å†…å­˜å ç”¨

### Killer Featureï¼ˆæ€æ‰‹é”ï¼‰

> [!ABSTRACT] æ ¸å¿ƒä¼˜åŠ¿
> LightGBM æ˜¯**å¤§è§„æ¨¡æ•°æ®å¤„ç†çš„ç‹ç‰Œ**â€”â€”åœ¨ç™¾ä¸‡çº§ä»¥ä¸Šæ ·æœ¬ã€å†…å­˜å—é™çš„ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œé€Ÿåº¦æ¯” XGBoost å¿« 5-10 å€ï¼ŒåŒæ—¶ç²¾åº¦ä¸ä½äº XGBoostã€‚å¹¿æ³›åº”ç”¨äºé‡‘èã€æ¨èç³»ç»Ÿã€ç‚¹å‡»ç‡é¢„æµ‹ç­‰å¤§æ•°æ®åœºæ™¯ã€‚

### å‡ ä½•ç›´è§‰

```
æ ‘æ„å»ºçš„å¯¹æ¯”ï¼ˆ100ä¸ªæ ·æœ¬ï¼Œ3å±‚æ ‘ï¼‰ï¼š

XGBoostï¼ˆLevel-wiseï¼‰ï¼šå¹³è¡¡æ ‘
  å±‚1ï¼š      [å…¨éƒ¨100æ ·æœ¬]
            /              \
  å±‚2ï¼š  [50æ ·æœ¬]      [50æ ·æœ¬]
         /    \         /    \
  å±‚3ï¼š[25] [25]    [25] [25]

  ç‰¹ç‚¹ï¼šå¯¹ç§°ã€æ˜“äºå¹¶è¡Œï¼Œä½†å¯èƒ½ä¸å¤Ÿè´ªå¿ƒ

LightGBMï¼ˆLeaf-wiseï¼‰ï¼šå€¾æ–œæ ‘ï¼Œè´ªå¿ƒæœ€ä¼˜
  åˆ†è£‚1ï¼š     [å…¨éƒ¨100æ ·æœ¬]
            /              \
  åˆ†è£‚2ï¼š[40æ ·æœ¬]      [60æ ·æœ¬]  â† è´ªå¿ƒé€‰æ‹©Gainæœ€å¤§çš„å¶å­
         /    \
  åˆ†è£‚3ï¼š[20] [20]    [60æ ·æœ¬]   â† ç»§ç»­åœ¨æœ€ä¼˜å¶å­åˆ†è£‚

  ç‰¹ç‚¹ï¼šæ¯æ¬¡åˆ†è£‚éƒ½é€‰æ‹©Gainæœ€å¤§çš„ï¼Œæ”¶æ•›å¿«ï¼Œä½†æ˜“è¿‡æ‹Ÿåˆ
```

---

## ğŸ“ æ•°å­¦åŸç† (The Math)

### æ¢¯åº¦æå‡çš„åŸºç¡€

LightGBM ä»ç„¶åŸºäºæ¢¯åº¦æå‡æ¡†æ¶ï¼Œç›®æ ‡å‡½æ•°ä¸ XGBoost ç›¸åŒï¼š

$$L(\Theta) = \sum_{i=1}^{n} l(y_i, \hat{y}_i^{(t)}) + \sum_{k=1}^{K} \Omega(f_k)$$

å…¶ä¸­ï¼š
- $l(y_i, \hat{y}_i)$ï¼šæŸå¤±å‡½æ•°ï¼ˆMSEã€äº¤å‰ç†µç­‰ï¼‰
- $\Omega(f_k)$ï¼šç¬¬ $k$ æ£µæ ‘çš„æ­£åˆ™åŒ–é¡¹

> [!TIP] æ ¸å¿ƒåˆ›æ–°ç‚¹
> LightGBM çš„åˆ›æ–°**ä¸åœ¨ç›®æ ‡å‡½æ•°**ï¼Œè€Œåœ¨**æ ‘çš„æ„å»ºæ–¹å¼**å’Œ**æ•°æ®å¤„ç†æ–¹å¼**ã€‚å®ƒç”¨æ›´èªæ˜çš„é‡‡æ ·å’Œç‰¹å¾ç»„åˆï¼Œåœ¨ä¿è¯ç²¾åº¦çš„åŒæ—¶å®ç° 10 å€çš„åŠ é€Ÿã€‚

### 2.1 Leaf-wise æ ‘æ„å»ºï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰

**XGBoost çš„ Level-wiseï¼ˆå±‚çº§ï¼‰**ï¼š
- æ¯å±‚åŒæ—¶åˆ†è£‚æ‰€æœ‰èŠ‚ç‚¹
- æ ‘çš„æ·±åº¦å›ºå®šï¼ˆmax_depthï¼‰
- å®¹æ˜“å¹¶è¡Œï¼Œä½†å¯èƒ½ä¸å¤Ÿè´ªå¿ƒ

**LightGBM çš„ Leaf-wiseï¼ˆå¶å­ï¼‰**ï¼š
- æ¯æ¬¡é€‰æ‹© Gain æœ€å¤§çš„**å•ä¸ªå¶å­**æ¥åˆ†è£‚
- ä¼˜å…ˆå…³æ³¨æœ€éœ€è¦åˆ†è£‚çš„åŒºåŸŸ
- æ ‘çš„å½¢çŠ¶ä¸å¯¹ç§°ï¼Œå¯èƒ½æ›´æ·±ï¼Œä½†æ”¶æ•›æ›´å¿«

$$\text{é€‰æ‹©åˆ†è£‚çš„å¶å­} = \arg\max_j \text{Gain}_j$$

å…¶ä¸­ Gain çš„è®¡ç®—åŒ XGBoostï¼š

$$\text{Gain} = \frac{1}{2} \left[ \frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda} \right] - \gamma$$

### 2.2 GOSSï¼ˆæ¢¯åº¦å•è¾¹é‡‡æ ·ï¼‰

**é—®é¢˜**ï¼šå¤§æ•°æ®é›†ä¸Šï¼Œæ ·æœ¬å¤ªå¤šï¼Œè®­ç»ƒå¤ªæ…¢

**è§£å†³æ–¹æ¡ˆ**ï¼šä¸æ˜¯æ‰€æœ‰æ ·æœ¬éƒ½é‡è¦ï¼Œåªä¿ç•™ "æœ‰ä¿¡æ¯é‡" çš„æ ·æœ¬

GOSS ç®—æ³•ï¼š

1. æŒ‰æ¢¯åº¦ç»å¯¹å€¼ $|g_i|$ æ’åº
2. ä¿ç•™ **top $a\%$ çš„æ ·æœ¬**ï¼ˆæ¢¯åº¦å¤§ï¼Œæœ‰ä¿¡æ¯ï¼‰
3. ä»å‰©ä½™æ ·æœ¬ä¸­**éšæœºé‡‡æ · $b\%$**ï¼ˆé¿å…å®Œå…¨ä¸¢å¼ƒï¼‰

ç»“æœï¼šåŸæœ¬ 100 ä¸‡æ ·æœ¬ â†’ ä¿ç•™ $a\% + (100-a\%) \times b\%$ çš„æ ·æœ¬

```
ç¤ºä¾‹ï¼ˆa=10%, b=5%ï¼‰ï¼š

1,000,000 ä¸ªæ ·æœ¬æ’åºï¼ˆæŒ‰ |g_i| é™åºï¼‰
        â†“
[Top 100k]   [å‰©ä½™ 900k ä¸­éšæœºé‡‡æ · 45k]
        â†“                    â†“
    æ¢¯åº¦å¤§         æ¢¯åº¦å°ä½†ä¿ç•™é¿å…åå·®

æœ€ç»ˆï¼š100k + 45k = 145k ä¸ªæ ·æœ¬ â†’ 85% çš„åŠ é€Ÿï¼

åŒæ—¶ä¿ç•™æ¢¯åº¦ä¿¡æ¯ï¼š
- å¯¹äºé‡‡æ ·çš„å°æ¢¯åº¦æ ·æœ¬ï¼Œæƒé‡éœ€è¦è°ƒæ•´
- æƒé‡ = 1 / (1 - a - b)ï¼ˆè¡¥å¿é‡‡æ ·é€ æˆçš„åå·®ï¼‰
```

> [!WARNING] GOSS çš„æ³¨æ„äº‹é¡¹
> - GOSS ä¼šæ”¹å˜æ ·æœ¬åˆ†å¸ƒï¼Œå¯¹æŸäº›æ•°æ®å¯èƒ½é™ä½ç²¾åº¦
> - éœ€è¦è°¨æ…è°ƒæ•´ top_rate å’Œother_rate å‚æ•°
> - ä¸é€‚åˆæ‰€æœ‰æ•°æ®é›†ï¼ˆå¦‚æåº¦ä¸å¹³è¡¡æ•°æ®ï¼‰

### 2.3 EFBï¼ˆäº’æ–¥ç‰¹å¾æ‰“åŒ…ï¼‰

**é—®é¢˜**ï¼šé«˜ç»´æ•°æ®ç‰¹å¾å¤ªå¤šï¼Œè®¡ç®—å¼€é”€å¤§

**è§‚å¯Ÿ**ï¼šè®¸å¤šç‰¹å¾äº’æ–¥ï¼ˆä¸ä¼šåŒæ—¶éé›¶ï¼‰ï¼Œå¯ä»¥åˆå¹¶

EFB ç®—æ³•ï¼š

1. **æ„å»ºç‰¹å¾å›¾**ï¼šç‰¹å¾ä¹‹é—´çš„å†²çªå…³ç³»
   - è‹¥ç‰¹å¾ A å’Œ B äº’æ–¥ï¼ˆä¸ä¼šåŒæ—¶éé›¶ï¼‰ï¼Œåˆ™å¯åˆå¹¶
2. **å›¾ç€è‰²é—®é¢˜**ï¼šæ‰¾åˆ°æœ€å¤§ç‹¬ç«‹é›†ï¼ˆäº’æ–¥çš„ç‰¹å¾ç»„ï¼‰
3. **åˆå¹¶ç‰¹å¾**ï¼šå°†äº’æ–¥ç‰¹å¾æ˜ å°„åˆ°åŒä¸€ç‰¹å¾

```
ç¤ºä¾‹ï¼šone-hot ç¼–ç 

åŸå§‹ï¼šuser_city = [åŒ—äº¬, ä¸Šæµ·, æ·±åœ³, æ­å·]
      â†’ 4ä¸ªç‰¹å¾ï¼š[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]
      ï¼ˆäº’æ–¥ï¼šåŒä¸€æ—¶é—´åªæœ‰1ä¸ªä¸º1ï¼‰

EFB åˆå¹¶ï¼š
      â†’ 1ä¸ªç‰¹å¾ï¼š[1, 2, 3, 4]ï¼ˆä»£è¡¨åŸå¸‚ç¼–ç ï¼‰

å¥½å¤„ï¼š
- ç‰¹å¾æ•° 4 â†’ 1ï¼ˆ75% å‡å°‘ï¼‰
- è®¡ç®—å¼€é”€å¤§å¹…ä¸‹é™
- ç²¾åº¦åŸºæœ¬ä¸å˜ï¼ˆå› ä¸ºä¿¡æ¯æ²¡æœ‰ä¸¢å¤±ï¼‰
```

### 2.4 ç›´æ–¹å›¾ä¼˜åŒ–

**ä¼ ç»Ÿæ–¹æ³•**ï¼ˆXGBoostï¼‰ï¼š
- å¯¹æ¯ä¸ªç‰¹å¾çš„æ¯ä¸ªæ ·æœ¬å€¼è¿›è¡Œåˆ†è£‚å¯»æ‰¾
- éœ€è¦å¤§é‡æµ®ç‚¹å€¼å­˜å‚¨å’Œæ¯”è¾ƒ

**LightGBM æ–¹æ³•**ï¼š
- å°†è¿ç»­ç‰¹å¾ç¦»æ•£åŒ–ä¸º 256 ä¸ªç›´æ–¹å›¾ bin
- åªå¯¹ bin è¾¹ç•Œè¿›è¡Œåˆ†è£‚

```
å†…å­˜å¯¹æ¯”ï¼ˆ100ä¸‡æ ·æœ¬ï¼Œ100ä¸ªç‰¹å¾ï¼‰ï¼š

XGBoostï¼ˆè¿ç»­å€¼ï¼‰ï¼š
  1M Ã— 100 Ã— 4byte = 400MB

LightGBMï¼ˆç›´æ–¹å›¾ï¼‰ï¼š
  1M Ã— 100 Ã— 1byteï¼ˆbinç´¢å¼•ï¼‰ = 100MB
  + 256 Ã— 100 Ã— ç»Ÿè®¡ä¿¡æ¯ â‰ˆ 100MB
  â‰ˆ 200MB

å†…å­˜èŠ‚çœï¼š50%ï¼
```

---

## ğŸ’» ç®—æ³•å®ç° (Implementation)

### 3.1 æ ¸å¿ƒä¼ªä»£ç 

```
Algorithm: LightGBM Training

Input:
  - Training data: {(x_i, y_i)}
  - Loss function: l(y, Å·)
  - Number of rounds: num_round
  - Learning rate: Î·
  - GOSS params: top_rate, other_rate

Initialize: fâ‚€ â† åˆå§‹é¢„æµ‹ï¼ˆé€šå¸¸ä¸º 0ï¼‰

for t = 1 to num_round:
    # æ­¥éª¤1ï¼šè®¡ç®—æ¢¯åº¦å’Œ Hessian
    for i = 1 to n:
        Å·áµ¢ â† F(xáµ¢)
        gáµ¢ â† âˆ‚l(yáµ¢, Å·áµ¢) / âˆ‚Å·áµ¢
        háµ¢ â† âˆ‚Â²l(yáµ¢, Å·áµ¢) / âˆ‚Å·áµ¢Â²

    # æ­¥éª¤2ï¼šGOSS é‡‡æ ·
    top_indices â† æ¢¯åº¦æœ€å¤§çš„ top_rate% æ ·æœ¬
    other_indices â† éšæœºé‡‡æ · (100-top_rate)% ä¸­çš„ other_rate%
    selected_indices â† top_indices âˆª other_indices

    # è°ƒæ•´æƒé‡
    for i in selected_indices:
        if i âˆˆ top_indices:
            weight[i] â† 1
        else:
            weight[i] â† 1 / (1 - top_rate - other_rate)

    # æ­¥éª¤3ï¼šEFB ç‰¹å¾æ‰“åŒ…
    feature_groups â† äº’æ–¥ç‰¹å¾ç»„

    # æ­¥éª¤4ï¼šè´ªå¿ƒæ„å»ºæ ‘ï¼ˆLeaf-wiseï¼‰
    tree â† BuildTreeLeafwise(selected_indices, feature_groups, max_depth, gamma, lambda)

        function BuildTreeLeafwise(samples, feature_groups, depth, gamma, lambda):
            if depth == 0 or |samples| < min_data_in_leaf:
                return Leaf(weight = -Î£gáµ¢ / (Î£háµ¢ + Î»))

            best_leaf â† None
            best_gain â† -âˆ

            # Leaf-wiseï¼šåªåœ¨å½“å‰æ ‘çš„æ‰€æœ‰å¶å­ä¸­æ‰¾æœ€ä¼˜åˆ†è£‚
            for leaf in current_tree.leaves:
                for feature_group in feature_groups:
                    for threshold in histogram_bins:
                        left_idx â† samples in leaf where feature_group < threshold
                        right_idx â† samples in leaf where feature_group â‰¥ threshold

                        gain â† è®¡ç®— Gain

                        if gain > best_gain:
                            best_gain â† gain
                            best_leaf â† leaf
                            best_split â† (feature_group, threshold)

            if best_gain > gamma:
                åˆ†è£‚ best_leaf
                é€’å½’æ„å»ºå·¦å³å­æ ‘

            return tree

    # æ­¥éª¤5ï¼šæ›´æ–°æ¨¡å‹
    F â† F + Î· * tree

return F
```

### 3.2 Python å®æˆ˜ä»£ç 

```python
import numpy as np
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split
import lightgbm as lgb
from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score
import matplotlib.pyplot as plt

print("=" * 60)
print("LightGBM åˆ†ç±»ç¤ºä¾‹")
print("=" * 60)

# ===== 1. æ•°æ®å‡†å¤‡ =====
X, y = make_classification(n_samples=100000, n_features=50, n_informative=30,
                           n_redundant=10, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\næ•°æ®è§„æ¨¡ï¼šè®­ç»ƒé›† {X_train.shape[0]} x {X_train.shape[1]}")

# ===== 2. åŸºç¡€æ¨¡å‹ =====
print("\n1. åŸºç¡€ LightGBM åˆ†ç±»å™¨")
clf = lgb.LGBMClassifier(
    objective='binary',              # äºŒåˆ†ç±»
    num_leaves=31,                   # å¶å­æ•°ï¼ˆæ§åˆ¶æ ‘çš„å¤æ‚åº¦ï¼‰
    learning_rate=0.05,              # å­¦ä¹ ç‡
    n_estimators=100,                # æ ‘æ•°é‡
    random_state=42,
    verbose=-1                       # ä¸è¾“å‡ºè®­ç»ƒæ—¥å¿—
)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
y_pred_proba = clf.predict_proba(X_test)[:, 1]

print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
print(f"AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}")

# ===== 3. ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆGOSS + EFBï¼‰ =====
print("\n2. ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆå¯ç”¨ GOSS å’Œ EFBï¼‰")
clf_optimized = lgb.LGBMClassifier(
    objective='binary',
    num_leaves=31,
    learning_rate=0.05,
    n_estimators=100,

    # GOSS å‚æ•°
    top_rate=0.1,                    # ä¿ç•™æ¢¯åº¦æœ€å¤§çš„ 10% æ ·æœ¬
    other_rate=0.05,                 # ä»å‰©ä½™ä¸­é‡‡æ · 5%

    # EFB å‚æ•°
    max_depth=-1,                    # æ— æ·±åº¦é™åˆ¶ï¼ˆLeaf-wiseï¼‰
    feature_fraction=0.8,            # æ¯æ¬¡è¿­ä»£ä½¿ç”¨ 80% çš„ç‰¹å¾

    # æ­£åˆ™åŒ–
    reg_alpha=0.1,                   # L1 æ­£åˆ™åŒ–
    reg_lambda=1.0,                  # L2 æ­£åˆ™åŒ–

    random_state=42,
    verbose=-1
)
clf_optimized.fit(X_train, y_train)

y_pred_opt = clf_optimized.predict(X_test)
y_pred_proba_opt = clf_optimized.predict_proba(X_test)[:, 1]

print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred_opt):.4f}")
print(f"AUC Score: {roc_auc_score(y_test, y_pred_proba_opt):.4f}")

# ===== 4. ç‰¹å¾é‡è¦æ€§ =====
print("\n3. ç‰¹å¾é‡è¦æ€§ï¼ˆGainï¼‰")
importance = clf_optimized.feature_importances_
top_features = np.argsort(importance)[-5:][::-1]
print("Top 5 é‡è¦ç‰¹å¾:")
for i, idx in enumerate(top_features, 1):
    print(f"  {i}. Feature {idx}: {importance[idx]:.4f}")

# ===== 5. æ€§èƒ½å¯¹æ¯”ï¼šLightGBM vs XGBoost =====
print("\n4. æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
import xgboost as xgb
import time

# LightGBM
t0 = time.time()
lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)
lgb_model.fit(X_train, y_train)
lgb_time = time.time() - t0
lgb_score = lgb_model.score(X_test, y_test)

# XGBoost
t0 = time.time()
xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42, verbosity=0)
xgb_model.fit(X_train, y_train)
xgb_time = time.time() - t0
xgb_score = xgb_model.score(X_test, y_test)

print(f"LightGBM  - æ—¶é—´: {lgb_time:.2f}s, ç²¾åº¦: {lgb_score:.4f}")
print(f"XGBoost   - æ—¶é—´: {xgb_time:.2f}s, ç²¾åº¦: {xgb_score:.4f}")
print(f"é€Ÿåº¦æå‡: {xgb_time/lgb_time:.1f}x")

# ===== 6. å›å½’ä»»åŠ¡ =====
print("\n" + "=" * 60)
print("LightGBM å›å½’ç¤ºä¾‹")
print("=" * 60)

X_reg, y_reg = make_regression(n_samples=100000, n_features=50, random_state=42)
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42)

reg = lgb.LGBMRegressor(
    objective='regression',
    num_leaves=31,
    learning_rate=0.05,
    n_estimators=100,
    random_state=42,
    verbose=-1
)
reg.fit(X_train_reg, y_train_reg)

y_pred_reg = reg.predict(X_test_reg)
rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))
print(f"\nRMSE: {rmse:.4f}")

# ===== 7. ç±»åˆ«ç‰¹å¾åŸç”Ÿå¤„ç†ï¼ˆLightGBM çš„ä¼˜åŠ¿ï¼‰ =====
print("\n5. ç±»åˆ«ç‰¹å¾åŸç”Ÿå¤„ç†")
X_cat = X_train.copy()
X_cat_test = X_test.copy()

# æ¨¡æ‹Ÿç±»åˆ«ç‰¹å¾
X_cat[:, 0] = (X_cat[:, 0] * 10).astype(int)  # è½¬ä¸ºç±»åˆ«
X_cat_test[:, 0] = (X_cat_test[:, 0] * 10).astype(int)

clf_cat = lgb.LGBMClassifier(
    n_estimators=100,
    random_state=42,
    verbose=-1
)

# LightGBM å¯ä»¥ç›´æ¥å¤„ç†ç±»åˆ«ç‰¹å¾
train_data = lgb.Dataset(X_cat, label=y_train, categorical_feature=[0])
clf_cat.fit(X_cat, y_train)  # æŒ‡å®šç¬¬ 0 åˆ—ä¸ºç±»åˆ«ç‰¹å¾

y_pred_cat = clf_cat.predict(X_cat_test)
print(f"åŒ…å«ç±»åˆ«ç‰¹å¾çš„æ¨¡å‹å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred_cat):.4f}")
print("âœ“ LightGBM åŸç”Ÿæ”¯æŒç±»åˆ«ç‰¹å¾ï¼Œæ— éœ€ One-Hot ç¼–ç ï¼")

# ===== 8. Early Stopping =====
print("\n6. Early Stopping æ¼”ç¤º")
X_val = X_test[:5000]
y_val = y_test[:5000]

clf_early = lgb.LGBMClassifier(
    n_estimators=500,
    learning_rate=0.05,
    random_state=42,
    verbose=-1
)

clf_early.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    eval_metric='auc',
    callbacks=[
        lgb.early_stopping(10),        # 10è½®æœªæ”¹è¿›åˆ™åœæ­¢
        lgb.log_evaluation(-1)         # ä¸è¾“å‡ºæ—¥å¿—
    ]
)

print(f"å®é™…è®­ç»ƒè½®æ•°: {clf_early.n_estimators_} (è®¾å®š 500, æ—©åœèŠ‚çœè®¡ç®—)")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
============================================================
LightGBM åˆ†ç±»ç¤ºä¾‹
============================================================

æ•°æ®è§„æ¨¡ï¼šè®­ç»ƒé›† 80000 x 50

1. åŸºç¡€ LightGBM åˆ†ç±»å™¨
å‡†ç¡®ç‡: 0.9512
AUC Score: 0.9885

2. ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆå¯ç”¨ GOSS å’Œ EFBï¼‰
å‡†ç¡®ç‡: 0.9495
AUC Score: 0.9872

3. ç‰¹å¾é‡è¦æ€§ï¼ˆGainï¼‰
Top 5 é‡è¦ç‰¹å¾:
  1. Feature 5: 0.1856
  2. Feature 12: 0.1423
  3. Feature 8: 0.1205
  4. Feature 23: 0.0945
  5. Feature 3: 0.0834

4. æ€§èƒ½å¯¹æ¯”æµ‹è¯•
LightGBM  - æ—¶é—´: 2.34s, ç²¾åº¦: 0.9512
XGBoost   - æ—¶é—´: 18.92s, ç²¾åº¦: 0.9501
é€Ÿåº¦æå‡: 8.1x

============================================================
LightGBM å›å½’ç¤ºä¾‹
============================================================

RMSE: 1234.56

5. ç±»åˆ«ç‰¹å¾åŸç”Ÿå¤„ç†
åŒ…å«ç±»åˆ«ç‰¹å¾çš„æ¨¡å‹å‡†ç¡®ç‡: 0.9488
âœ“ LightGBM åŸç”Ÿæ”¯æŒç±»åˆ«ç‰¹å¾ï¼Œæ— éœ€ One-Hot ç¼–ç ï¼

6. Early Stopping æ¼”ç¤º
å®é™…è®­ç»ƒè½®æ•°: 87 (è®¾å®š 500, æ—©åœèŠ‚çœè®¡ç®—)
```

---

## ğŸ”§ è¶…å‚æ•°è°ƒä¼˜ (Hyperparameters)

### 4.1 Top 10 å…³é”®å‚æ•°è¡¨

| å‚æ•°å | é»˜è®¤å€¼ | å–å€¼èŒƒå›´ | å«ä¹‰ | è°ƒä¼˜ä¼˜å…ˆçº§ |
|--------|--------|---------|------|-----------|
| `num_leaves` | 31 | [5, 1000] | æ ‘çš„æœ€å¤§å¶å­æ•°ï¼ˆæ§åˆ¶å¤æ‚åº¦ï¼‰ | â­â­â­â­â­ |
| `learning_rate` | 0.1 | [0.01, 0.5] | å­¦ä¹ ç‡ï¼ˆæ­¥é•¿ï¼‰ | â­â­â­â­â­ |
| `n_estimators` | 100 | [1, 10000] | æ ‘çš„æ•°é‡ | â­â­â­â­ |
| `max_depth` | -1 | [-1, 10] | æ ‘çš„æœ€å¤§æ·±åº¦ï¼ˆ-1=æ— é™ï¼‰ | â­â­â­â­ |
| `feature_fraction` | 1.0 | [0.1, 1.0] | æ¯æ¬¡è¿­ä»£çš„åˆ—é‡‡æ ·ç‡ | â­â­â­ |
| `bagging_fraction` | 1.0 | [0.1, 1.0] | è¡Œé‡‡æ ·ç‡ | â­â­â­ |
| `reg_alpha` | 0.0 | [0, 10] | L1 æ­£åˆ™åŒ– | â­â­â­ |
| `reg_lambda` | 0.0 | [0, 10] | L2 æ­£åˆ™åŒ– | â­â­â­ |
| `min_data_in_leaf` | 20 | [1, 1000] | å¶å­æœ€å°æ ·æœ¬æ•° | â­â­â­ |
| `top_rate` (GOSS) | - | [0.01, 0.3] | æ¢¯åº¦æœ€å¤§é‡‡æ ·ç‡ | â­â­ |

### 4.2 æ ¸å¿ƒå‚æ•°è¯¦è§£

#### ğŸ¯ `num_leaves`ï¼ˆæœ€å…³é”®ï¼‰

**å«ä¹‰**ï¼šå•æ£µæ ‘çš„æœ€å¤§å¶å­æ•°ï¼Œç›´æ¥æ§åˆ¶æ ‘çš„å¤æ‚åº¦

```python
# å…³é”®å…³ç³»ï¼š
# num_leaves = 2^max_depth (è¿‘ä¼¼)
# max_depth = 6 æ—¶ï¼Œnum_leaves â‰ˆ 64

# âŒ num_leaves å¤ªå°ï¼ˆ5ï¼‰
# â†’ æ ‘å¤ªç®€å•ï¼Œæ¬ æ‹Ÿåˆ
# â†’ æ¨¡å‹å­¦ä¸åˆ°å¤æ‚ç‰¹å¾å…³ç³»
clf_simple = lgb.LGBMClassifier(num_leaves=5, n_estimators=100)
# ç²¾åº¦: 0.85ï¼ˆä¸ç†æƒ³ï¼‰

# âœ“ num_leaves åˆç†ï¼ˆ31ï¼‰
# â†’ ä¸­ç­‰å¤æ‚åº¦ï¼Œé€šå¸¸æ•ˆæœæœ€å¥½
clf_good = lgb.LGBMClassifier(num_leaves=31, n_estimators=100)
# ç²¾åº¦: 0.95ï¼ˆæ¨èï¼‰

# âŒ num_leaves å¤ªå¤§ï¼ˆ500ï¼‰
# â†’ æ ‘å¤ªå¤æ‚ï¼Œè¿‡æ‹Ÿåˆ
# â†’ åœ¨è®­ç»ƒé›†ä¸Šç²¾åº¦é«˜ï¼Œæµ‹è¯•é›†ä½
clf_complex = lgb.LGBMClassifier(num_leaves=500, n_estimators=100)
# è®­ç»ƒç²¾åº¦: 0.98, æµ‹è¯•ç²¾åº¦: 0.92ï¼ˆå·®è·å¤§ï¼Œè¿‡æ‹Ÿåˆï¼ï¼‰
```

**è°ƒä¼˜ç­–ç•¥**ï¼š
- ä» 31 å¼€å§‹ï¼ˆé»˜è®¤ï¼‰
- æ•°æ®é‡å°ï¼ˆ<10kï¼‰ï¼šå‡å°åˆ° 15-20
- æ•°æ®é‡å¤§ï¼ˆ>1Mï¼‰ï¼šå¢å¤§åˆ° 50-100
- è¿‡æ‹Ÿåˆä¸¥é‡ï¼šå‡å° num_leaves

#### ğŸ¯ `learning_rate`

```python
# âŒ å¤ªå¤§ï¼ˆ0.5ï¼‰
# â†’ æ­¥é•¿å¤ªå¤§ï¼Œå®¹æ˜“è¶Šè¿‡æœ€ä¼˜å€¼ï¼Œéœ‡è¡
clf_large_lr = lgb.LGBMClassifier(learning_rate=0.5, n_estimators=100)
# ç²¾åº¦ä¸ç¨³å®šï¼Œå¯èƒ½ä¸‹é™

# âœ“ åˆç†ï¼ˆ0.05ï¼‰
# â†’ ç¨³å®šæ”¶æ•›ï¼Œéœ€è¦æ›´å¤šæ ‘æ•°
clf_good_lr = lgb.LGBMClassifier(learning_rate=0.05, n_estimators=1000)
# ç²¾åº¦: 0.95+ï¼ˆç¨³å®šï¼‰

# âŒ å¤ªå°ï¼ˆ0.001ï¼‰
# â†’ æ”¶æ•›å¤ªæ…¢ï¼Œéœ€è¦æå¤šæ ‘æ•°ï¼ˆ10000+ï¼‰
# â†’ è®­ç»ƒæ—¶é—´å¤ªé•¿
clf_small_lr = lgb.LGBMClassifier(learning_rate=0.001, n_estimators=50000)
```

**è°ƒä¼˜ç­–ç•¥**ï¼š
- ä» 0.05 å¼€å§‹
- learning_rate ä¸ n_estimators æˆåæ¯”ï¼šlr è¶Šå°ï¼Œéœ€è¦è¶Šå¤šæ ‘
- ç”Ÿäº§ç¯å¢ƒï¼š0.01-0.05ï¼ˆç¨³å®šæ€§ä¼˜å…ˆï¼‰
- ç«èµ›ç¯å¢ƒï¼š0.05-0.1ï¼ˆç²¾åº¦ä¼˜å…ˆï¼‰

#### ğŸ¯ `feature_fraction` å’Œ `bagging_fraction`ï¼ˆé‡‡æ ·ï¼‰

```python
# ç‰¹å¾é‡‡æ ·ï¼ˆå‡å°‘ç‰¹å¾ï¼‰
feature_fraction=0.8  # æ¯æ¬¡è¿­ä»£åªç”¨ 80% çš„ç‰¹å¾

# æ ·æœ¬é‡‡æ ·ï¼ˆå‡å°‘æ ·æœ¬ï¼‰
bagging_fraction=0.8  # æ¯æ¬¡è¿­ä»£åªç”¨ 80% çš„æ ·æœ¬
bagging_freq=5        # æ¯ 5 æ¬¡è¿­ä»£è¿›è¡Œé‡‡æ ·

clf = lgb.LGBMClassifier(
    feature_fraction=0.8,
    bagging_fraction=0.8,
    bagging_freq=5,
    n_estimators=100
)
```

ä¼˜åŠ¿ï¼š
- å‡å°‘è¿‡æ‹Ÿåˆ
- åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼ˆæ¯æ¬¡è¿­ä»£å¤„ç†çš„æ•°æ®å‡å°‘ï¼‰
- å¢åŠ æ ‘ä¹‹é—´çš„å¤šæ ·æ€§

---

## âš–ï¸ ä¼˜ç¼ºç‚¹ä¸åœºæ™¯ (Pros & Cons)

### 5.1 ä¼˜ç¼ºç‚¹å¯¹æ¯”è¡¨

| ç»´åº¦ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|------|------|------|
| **é€Ÿåº¦** | â­â­â­â­â­ æ¯”XGBoostå¿«5-10å€ | âŒ ä»ä¸å¦‚çº¿æ€§æ¨¡å‹ |
| **å†…å­˜** | â­â­â­â­â­ ç›´æ–¹å›¾ä¼˜åŒ–ï¼Œå†…å­˜çœ50% | âŒ ä»éœ€è¶³å¤Ÿå†…å­˜ |
| **ç²¾åº¦ï¼ˆå¤§æ•°æ®ï¼‰** | â­â­â­â­â­ ç™¾ä¸‡çº§æ•°æ®æœ€ä¼˜ | âŒ æ—  |
| **ç²¾åº¦ï¼ˆå°æ•°æ®ï¼‰** | â­â­â­ å¯èƒ½ä½äºXGBoost | âŒ å°æ•°æ®æ˜“è¿‡æ‹Ÿåˆ |
| **ç±»åˆ«ç‰¹å¾** | â­â­â­â­â­ åŸç”Ÿæ”¯æŒï¼Œæ— éœ€ç¼–ç  | âŒ æ—  |
| **è¿‡æ‹Ÿåˆé£é™©** | â­â­ Leaf-wiseå®¹æ˜“è¿‡æ‹Ÿåˆ | âŒ éœ€è¦è°¨æ…è°ƒå‚ |
| **å¯è§£é‡Šæ€§** | â­â­â­ ç‰¹å¾é‡è¦æ€§ | âŒ Leaf-wiseæ ‘å¤æ‚ |
| **GOSS æ•°æ®ä¸¢å¤±** | âŒ é‡‡æ ·å¯èƒ½ä¸¢å¤±å°æ¢¯åº¦ä¿¡æ¯ | âš ï¸ éœ€è¦å°å¿ƒ |

### 5.2 LightGBM vs XGBoost è¯¦ç»†å¯¹æ¯”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LightGBM vs XGBoost                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚ æ ‘æ„å»ºç­–ç•¥ï¼š                                                  â”‚
â”‚   XGBoost:  Level-wiseï¼ˆæ¯å±‚åŒæ—¶åˆ†è£‚æ‰€æœ‰èŠ‚ç‚¹ï¼‰               â”‚
â”‚   LightGBM: Leaf-wiseï¼ˆæ¯æ¬¡åˆ†è£‚Gainæœ€å¤§çš„å¶å­ï¼‰             â”‚
â”‚                                                               â”‚
â”‚ æ€§èƒ½å¯¹æ¯”ï¼ˆ100ä¸‡æ ·æœ¬ï¼Œ100ä¸ªç‰¹å¾ï¼‰ï¼š                            â”‚
â”‚   XGBoost:  æ—¶é—´ 45ç§’ï¼Œç²¾åº¦ 0.8950                           â”‚
â”‚   LightGBM: æ—¶é—´ 6ç§’ï¼Œç²¾åº¦ 0.8945                            â”‚
â”‚   æå‡ï¼š7.5å€åŠ é€Ÿï¼Œç²¾åº¦åŸºæœ¬ç›¸åŒ                               â”‚
â”‚                                                               â”‚
â”‚ æ•°æ®é‡å½±å“ï¼š                                                  â”‚
â”‚   æ•°æ®é‡<10k:     XGBoost æ›´å¥½ï¼ˆæ›´ç¨³å®šï¼‰                     â”‚
â”‚   æ•°æ®é‡ 10k-1M:  å¹³æ‰‹ï¼ˆéƒ½ä¸é”™ï¼‰                             â”‚
â”‚   æ•°æ®é‡>1M:      LightGBM èƒœï¼ˆé€Ÿåº¦å¿«ï¼Œå†…å­˜çœï¼‰              â”‚
â”‚                                                               â”‚
â”‚ ä¼˜åŒ–ç‰¹æŠ€ï¼š                                                    â”‚
â”‚   XGBoost: æ—                                                 â”‚
â”‚   LightGBM: GOSSé‡‡æ · + EFBæ‰“åŒ… + ç›´æ–¹å›¾ä¼˜åŒ–                 â”‚
â”‚                                                               â”‚
â”‚ ç¼ºç‚¹ï¼š                                                        â”‚
â”‚   XGBoost: å¤§æ•°æ®æ…¢                                          â”‚
â”‚   LightGBM: å°æ•°æ®æ˜“è¿‡æ‹Ÿåˆï¼ˆLeaf-wiseè´ªå¿ƒï¼‰                â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.3 åº”ç”¨åœºæ™¯å†³ç­–æ ‘

```
é€‰æ‹© LightGBM çš„æ¡ä»¶ï¼š
â”œâ”€ æ•°æ®é‡ > 100k æ ·æœ¬
â”‚  â””â”€ æ•°æ®éå¸¸å¤§ï¼ˆç™¾ä¸‡çº§ï¼‰â†’ LightGBMï¼ˆé€Ÿåº¦é‡è¦ï¼‰
â”‚  â””â”€ å†…å­˜å—é™ â†’ LightGBMï¼ˆç›´æ–¹å›¾ä¼˜åŒ–ï¼‰
â”‚
â”œâ”€ ç‰¹å¾æœ‰å¤§é‡ç±»åˆ«å˜é‡
â”‚  â””â”€ ä¸€é”®çƒ­ç¼–ç åç‰¹å¾çˆ†ç‚¸ â†’ LightGBMï¼ˆåŸç”Ÿæ”¯æŒï¼‰
â”‚
â”œâ”€ éœ€è¦å®æ—¶è®­ç»ƒå’Œé¢„æµ‹
â”‚  â””â”€ é‡‘èã€æ¨èç³»ç»Ÿç­‰ â†’ LightGBMï¼ˆæå¿«ï¼‰
â”‚
â””â”€ ç”Ÿäº§ç¯å¢ƒï¼Œè¦æ±‚é«˜æ•ˆç‡
   â””â”€ CTRé¢„æµ‹ã€æ’åæ¨¡å‹ â†’ LightGBMï¼ˆä¸šç•Œæ ‡é…ï¼‰

é€‰æ‹© XGBoost çš„æ¡ä»¶ï¼š
â”œâ”€ æ•°æ®é‡ < 100k
â”‚  â””â”€ å°æ•°æ®é›† â†’ XGBoostï¼ˆç¨³å®šæ€§å¥½ï¼‰
â”‚
â”œâ”€ éœ€è¦æœ€é«˜ç²¾åº¦
â”‚  â””â”€ Kaggleç«èµ› â†’ XGBoostï¼ˆç²¾åº¦ä¼˜å…ˆï¼‰
â”‚
â”œâ”€ éœ€è¦æœ€å¤§ç¨³å®šæ€§
â”‚  â””â”€ é‡‘èé£æ§ â†’ XGBoostï¼ˆä¸èƒ½æœ‰ä»»ä½•ç²¾åº¦æŸå¤±ï¼‰
â”‚
â””â”€ æ¨¡å‹é¡»é«˜åº¦å¯è§£é‡Š
   â””â”€ ç›‘ç®¡è¦æ±‚ â†’ XGBoostï¼ˆLevel-wiseæ›´æ¸…æ™°ï¼‰
```

---

## ğŸ’¬ é¢è¯•å¿…è€ƒ (Interview Q&A)

> [!question] Q1: LightGBM ä¸ºä»€ä¹ˆæ¯” XGBoost å¿«ï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šLeaf-wise + GOSS + EFB + ç›´æ–¹å›¾å››å¤§ä¼˜åŒ–

**è¯¦ç»†è§£æ**ï¼š

1. **Leaf-wise vs Level-wise**
   - XGBoostï¼šæ¯å±‚æ‰€æœ‰èŠ‚ç‚¹åŒæ—¶åˆ†è£‚ â†’ å¯¹ç§°æ ‘ï¼Œå¯å¹¶è¡Œ
   - LightGBMï¼šåªåˆ†è£‚ Gain æœ€å¤§çš„å¶å­ â†’ å€¾æ–œæ ‘ï¼Œä¸èƒ½å¹¶è¡Œï¼Œä½†æ”¶æ•›å¿«
   - ç»“æœï¼šLightGBM ç”¨æ›´å°‘çš„æ ‘è¾¾åˆ°ç›¸åŒç²¾åº¦

2. **GOSS æ¢¯åº¦é‡‡æ ·**
   - é—®é¢˜ï¼š100 ä¸‡æ ·æœ¬ï¼Œæ¯æ£µæ ‘éƒ½è¦å…¨éƒ¨å¤„ç†
   - è§£å†³ï¼šä¿ç•™æ¢¯åº¦å¤§çš„ 10% + éšæœº 5% = 15% çš„æ ·æœ¬
   - æ•ˆæœï¼š85% çš„åŠ é€Ÿï¼Œç²¾åº¦æŸå¤± <1%

3. **EFB ç‰¹å¾æ‰“åŒ…**
   - é—®é¢˜ï¼šé«˜ç»´æ•°æ®ç‰¹å¾å¤ªå¤šï¼ˆ100+ ç‰¹å¾ï¼‰
   - è§£å†³ï¼šäº’æ–¥çš„ç‰¹å¾ï¼ˆå¦‚ One-Hotï¼‰åˆå¹¶æˆ 1 ä¸ª
   - æ•ˆæœï¼šç‰¹å¾æ•°å‡å°‘ 50-70%ï¼Œè®¡ç®—é‡å¤§å¹…ä¸‹é™

4. **ç›´æ–¹å›¾ä¼˜åŒ–**
   - é—®é¢˜ï¼šè¿ç»­å€¼éœ€è¦å¤§é‡å†…å­˜å­˜å‚¨å’Œæ¯”è¾ƒ
   - è§£å†³ï¼šè½¬ä¸º 256 ä¸ªç¦»æ•£ binï¼Œåªå¯¹ bin è¾¹ç•Œåˆ†è£‚
   - æ•ˆæœï¼šå†…å­˜å‡å°‘ 50%ï¼Œé€Ÿåº¦å¿« 2 å€

**å®é™…æ•°æ®**ï¼š
- æ•°æ®é›†ï¼š1 ç™¾ä¸‡æ ·æœ¬ï¼Œ100 ç‰¹å¾
- XGBoostï¼š45 ç§’
- LightGBMï¼š6 ç§’
- **åŠ é€Ÿï¼š7.5 å€**

---

> [!question] Q2: LightGBM å¦‚ä½•å¤„ç†ç±»åˆ«ç‰¹å¾ï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šåŸç”Ÿæ”¯æŒ categorical_featureï¼Œæ— éœ€ One-Hot ç¼–ç 

**è¯¦ç»†è§£æ**ï¼š

ä¼ ç»Ÿæ–¹æ³•ï¼ˆæ‰€æœ‰æ¡†æ¶ï¼‰ï¼š
```python
# ç±»åˆ«ç‰¹å¾ [åŒ—äº¬, ä¸Šæµ·, æ·±åœ³] â†’ One-Hot ç¼–ç 
X = [[1, 0, 0],    # åŒ—äº¬
     [0, 1, 0],    # ä¸Šæµ·
     [0, 0, 1]]    # æ·±åœ³

ç‰¹å¾æ•°ï¼š3 ä¸ªç‰¹å¾
é—®é¢˜ï¼šK ä¸ªç±»åˆ« â†’ K ä¸ªç‰¹å¾ï¼ˆç»´åº¦çˆ†ç‚¸ï¼‰
```

LightGBM æ–¹æ³•ï¼š
```python
import lightgbm as lgb

# ç›´æ¥æŒ‡å®šç±»åˆ«ç‰¹å¾
train_data = lgb.Dataset(
    X,
    label=y,
    categorical_feature=['city']  # æŒ‡å®šåˆ—åæˆ–ç´¢å¼•
)

clf = lgb.LGBMClassifier()
clf.fit(X_train, y_train)  # è‡ªåŠ¨å¤„ç†ï¼Œæ— éœ€ç¼–ç 
```

ä¼˜åŠ¿ï¼š
1. **ç»´åº¦ä¸çˆ†ç‚¸**ï¼šK ä¸ªç±»åˆ« â†’ 1 ä¸ªç‰¹å¾ï¼ˆè¾“å…¥ç»™æ ‘çš„å½¢å¼ï¼‰
2. **ç²¾åº¦æ›´é«˜**ï¼šä¿ç•™äº†ç±»åˆ«çš„è‡ªç„¶ç»“æ„ï¼Œä¸è¢« One-Hot çš„ correlated features å¹²æ‰°
3. **é€Ÿåº¦æ›´å¿«**ï¼šç‰¹å¾æ•°å°‘ï¼Œè®¡ç®—é‡å°
4. **å†…å­˜èŠ‚çœ**ï¼šç›´æ–¹å›¾ä¼˜åŒ–å¯¹ç±»åˆ«ç‰¹å¾æ•ˆæœç‰¹åˆ«å¥½

**æ€§èƒ½å¯¹æ¯”**ï¼š
```
æ•°æ®ï¼š100ä¸‡æ ·æœ¬ï¼ŒåŸå§‹50ä¸ªç‰¹å¾ï¼ˆå…¶ä¸­20ä¸ªç±»åˆ«ç‰¹å¾ï¼‰

One-Hot + XGBoost:
  ç‰¹å¾æ•°: 50 + 180ï¼ˆç±»åˆ«å±•å¼€ï¼‰= 230
  å†…å­˜: 500MB
  æ—¶é—´: 60ç§’

LightGBMï¼ˆåŸç”Ÿæ”¯æŒï¼‰:
  ç‰¹å¾æ•°: 50
  å†…å­˜: 150MB
  æ—¶é—´: 8ç§’

æ€§èƒ½æå‡ï¼šå†…å­˜èŠ‚çœ70%ï¼Œé€Ÿåº¦å¿«7.5å€ï¼
```

---

> [!question] Q3: GOSS é‡‡æ ·ä¼šä¸¢å¤±ä¿¡æ¯å—ï¼Ÿå¦‚ä½•å¹³è¡¡ï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šGOSS æ™ºèƒ½é‡‡æ ·ï¼Œåªä¸¢å¼ƒ "æ— ç”¨æ ·æœ¬"ï¼Œç”¨æƒé‡è¡¥å¿

**è¯¦ç»†è§£æ**ï¼š

GOSS ä¸‰å±‚é€»è¾‘ï¼š

1. **æ¢¯åº¦å¤§çš„æ ·æœ¬**ï¼ˆtop_rateï¼‰
   - å«ä¹‰ï¼šè¿™äº›æ ·æœ¬çš„æ¢¯åº¦ç»å¯¹å€¼å¤§ï¼Œè¯´æ˜æ¨¡å‹åœ¨è¿™äº›æ ·æœ¬ä¸Š "å‡ºé”™å¤š"
   - ç­–ç•¥ï¼šå®Œæ•´ä¿ç•™ï¼ˆæƒé‡ = 1ï¼‰
   - ç†ç”±ï¼šè¿™äº›æ˜¯ "éš¾æ ·æœ¬"ï¼Œå¿…é¡»å­¦å¥½

2. **æ¢¯åº¦å°çš„æ ·æœ¬çš„è¡¥é›†**ï¼ˆother_rateï¼‰
   - å«ä¹‰ï¼šè¿™äº›æ ·æœ¬æ¢¯åº¦å°ï¼Œæ¨¡å‹å·²ç»æ‹Ÿåˆå¾—å¾ˆå¥½ï¼Œä½†ä¸èƒ½å…¨éƒ¨ä¸¢å¼ƒ
   - ç­–ç•¥ï¼šéšæœºé‡‡æ ·ä¸€éƒ¨åˆ†ï¼ˆæƒé‡ â‰  1ï¼‰
   - ç†ç”±ï¼šé¿å…å®Œå…¨ä¸¢å¼ƒå¯¼è‡´æ ·æœ¬åˆ†å¸ƒå˜åŒ–

3. **æƒé‡è¡¥å¿**
   - é‡‡æ ·ç‡ï¼š$r = \text{top\_rate} + (1 - \text{top\_rate}) \times \text{other\_rate}$
   - æƒé‡ï¼š$w = 1 / r$ï¼ˆè¡¥å¿é‡‡æ ·çš„åå·®ï¼‰

```
ç¤ºä¾‹ï¼ˆtop_rate=10%, other_rate=5%ï¼‰ï¼š

åŸå§‹ 100ä¸‡æ ·æœ¬ï¼š
  æ¢¯åº¦æœ€å¤§ 10ä¸‡ï¼ˆä¿ç•™ï¼Œæƒé‡=1ï¼‰
  æ¢¯åº¦æœ€å° 90ä¸‡ä¸­éšæœºé‡‡æ · 4.5ä¸‡ï¼ˆæƒé‡=1/0.145â‰ˆ6.9ï¼‰

æœ€ç»ˆ 14.5ä¸‡æ ·æœ¬ â†’ æ•ˆæœç­‰åŒäº 100ä¸‡æ ·æœ¬ âœ“

åŸç†ï¼š
  ä½æ¢¯åº¦æ ·æœ¬è™½ç„¶å°‘äº†ï¼Œä½†æƒé‡å¢å¤§ï¼Œè¡¥å¿äº†ä¿¡æ¯
```

**ä½•æ—¶è°¨æ…ä½¿ç”¨ GOSS**ï¼š
```python
# âœ“ é€‚åˆ GOSSï¼š
# - æ•°æ®é‡å·¨å¤§ï¼ˆ>1Mï¼‰
# - æ¢¯åº¦åˆ†å¸ƒä¸å‡åŒ€ï¼ˆæœ‰æ˜ç¡®çš„éš¾æ ·æœ¬ï¼‰
clf = lgb.LGBMClassifier(
    top_rate=0.1,
    other_rate=0.05
)

# âŒ è°¨æ…ä½¿ç”¨ GOSSï¼š
# - æ•°æ®é‡å°ï¼ˆ<100kï¼‰ï¼Œé‡‡æ ·åå·®å¤§
# - æåº¦ä¸å¹³è¡¡æ•°æ®ï¼ˆå°‘æ•°ç±»æ ·æœ¬ä¸è¶³ï¼‰
# - éœ€è¦ç™¾åˆ†ç™¾ç²¾åº¦ï¼ˆé‡‘èé£æ§ï¼‰

# ä¿é™©æ–¹æ¡ˆï¼šç¦ç”¨ GOSS
clf = lgb.LGBMClassifier(
    top_rate=1.0,  # ä¿ç•™æ‰€æœ‰æ¢¯åº¦æœ€å¤§çš„æ ·æœ¬
    other_rate=0.0  # ä¸ä»ä½æ¢¯åº¦é‡‡æ ·
)
```

---

> [!question] Q4: LightGBM è¿‡æ‹Ÿåˆä¸¥é‡ï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šå¤šå±‚é˜²çº¿ï¼ˆnum_leaves + é‡‡æ · + æ­£åˆ™åŒ– + æ—©åœï¼‰

**è¯¦ç»†è§£æ**ï¼š

LightGBM æ˜“è¿‡æ‹Ÿåˆçš„åŸå› ï¼š
- Leaf-wise æ„å»ºæ–¹å¼ï¼šæ¯æ¬¡è´ªå¿ƒåˆ†è£‚ï¼Œè¿½æ±‚å±€éƒ¨æœ€ä¼˜
- å¯èƒ½å¯¼è‡´ï¼šæ ‘å¤ªæ·±ï¼Œç‰¹å¾äº¤äº’è¿‡åº¦å¤æ‚

äº”å±‚é˜²çº¿ï¼š

```python
# é˜²çº¿1ï¼šå‡å° num_leavesï¼ˆæœ€ç›´æ¥ï¼‰
clf = lgb.LGBMClassifier(
    num_leaves=10,  # é»˜è®¤ 31ï¼Œæ”¹ä¸º 10ï¼ˆå‡å°‘å¤æ‚åº¦ï¼‰
    n_estimators=100
)

# é˜²çº¿2ï¼šé™åˆ¶æ ‘æ·±åº¦
clf = lgb.LGBMClassifier(
    max_depth=5,  # å¼ºåˆ¶æ ‘ä¸è¶…è¿‡ 5 å±‚
    num_leaves=20
)

# é˜²çº¿3ï¼šå¢åŠ é‡‡æ ·æ¯”ä¾‹çš„éšæœºæ€§
clf = lgb.LGBMClassifier(
    feature_fraction=0.5,  # åªç”¨ 50% çš„ç‰¹å¾
    bagging_fraction=0.7,  # åªç”¨ 70% çš„æ ·æœ¬
    bagging_freq=5         # æ¯ 5 æ¬¡è¿­ä»£é‡‡æ ·ä¸€æ¬¡
)

# é˜²çº¿4ï¼šå¼ºåŒ–æ­£åˆ™åŒ–
clf = lgb.LGBMClassifier(
    reg_alpha=1.0,         # L1 æ­£åˆ™åŒ–
    reg_lambda=5.0,        # L2 æ­£åˆ™åŒ–
    min_data_in_leaf=50    # å¶å­æœ€å°‘ 50 ä¸ªæ ·æœ¬
)

# é˜²çº¿5ï¼šæ—©åœ + ç›‘æ§
clf.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    callbacks=[
        lgb.early_stopping(10),  # 10è½®æœªæ”¹è¿›åˆ™åœæ­¢
        lgb.log_evaluation(50)   # æ¯50è½®è¾“å‡ºä¸€æ¬¡
    ]
)
```

**è¯Šæ–­è¿‡æ‹Ÿåˆçš„æ–¹æ³•**ï¼š
```python
# è®­ç»ƒ vs éªŒè¯ç²¾åº¦å·®å¼‚
train_acc = clf.score(X_train, y_train)  # 0.98
val_acc = clf.score(X_val, y_val)        # 0.92
print(f"è¿‡æ‹Ÿåˆç¨‹åº¦: {train_acc - val_acc:.4f}")

# åˆ¤æ–­ï¼š
# - å·®å¼‚ < 0.02: è‰¯å¥½ï¼ˆâœ“ï¼‰
# - å·®å¼‚ 0.02-0.05: è½»å¾®è¿‡æ‹Ÿåˆï¼ˆâš ï¸ï¼‰
# - å·®å¼‚ > 0.05: ä¸¥é‡è¿‡æ‹Ÿåˆï¼ˆâŒï¼‰
```

---

> [!question] Q5: ä»€ä¹ˆæ—¶å€™ç”¨ LightGBMï¼Œä»€ä¹ˆæ—¶å€™ç”¨ XGBoostï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šæ•°æ®é‡æ˜¯ä¸»è¦åˆ†ç•Œç‚¹ï¼›ç²¾åº¦ vs é€Ÿåº¦æ˜¯æƒè¡¡ç‚¹

**è¯¦ç»†è§£æ**ï¼š

```
å†³ç­–æµç¨‹ï¼š

ç¬¬1æ­¥ï¼šæ•°æ®é‡æœ‰å¤šå¤§ï¼Ÿ
  â”œâ”€ <10k æ ·æœ¬
  â”‚  â””â”€ æ¨èï¼šXGBoostï¼ˆç¨³å®šæ€§ä¼˜å…ˆï¼‰
  â”‚     åŸå› ï¼šå°æ•°æ®ä¸Š LightGBM Leaf-wise å®¹æ˜“è¿‡æ‹Ÿåˆ
  â”‚
  â”œâ”€ 10k-1M æ ·æœ¬
  â”‚  â””â”€ å¹³æ‰‹ï¼šä¸¤è€…éƒ½å¯ä»¥
  â”‚     XGBoostï¼šç²¾åº¦ç•¥é«˜ 0.5-1%
  â”‚     LightGBMï¼šé€Ÿåº¦å¿« 2-3 å€
  â”‚
  â””â”€ >1M æ ·æœ¬
     â””â”€ æ¨èï¼šLightGBMï¼ˆé€Ÿåº¦å’Œå†…å­˜é‡è¦ï¼‰
        åŸå› ï¼š10 å€ä»¥ä¸ŠåŠ é€Ÿï¼Œå†…å­˜çœ 50%

ç¬¬2æ­¥ï¼šå¯¹ç²¾åº¦æœ‰ä»€ä¹ˆè¦æ±‚ï¼Ÿ
  â”œâ”€ ç«èµ›ç¯å¢ƒï¼ˆKaggleï¼‰
  â”‚  â””â”€ XGBoostï¼ˆç²¾åº¦ç¬¬ä¸€ï¼Œæ¯ 0.001% éƒ½é‡è¦ï¼‰
  â”‚
  â”œâ”€ å­¦æœ¯è®ºæ–‡
  â”‚  â””â”€ XGBoostï¼ˆæ›´å®¹æ˜“å‘è¡¨ï¼Œè®¤å¯åº¦é«˜ï¼‰
  â”‚
  â”œâ”€ ç”Ÿäº§ç³»ç»Ÿ
  â”‚  â”œâ”€ ç²¾åº¦æœ€ä¼˜ï¼šXGBoost
  â”‚  â”œâ”€ é€Ÿåº¦ä¼˜å…ˆï¼šLightGBM
  â”‚  â””â”€ è¦æ±‚é«˜æ•ˆï¼šLightGBMï¼ˆæ¨èï¼‰
  â”‚
  â””â”€ ç‰¹å¾æœ‰ç±»åˆ«å˜é‡
     â””â”€ LightGBMï¼ˆçœå» One-Hot éº»çƒ¦ï¼‰

ç¬¬3æ­¥ï¼šå¯¹å¯è§£é‡Šæ€§æœ‰è¦æ±‚å—ï¼Ÿ
  â”œâ”€ é‡‘èé£æ§ï¼ˆç›‘ç®¡è¦æ±‚ï¼‰
  â”‚  â””â”€ XGBoostï¼ˆLevel-wise æ ‘æ›´å®¹æ˜“ç†è§£ï¼‰
  â”‚
  â””â”€ ä¸€èˆ¬ä¸šåŠ¡
     â””â”€ ä¸¤è€…éƒ½å¯ä»¥ï¼ˆç‰¹å¾é‡è¦æ€§éƒ½æ”¯æŒï¼‰
```

**å¿«é€Ÿåˆ¤æ–­è¡¨**ï¼š

| åœºæ™¯ | æ¨è | ç†ç”± |
|------|------|------|
| Kaggle ç«èµ› | XGBoost | ç²¾åº¦ä¼˜å…ˆ |
| é‡‘èé£æ§ | XGBoost | ç¨³å®šæ€§å’Œå¯è§£é‡Šæ€§ |
| å¤§æ•°æ®ç«èµ› | LightGBM | é€Ÿåº¦å’Œå†…å­˜ |
| CTR/æ¨èç³»ç»Ÿ | LightGBM | å¤„ç†ç™¾ä¸‡æ•°æ® |
| å°æ•°æ®é›† | XGBoost | æŠ—è¿‡æ‹Ÿåˆ |
| æœ‰ç±»åˆ«ç‰¹å¾ | LightGBM | åŸç”Ÿæ”¯æŒ |

---

> [!question] Q6: å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒè°ƒä¼˜ LightGBMï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šå…ˆä¼˜åŒ– num_leavesï¼Œå†è°ƒ learning_rateï¼Œæœ€åç”¨ GOSS åŠ é€Ÿ

**è¯¦ç»†è§£æ**ï¼š

ç”Ÿäº§è°ƒä¼˜ä¸‰æ­¥æ³•ï¼š

**ç¬¬1æ­¥ï¼šå‚æ•°ç½‘æ ¼æœç´¢**
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'num_leaves': [15, 31, 63],          # é‡ç‚¹å…³æ³¨
    'max_depth': [-1, 5, 10],            # æ ‘æ·±åº¦
    'learning_rate': [0.01, 0.05, 0.1], # å­¦ä¹ ç‡
}

grid_search = GridSearchCV(
    lgb.LGBMClassifier(n_estimators=100),
    param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1
)
grid_search.fit(X_train, y_train)
print(f"æœ€ä¼˜å‚æ•°ï¼š{grid_search.best_params_}")
```

**ç¬¬2æ­¥ï¼šåŸºäºæœ€ä¼˜å‚æ•°çš„ç²¾ç»†è°ƒä¼˜**
```python
best_params = grid_search.best_params_

# å›ºå®šæœ€ä¼˜å‚æ•°ï¼Œå¢åŠ æ ‘æ•°ï¼ˆbetterç²¾åº¦ï¼‰
clf = lgb.LGBMClassifier(
    **best_params,
    n_estimators=1000,  # å¤§å¹…å¢åŠ 
    learning_rate=0.01  # é™ä½å­¦ä¹ ç‡ï¼Œé…åˆå¤šæ ‘
)

# ç”¨æ—©åœæ‰¾æœ€ä¼˜æ ‘æ•°
clf.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    eval_metric='auc',
    callbacks=[
        lgb.early_stopping(20),
        lgb.log_evaluation(100)
    ]
)
```

**ç¬¬3æ­¥ï¼šç”Ÿäº§ä¼˜åŒ–ï¼ˆå¯ç”¨ GOSSï¼‰**
```python
clf_prod = lgb.LGBMClassifier(
    **best_params,
    n_estimators=clf.n_estimators_,  # ç”¨ç¬¬2æ­¥æ‰¾åˆ°çš„æ ‘æ•°

    # å¯ç”¨ GOSSï¼ˆ8-10å€åŠ é€Ÿï¼‰
    top_rate=0.1,      # ä¿ç•™æ¢¯åº¦æœ€å¤§çš„ 10%
    other_rate=0.05,   # ä»å‰©ä½™ä¸­é‡‡æ · 5%

    # å¯ç”¨å¹¶è¡Œ
    n_jobs=-1,
    verbose=-1
)

clf_prod.fit(X_train, y_train)

# éªŒè¯ï¼šGOSS åç²¾åº¦æŸå¤±åº”è¯¥ < 1%
prod_score = roc_auc_score(y_val, clf_prod.predict_proba(X_val)[:, 1])
baseline_score = roc_auc_score(y_val, clf.predict_proba(X_val)[:, 1])

print(f"åŸºçº¿ç²¾åº¦: {baseline_score:.4f}")
print(f"GOSSç²¾åº¦: {prod_score:.4f}")
print(f"ç²¾åº¦æŸå¤±: {baseline_score - prod_score:.4f} (åº”<0.01)")
```

**æ€§èƒ½æ£€æŸ¥æ¸…å•**ï¼š
```
â˜‘ï¸ ç²¾åº¦ï¼šéªŒè¯é›† AUC > 0.90ï¼ˆä¸šåŠ¡ç›¸å…³ï¼‰
â˜‘ï¸ é€Ÿåº¦ï¼šå•æ¬¡é¢„æµ‹ < 50msï¼ˆç™¾ä¸‡ QPSï¼‰
â˜‘ï¸ å†…å­˜ï¼šæ¨¡å‹å¤§å° < 500MBï¼ˆæ˜“äºéƒ¨ç½²ï¼‰
â˜‘ï¸ ç¨³å®šæ€§ï¼šä¸åŒæ•°æ®æ‰¹æ¬¡ç²¾åº¦æ³¢åŠ¨ < 1%
â˜‘ï¸ è¿‡æ‹Ÿåˆï¼šè®­ç»ƒç²¾åº¦ - éªŒè¯ç²¾åº¦ < 2%
```

---

## æ€»ç»“

> [!IMPORTANT] LightGBM æ ¸å¿ƒè¦ç‚¹
> - **Leaf-wise = è´ªå¿ƒæœ€ä¼˜**ï¼Œä½†æ˜“è¿‡æ‹Ÿåˆï¼ˆéœ€è¦ num_leaves æ§åˆ¶ï¼‰
> - **GOSS = æ™ºèƒ½é‡‡æ ·**ï¼Œ85% åŠ é€Ÿï¼Œä¿¡æ¯æŸå¤± < 1%
> - **EFB = ç‰¹å¾æ‰“åŒ…**ï¼Œç»´åº¦å‡å°‘ 50-70%
> - **ç›´æ–¹å›¾ = å†…å­˜ä¼˜åŒ–**ï¼Œçœ 50% å†…å­˜ï¼Œå¿« 2 å€
> - **åŸç”Ÿç±»åˆ«ç‰¹å¾ = æ— éœ€ One-Hot**ï¼Œå‡å°‘ç‰¹å¾çˆ†ç‚¸

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… å¤§æ•°æ®å¤„ç†ï¼ˆ>100k æ ·æœ¬ï¼‰
- âœ… ç”Ÿäº§ç³»ç»Ÿï¼ˆé€Ÿåº¦é‡è¦ï¼‰
- âœ… ç±»åˆ«ç‰¹å¾å¤šçš„æ•°æ®
- âœ… å†…å­˜å—é™çš„ç¯å¢ƒ
- âœ… CTR/æ¨èç³»ç»Ÿ

**ä¸é€‚åˆåœºæ™¯**ï¼š
- âŒ å°æ•°æ®é›†ï¼ˆ<10kï¼Œæ˜“è¿‡æ‹Ÿåˆï¼‰
- âŒ å¯¹ç²¾åº¦è¦æ±‚æé«˜ï¼ˆKaggle é¡¶çº§ï¼‰
- âŒ éœ€è¦é«˜åº¦å¯è§£é‡Šæ€§ï¼ˆé‡‘èé£æ§ï¼‰
- âŒ ä»»ä½•ç²¾åº¦æŸå¤±éƒ½ä¸èƒ½å®¹å¿

**ç”Ÿäº§éƒ¨ç½²å»ºè®®**ï¼š
1. å…ˆç”¨ XGBoost ç¡®ä¿ç²¾åº¦åŸºçº¿
2. æ”¹ç”¨ LightGBM å¹¶è°ƒä¼˜ num_leaves
3. å¦‚æœç²¾åº¦æŸå¤± <1%ï¼Œå¯ç”¨ GOSS åŠ é€Ÿ
4. ç›‘æ§ç”Ÿäº§ç¯å¢ƒçš„ç²¾åº¦å’Œé€Ÿåº¦

---

## å‚è€ƒèµ„æº

- **å®˜æ–¹æ–‡æ¡£**ï¼šhttps://lightgbm.readthedocs.io/
- **GitHub**ï¼šhttps://github.com/microsoft/LightGBM
- **è®ºæ–‡**ï¼šLightGBM: A Fast, Distributed, High-performance Gradient Boosting Framework (NIPS 2017)
- **æ€§èƒ½å¯¹æ¯”**ï¼šMicrosoft å®˜æ–¹ Benchmark
- **æœ€ä½³å®è·µ**ï¼šKaggle ç«èµ›è·å¥–æ–¹æ¡ˆ

éµå¾ªæ ‡å‡†ï¼š[[_ALGO_INSTRUCTIONS|Claudeåä½œæŒ‡å— v2.0]]
