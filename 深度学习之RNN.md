---
tags: [ç®—æ³•, æ·±åº¦å­¦ä¹ , åºåˆ—æ¨¡å‹, RNN, LSTM, GRU]
math: true
difficulty: å›°éš¾
---

# å¾ªç¯ç¥ç»ç½‘ç»œ (RNN, LSTM, GRU)

## ğŸ’¡ æ ¸å¿ƒç›´è§‰

- **ä¸€å¥è¯å®šä¹‰**ï¼šé€šè¿‡åœ¨æ—¶é—´æ­¥é—´å…±äº«å‚æ•°ï¼Œä½¿ç½‘ç»œå…·æœ‰"è®°å¿†"ï¼Œèƒ½å¤„ç†ä»»æ„é•¿åº¦çš„åºåˆ—ï¼Œæ•æ‰åºåˆ—ä¸­çš„é•¿æœŸä¾èµ–ã€‚

- **è§£å†³é—®é¢˜**ï¼šè§£å†³äº†å…¨è¿æ¥ç¥ç»ç½‘ç»œæ— æ³•å¤„ç†å˜é•¿åºåˆ—ã€ä¸¢å¤±æ—¶é—´ä¿¡æ¯çš„é—®é¢˜ã€‚RNN æ¨åŠ¨äº† NLP ä» n-gramã€ç‰¹å¾å·¥ç¨‹æ—¶ä»£è¿›å…¥æ·±åº¦å­¦ä¹ æ—¶ä»£ã€‚

- **æ ¸å¿ƒé€»è¾‘**ï¼šRNN = å‚æ•°å…±äº« + éšçŠ¶æ€ä¼ é€’ã€‚æ¯ä¸ªæ—¶åˆ»éƒ½ç”¨ç›¸åŒçš„å‚æ•°é›†ï¼ŒéšçŠ¶æ€ $h_t$ æ—¢æ˜¯è¾“å‡ºï¼Œä¹Ÿæ˜¯ä¸‹ä¸€æ—¶åˆ»çš„è¾“å…¥ï¼Œå½¢æˆ"è®°å¿†"ã€‚

- **å‡ ä½•æ„ä¹‰**ï¼š
  - **å…¨è¿æ¥ç½‘ç»œ**ï¼šæ¯ä¸ªè¾“å‡ºç‹¬ç«‹è®¡ç®—ï¼Œæ— æ—¶é—´ä¾èµ–
  - **RNN**ï¼šéšçŠ¶æ€æ²¿æ—¶é—´é“¾ä¼ é€’ï¼Œæ¯å±‚å¯"çœ‹åˆ°"å†å²ä¿¡æ¯ï¼ˆé€šè¿‡ $h_{t-1}$ï¼‰
  - **LSTM**ï¼šå¢åŠ äº†æ§åˆ¶ä¿¡æ¯æµçš„"é—¸é—¨"ï¼Œèƒ½é€‰æ‹©è®°ä½æˆ–é—å¿˜ä¿¡æ¯

- **æ€æ‰‹é” (Killer Feature)**ï¼š**å‚æ•°å…±äº«** + **æƒé‡é€’å½’** ä½¿åºåˆ—å­¦ä¹ æˆä¸ºå¯èƒ½ã€‚RNN æ˜¯æ‰€æœ‰åºåˆ—å»ºæ¨¡çš„ç»å…¸èŒƒå¼ï¼šæœºå™¨ç¿»è¯‘ï¼ˆseq2seqï¼‰ã€è¯­è¨€æ¨¡å‹ã€æ—¶é—´åºåˆ—é¢„æµ‹ç­‰ã€‚

> [!TIP] RNN vs LSTM vs Transformer çš„å¯¹æ¯”
>
> ```
> RNN:          xâ‚€ â†’ RNN â†’ hâ‚€ â†’ RNN â†’ hâ‚ â†’ RNN â†’ hâ‚‚ â†’ ... (é€æ­¥å¤„ç†ï¼Œæ¢¯åº¦æ¶ˆå¤±)
>               â†“            â†“            â†“
>               yâ‚€           yâ‚           yâ‚‚
>
> LSTM:         xâ‚€ â†’ [è¾“å…¥é—¨|é—å¿˜é—¨|è¾“å‡ºé—¨] â†’ hâ‚€ â†’ ... (é—¨æ§åˆ¶ä¿¡æ¯æµ)
>               â†“           â†“
>               yâ‚€      (ç»†ç²’åº¦æ§åˆ¶)
>
> Transformer:  [xâ‚€, xâ‚, xâ‚‚, ...] â†’ Self-Attention â†’ [hâ‚€, hâ‚, hâ‚‚, ...] (å®Œå…¨å¹¶è¡Œ)
>               â†“                                      â†“
>               åŒæ—¶å¤„ç†æ‰€æœ‰ tokenï¼Œæ— åºåˆ—ä¾èµ–
> ```

---

## ğŸ“ æ•°å­¦åŸç†

### 1. åŸºç¡€ RNN çš„å‰å‘ä¼ æ’­

åœ¨æ—¶åˆ» $t$ï¼ŒRNN çš„è®¡ç®—ä¸ºï¼š

$$h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$

$$y_t = W_{hy} h_t + b_y$$

å…¶ä¸­ï¼š
- $x_t$ï¼šæ—¶åˆ» $t$ çš„è¾“å…¥ï¼Œå½¢çŠ¶ $(batch, input\_size)$
- $h_t$ï¼šæ—¶åˆ» $t$ çš„éšçŠ¶æ€ï¼Œå½¢çŠ¶ $(batch, hidden\_size)$ï¼Œä¹Ÿæ˜¯"è®°å¿†"
- $y_t$ï¼šæ—¶åˆ» $t$ çš„è¾“å‡ºï¼Œå½¢çŠ¶ $(batch, output\_size)$
- $W_{hh}$ï¼šéšçŠ¶æ€æƒé‡ï¼Œå½¢çŠ¶ $(hidden\_size, hidden\_size)$ï¼Œ**è·¨æ—¶é—´å…±äº«**
- $W_{xh}$ï¼šè¾“å…¥æƒé‡ï¼Œå½¢çŠ¶ $(input\_size, hidden\_size)$ï¼Œ**è·¨æ—¶é—´å…±äº«**
- $W_{hy}$ï¼šè¾“å‡ºæƒé‡ï¼Œå½¢çŠ¶ $(hidden\_size, output\_size)$ï¼Œ**è·¨æ—¶é—´å…±äº«**

**å…³é”®**ï¼šå‚æ•° $W_{hh}, W_{xh}, W_{hy}$ åœ¨æ‰€æœ‰æ—¶åˆ» $t$ å…±äº«ï¼Œè¿™å°±æ˜¯ RNN çš„ç²¾é«“ã€‚

> [!ABSTRACT] å‚æ•°å…±äº«çš„å«ä¹‰
>
> - **ä¼˜åŠ¿**ï¼šå‚æ•°æ•°é‡ä¸åºåˆ—é•¿åº¦æ— å…³ï¼Œèƒ½å¤„ç†ä»»æ„é•¿åºåˆ—
> - **åŠ£åŠ¿**ï¼šåŒä¸€å‚æ•°åœ¨å¤šä¸ªæ—¶é—´æ­¥é‡å¤ä½¿ç”¨ï¼Œæ¢¯åº¦åœ¨åå‘ä¼ æ’­æ—¶ç´¯ç§¯ç›¸ä¹˜ï¼Œæ˜“æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

### 2. åå‘ä¼ æ’­é€šè¿‡æ—¶é—´ (BPTT - BackPropagation Through Time)

ä»æ—¶åˆ» $T$ åå‘ä¼ æ’­åˆ°æ—¶åˆ» 1ï¼Œè®¡ç®— $\frac{\partial L}{\partial W_{hh}}$ éœ€é“¾å¼æ³•åˆ™ï¼š

$$\frac{\partial L}{\partial W_{hh}} = \sum_{t=1}^{T} \frac{\partial L}{\partial h_t} \frac{\partial h_t}{\partial W_{hh}}$$

ä½†å…³é”®æ˜¯ $\frac{\partial h_t}{\partial W_{hh}}$ ä¾èµ–æ‰€æœ‰ä¹‹å‰çš„æ—¶åˆ»ï¼š

$$\frac{\partial h_t}{\partial W_{hh}} = \sum_{k=1}^{t} \frac{\partial h_t}{\partial h_k} \frac{\partial h_k}{\partial W_{hh}}$$

å…¶ä¸­é“¾å¼æ¶‰åŠï¼š

$$\frac{\partial h_t}{\partial h_{t-1}} = \text{diag}(1 - h_t^2) W_{hh}$$

ï¼ˆtanh çš„å¯¼æ•°ä¸º $1 - h^2$ï¼‰

**æ¢¯åº¦æ¶ˆå¤±é—®é¢˜**ï¼šå½“ $t$ å¾ˆå¤§æ—¶ï¼Œæ¢¯åº¦éœ€è¦ä¹˜ä»¥ $t-1$ æ¬¡çš„ $\frac{\partial h_{t'}}{\partial h_{t'-1}}$ï¼Œè‹¥å…¶ç»å¯¹å€¼ $< 1$ï¼Œæ¢¯åº¦æŒ‡æ•°è¡°å‡ï¼š

$$\left|\frac{\partial h_T}{\partial h_1}\right| \approx \left|\text{tanh}'(z) \cdot W_{hh}\right|^{T-1}$$

è‹¥ $|\text{tanh}'| \leq 1$ ä¸” $\|W_{hh}\| < 1$ï¼Œåˆ™ $|\frac{\partial h_T}{\partial h_1}| \to 0$ã€‚

### 3. LSTM (Long Short-Term Memory) çš„é—¨æœºåˆ¶

LSTM é€šè¿‡ä¸‰ä¸ªé—¨æ§åˆ¶ä¿¡æ¯æµï¼Œè§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼š

**è¾“å…¥é—¨**ï¼ˆå†³å®šä¿ç•™å¤šå°‘æ–°ä¿¡æ¯ï¼‰ï¼š
$$i_t = \sigma(W_{ii} x_t + W_{hi} h_{t-1} + b_i)$$

**é—å¿˜é—¨**ï¼ˆå†³å®šä¸¢å¼ƒå¤šå°‘æ—§ä¿¡æ¯ï¼‰ï¼š
$$f_t = \sigma(W_{if} x_t + W_{hf} h_{t-1} + b_f)$$

**å€™é€‰éšçŠ¶æ€**ï¼ˆæ–°çš„ä¿¡æ¯ï¼‰ï¼š
$$\tilde{h}_t = \tanh(W_{ih} x_t + W_{hh} h_{t-1} + b_h)$$

**å•å…ƒçŠ¶æ€æ›´æ–°**ï¼ˆé•¿æœŸè®°å¿†ï¼Œæ ¸å¿ƒåˆ›æ–°ï¼‰ï¼š
$$c_t = f_t \odot c_{t-1} + i_t \odot \tilde{h}_t$$

**è¾“å‡ºé—¨**ï¼ˆå†³å®šè¾“å‡ºå¤šå°‘ä¿¡æ¯ï¼‰ï¼š
$$o_t = \sigma(W_{io} x_t + W_{ho} h_{t-1} + b_o)$$

**éšçŠ¶æ€æ›´æ–°**ï¼š
$$h_t = o_t \odot \tanh(c_t)$$

å…¶ä¸­ï¼š
- $\sigma$ï¼šsigmoid å‡½æ•°ï¼Œè¾“å‡ºèŒƒå›´ $[0, 1]$ï¼ˆé—¨çš„"å¼€åº¦"ï¼‰
- $\odot$ï¼šelement-wise ä¹˜æ³•ï¼ˆHadamardç§¯ï¼‰
- $c_t$ï¼šå•å…ƒçŠ¶æ€ï¼ˆcell stateï¼‰ï¼Œç‹¬ç«‹äºéšçŠ¶æ€ï¼Œä¼ é€’é•¿æœŸä¾èµ–

> [!TIP] LSTM ä¸ºä»€ä¹ˆè§£å†³æ¢¯åº¦æ¶ˆå¤±ï¼Ÿ
>
> å…³é”®æ˜¯**å•å…ƒçŠ¶æ€çš„ç›´æ¥ä¼ é€’**ï¼š
>
> $$c_t = f_t \odot c_{t-1} + \ldots$$
>
> åå‘ä¼ æ’­æ—¶ï¼š
> $$\frac{\partial L}{\partial c_{t-1}} = \frac{\partial L}{\partial c_t} \cdot f_t$$
>
> è‹¥ $f_t$ ï¼ˆé—å¿˜é—¨ï¼‰æ¥è¿‘ 1ï¼Œæ¢¯åº¦ä¸ä¼šè¡°å‡ï¼Œå¯ä¼ é€’å¾ˆå¤šæ—¶åˆ»ã€‚
> ä¸ä¼ ç»Ÿ RNN çš„ $\text{tanh}' \cdot W_{hh}$ ä¸åŒï¼Œè¿™é‡Œæ˜¯**ç›¸ä¹˜è€Œéé“¾å¼**ï¼Œæ¢¯åº¦å¯é•¿æœŸä¿æŒã€‚

### 4. GRU (Gated Recurrent Unit) - LSTM çš„ç®€åŒ–ç‰ˆ

GRU åˆå¹¶äº†é—å¿˜é—¨å’Œè¾“å…¥é—¨ï¼Œå‚æ•°æ›´å°‘ï¼š

**é‡ç½®é—¨**ï¼š
$$r_t = \sigma(W_{ir} x_t + W_{hr} h_{t-1} + b_r)$$

**æ›´æ–°é—¨**ï¼š
$$z_t = \sigma(W_{iz} x_t + W_{hz} h_{t-1} + b_z)$$

**å€™é€‰éšçŠ¶æ€**ï¼š
$$\tilde{h}_t = \tanh(W_{ih} x_t + W_{hh} (r_t \odot h_{t-1}) + b_h)$$

**éšçŠ¶æ€æ›´æ–°**ï¼š
$$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$$

**å¯¹æ¯”**ï¼š
| ç‰¹æ€§ | LSTM | GRU |
|---|---|---|
| å‚æ•°æ•°é‡ | å¤šï¼ˆ3 ä¸ªé—¨ + cell stateï¼‰ | å°‘ï¼ˆ2 ä¸ªé—¨ï¼Œæ—  cell stateï¼‰ |
| æ¢¯åº¦æµ | é€šè¿‡ cell state ç›´æ¥ä¼ é€’ | é€šè¿‡éšçŠ¶æ€åŠ æƒå¹³å‡ |
| æ•ˆæœ | ç•¥ä¼˜ï¼Œå°¤å…¶é•¿åºåˆ— | è®¡ç®—å¿«ï¼Œä¸­ç­‰åºåˆ—è¶³å¤Ÿ |
| æ¨è | éœ€è¦ç²¾åº¦æ—¶ | è®¡ç®—èµ„æºç´§å¼ æ—¶ |

---

## ğŸ’» ç®—æ³•å®ç°

### PyTorch å®Œæ•´å®ç°ï¼ˆLSTM ç¼–ç å™¨-è§£ç å™¨ï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np

class SimpleRNNCell(nn.Module):
    """åŸºç¡€ RNN å•å…ƒï¼ˆæ¼”ç¤ºç”¨ï¼‰"""

    def __init__(self, input_size, hidden_size):
        super(SimpleRNNCell, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size

        # å‚æ•°å…±äº«
        self.W_ih = nn.Linear(input_size, hidden_size)   # è¾“å…¥ â†’ éšçŠ¶æ€
        self.W_hh = nn.Linear(hidden_size, hidden_size)  # éšçŠ¶æ€ â†’ éšçŠ¶æ€
        self.tanh = nn.Tanh()

    def forward(self, x, h_prev):
        """
        Args:
            x: (batch_size, input_size)
            h_prev: (batch_size, hidden_size)
        Returns:
            h: (batch_size, hidden_size)
        """
        h = self.tanh(self.W_ih(x) + self.W_hh(h_prev))
        return h

class LSTMModel(nn.Module):
    """åŸºäº LSTM çš„åºåˆ—æ¨¡å‹"""

    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.5):
        super(LSTMModel, self).__init__()

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM å±‚
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,  # è¾“å…¥å½¢çŠ¶ï¼š(batch, seq_len, input_size)
            dropout=dropout if num_layers > 1 else 0
        )

        # è¾“å‡ºå±‚
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x, hidden=None):
        """
        Args:
            x: (batch_size, seq_len, input_size)
            hidden: tuple of (h_0, c_0)ï¼Œè‹¥ä¸º None åˆ™è‡ªåŠ¨åˆå§‹åŒ–ä¸º 0
        Returns:
            output: (batch_size, seq_len, output_size) æˆ– (batch_size, output_size)
            hidden: (h_n, c_n)
        """
        lstm_out, hidden = self.lstm(x, hidden)  # (batch, seq_len, hidden_size)

        # ä»…ç”¨æœ€åæ—¶åˆ»çš„è¾“å‡ºè¿›è¡Œåˆ†ç±»
        last_output = lstm_out[:, -1, :]  # (batch_size, hidden_size)
        output = self.fc(last_output)     # (batch_size, output_size)

        return output, hidden

class BidirectionalLSTM(nn.Module):
    """åŒå‘ LSTMï¼ˆå¯æ•æ‰å‰å‘å’Œåå‘ä¿¡æ¯ï¼‰"""

    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.5):
        super(BidirectionalLSTM, self).__init__()

        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,  # å…³é”®ï¼šåŒå‘
            dropout=dropout if num_layers > 1 else 0
        )

        # è¾“å‡ºå±‚ï¼šåŒå‘åéšçŠ¶æ€ç»´åº¦åŠ å€
        self.fc = nn.Linear(hidden_size * 2, output_size)

    def forward(self, x):
        lstm_out, hidden = self.lstm(x)  # (batch, seq_len, hidden_size * 2)

        # æœ€åæ—¶åˆ»çš„è¾“å‡ºï¼ˆåŒ…å«å‰å‘å’Œåå‘ä¿¡æ¯ï¼‰
        last_output = lstm_out[:, -1, :]  # (batch_size, hidden_size * 2)
        output = self.fc(last_output)

        return output

class Seq2SeqModel(nn.Module):
    """åºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼ˆç¼–ç å™¨-è§£ç å™¨ï¼‰"""

    def __init__(self, input_size, hidden_size, output_size, num_layers=2):
        super(Seq2SeqModel, self).__init__()

        # ç¼–ç å™¨ï¼šè¯»å–è¾“å…¥åºåˆ—ï¼Œç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡
        self.encoder = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )

        # è§£ç å™¨ï¼šä»ä¸Šä¸‹æ–‡å‘é‡ç”Ÿæˆè¾“å‡ºåºåˆ—
        self.decoder = nn.LSTM(
            input_size=output_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )

        self.fc = nn.Linear(hidden_size, output_size)
        self.output_size = output_size

    def forward(self, src, tgt, teacher_forcing_ratio=0.5):
        """
        Args:
            src: (batch_size, src_len, input_size) - æºåºåˆ—
            tgt: (batch_size, tgt_len, output_size) - ç›®æ ‡åºåˆ—ï¼ˆç”¨äº teacher forcingï¼‰
            teacher_forcing_ratio: ä½¿ç”¨çœŸå®æ ‡ç­¾çš„æ¦‚ç‡
        Returns:
            output: (batch_size, tgt_len, output_size) - é¢„æµ‹çš„ç›®æ ‡åºåˆ—
        """
        batch_size = src.shape[0]
        tgt_len = tgt.shape[1]

        # ç¼–ç å™¨ï¼šå¤„ç†è¾“å…¥åºåˆ—ï¼Œè·å¾—ä¸Šä¸‹æ–‡å‘é‡ï¼ˆæœ€åçš„éšçŠ¶æ€ï¼‰
        _, (hidden, cell) = self.encoder(src)  # hidden: (num_layers, batch, hidden_size)

        # è§£ç å™¨ï¼šé€æ—¶åˆ»ç”Ÿæˆè¾“å‡º
        decoder_input = tgt[:, 0, :].unsqueeze(1)  # ç¬¬ä¸€ä¸ªæ—¶åˆ»çš„ç›®æ ‡ï¼ˆbatch, 1, output_sizeï¼‰
        outputs = []

        for t in range(1, tgt_len):
            # è§£ç ä¸€æ­¥
            decoder_output, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))
            # (1, batch, hidden_size)

            # è¾“å‡ºå±‚
            output = self.fc(decoder_output.squeeze(1))  # (batch, output_size)
            outputs.append(output)

            # å†³å®šä¸‹ä¸€æ—¶åˆ»çš„è¾“å…¥ï¼šteacher forcing æˆ– è‡ªå›å½’
            if np.random.random() < teacher_forcing_ratio:
                decoder_input = tgt[:, t, :].unsqueeze(1)  # çœŸå®ä¸‹ä¸€æ—¶åˆ»
            else:
                decoder_input = output.unsqueeze(1)  # æ¨¡å‹é¢„æµ‹çš„ä¸‹ä¸€æ—¶åˆ»

        outputs = torch.stack(outputs, dim=1)  # (batch, tgt_len-1, output_size)
        return outputs

# ===== è®­ç»ƒå™¨ =====
class RNNTrainer:
    """RNN è®­ç»ƒå™¨"""

    def __init__(self, model, learning_rate=1e-3, device='cpu'):
        self.model = model.to(device)
        self.device = device
        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        self.criterion = nn.CrossEntropyLoss()

    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ª epoch"""
        self.model.train()
        epoch_loss = 0

        for X_batch, y_batch in train_loader:
            X_batch = X_batch.to(self.device)  # (batch, seq_len, input_size)
            y_batch = y_batch.to(self.device)

            output, _ = self.model(X_batch)
            loss = self.criterion(output, y_batch)

            self.optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

            self.optimizer.step()

            epoch_loss += loss.item()

        return epoch_loss / len(train_loader)

    def validate(self, val_loader):
        """éªŒè¯"""
        self.model.eval()
        val_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch = X_batch.to(self.device)
                y_batch = y_batch.to(self.device)

                output, _ = self.model(X_batch)
                loss = self.criterion(output, y_batch)
                val_loss += loss.item()

                _, predicted = torch.max(output, 1)
                correct += (predicted == y_batch).sum().item()
                total += y_batch.size(0)

        return val_loss / len(val_loader), correct / total

    def train(self, train_loader, val_loader, epochs=10):
        """å®Œæ•´è®­ç»ƒå¾ªç¯"""
        train_losses = []
        val_losses = []
        val_accs = []

        for epoch in range(epochs):
            train_loss = self.train_epoch(train_loader)
            val_loss, val_acc = self.validate(val_loader)

            train_losses.append(train_loss)
            val_losses.append(val_loss)
            val_accs.append(val_acc)

            if (epoch + 1) % 2 == 0:
                print(f"Epoch {epoch+1}/{epochs}, "
                      f"Train Loss: {train_loss:.4f}, "
                      f"Val Loss: {val_loss:.4f}, "
                      f"Val Acc: {val_acc:.4f}")

        return train_losses, val_losses, val_accs

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    # ç”Ÿæˆæ¨¡æ‹Ÿæ—¶é—´åºåˆ—æ•°æ®
    batch_size = 32
    seq_len = 50
    input_size = 10
    hidden_size = 128
    num_classes = 5
    num_samples = 500

    # éšæœºç”Ÿæˆåºåˆ—å’Œæ ‡ç­¾
    X = torch.randn(num_samples, seq_len, input_size)
    y = torch.randint(0, num_classes, (num_samples,))

    dataset = TensorDataset(X, y)
    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    # åˆ›å»ºæ¨¡å‹
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = LSTMModel(
        input_size=input_size,
        hidden_size=hidden_size,
        num_layers=2,
        output_size=num_classes,
        dropout=0.5
    )

    # è®­ç»ƒ
    trainer = RNNTrainer(model, learning_rate=1e-3, device=device)
    train_losses, val_losses, val_accs = trainer.train(train_loader, val_loader, epochs=10)

    print("\nè®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ç»ˆéªŒè¯ç²¾åº¦ï¼š{val_accs[-1]:.4f}")

    # ===== å¯¹æ¯”ä¸åŒæ¨¡å‹ =====
    print("\n===== æ¨¡å‹å¯¹æ¯” =====")

    # æ¨¡å‹ 1ï¼šå•å±‚ LSTM
    model1 = LSTMModel(input_size=input_size, hidden_size=64, num_layers=1, output_size=num_classes)
    print(f"å•å±‚ LSTM å‚æ•°æ•°ï¼š{sum(p.numel() for p in model1.parameters())}")

    # æ¨¡å‹ 2ï¼šåŒå‘ LSTM
    model2 = BidirectionalLSTM(input_size=input_size, hidden_size=64, num_layers=2, output_size=num_classes)
    print(f"åŒå‘ LSTM å‚æ•°æ•°ï¼š{sum(p.numel() for p in model2.parameters())}")

    # æ¨¡å‹ 3ï¼šSeq2Seq
    model3 = Seq2SeqModel(input_size=input_size, hidden_size=hidden_size, output_size=input_size)
    print(f"Seq2Seq å‚æ•°æ•°ï¼š{sum(p.numel() for p in model3.parameters())}")
```

---

## ğŸ”§ è¶…å‚æ•°è°ƒä¼˜

### å…³é”®å‚æ•°è¯¦è§£

| å‚æ•° | å«ä¹‰ | å¯¹æ€§èƒ½çš„å½±å“ | æ¨èå€¼ |
|---|---|---|---|
| **hidden_size** | LSTM éšçŠ¶æ€ç»´åº¦ | è¶Šå¤§ â†’ è¡¨è¾¾èƒ½åŠ›å¼ºä½†å‚æ•°å¤šï¼›éœ€å¹³è¡¡ | 128-512 |
| **num_layers** | LSTM å †å å±‚æ•° | è¶Šæ·± â†’ æ›´å¤æ‚çš„ç‰¹å¾ï¼Œä½†éš¾ä»¥è®­ç»ƒï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰ï¼›é€šå¸¸ 2-3 è¶³å¤Ÿ | 2-3 |
| **dropout** | Dropout æ¯”ä¾‹ | é˜²è¿‡æ‹Ÿåˆï¼›å¤šå±‚ LSTM æ‰æœ‰æ•ˆï¼ˆå±‚é—´ dropoutï¼‰| 0.3-0.5 |
| **learning_rate** | å­¦ä¹ ç‡ | RNN å¯¹ lr æ•æ„Ÿï¼›å¤ªå¤§æ˜“æ¢¯åº¦çˆ†ç‚¸ï¼Œå¤ªå°æ”¶æ•›æ…¢ | 1e-3-1e-4ï¼ˆéœ€æ¢¯åº¦è£å‰ªï¼‰ |
| **batch_size** | æ‰¹é‡å¤§å° | å° batch â†’ æ¢¯åº¦å™ªå£°å¤§ä½†é€ƒç¦»å±€éƒ¨æœ€å°å€¼ï¼›å¤§ batch â†’ ç¨³å®šä½†å¯èƒ½æ¬ æ‹Ÿåˆ | 32-64 |
| **seq_len** | åºåˆ—é•¿åº¦ | é•¿åºåˆ— â†’ BPTT éš¾åº¦å¤§ï¼Œæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é£é™©é«˜ï¼›å¯ç”¨æˆªæ–­ | 50-100ï¼ˆå¯æˆªæ–­ï¼‰ |
| **gradient_clip** | æ¢¯åº¦è£å‰ªé˜ˆå€¼ | RNN å¿…é¡»ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼›é€šå¸¸ 1.0 | 1.0 |
| **bidirectional** | æ˜¯å¦åŒå‘ | åŒå‘ â†’ å‚æ•°ç¿»å€ï¼Œç²¾åº¦ç•¥é«˜ï¼›åªèƒ½ç”¨äºéç”Ÿæˆä»»åŠ¡ | Trueï¼ˆåˆ†ç±»ï¼‰/ Falseï¼ˆç”Ÿæˆï¼‰ |

> [!TIP] RNN ç‰¹æœ‰çš„è°ƒä¼˜æŠ€å·§
>
> 1. **æ¢¯åº¦è£å‰ªï¼ˆå¿…é¡»ï¼‰**ï¼šRNN æ˜“æ¢¯åº¦çˆ†ç‚¸ï¼ˆä¸ä»…æ¶ˆå¤±ï¼‰
>    ```python
>    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
>    ```
> 2. **åºåˆ—é•¿åº¦æˆªæ–­**ï¼šé•¿åºåˆ— BPTT è®¡ç®—é‡çˆ†ç‚¸ï¼Œå¯æˆªæ–­ä¸º 32-50
> 3. **é—¨æœºåˆ¶é€‰æ‹©**ï¼šLSTMï¼ˆç²¾åº¦ï¼‰vs GRUï¼ˆé€Ÿåº¦ï¼‰
> 4. **åŒå‘é™åˆ¶**ï¼šåŒå‘åªé€‚åˆç¼–ç é˜¶æ®µï¼ˆåˆ†ç±»ã€NERï¼‰ï¼Œè§£ç ï¼ˆç”Ÿæˆï¼‰å¿…é¡»å•å‘
> 5. **åˆå§‹åŒ–**ï¼šæ­£äº¤åˆå§‹åŒ–æ¯”é«˜æ–¯æ›´ç¨³å®š
>    ```python
>    nn.init.orthogonal_(lstm.weight_hh_l0)
>    ```

---

## âš–ï¸ ä¼˜ç¼ºç‚¹ä¸åœºæ™¯

### âœ… ä¼˜åŠ¿ (Pros)

1. **å¤©ç„¶å¤„ç†å˜é•¿åºåˆ—**ï¼šå‚æ•°å…±äº«ä½¿æ¨¡å‹å¯¹ä»»æ„é•¿åº¦åºåˆ—é€‚ç”¨
2. **æ•æ‰é•¿æœŸä¾èµ–**ï¼ˆLSTM/GRUï¼‰ï¼šé—¨æœºåˆ¶ç›´æ¥è§£å†³æ¢¯åº¦æ¶ˆå¤±
3. **åŒå‘å»ºæ¨¡**ï¼šå¯åŒæ—¶åˆ©ç”¨å‰å‘å’Œåå‘ä¿¡æ¯
4. **å‚æ•°é«˜æ•ˆ**ï¼šç›¸æ¯”åŒç­‰æ·±åº¦çš„å…¨è¿æ¥ç½‘ç»œå°‘å¾ˆå¤šå‚æ•°
5. **å¯è§£é‡Šçš„éšçŠ¶æ€**ï¼š$h_t$ æ˜¾å¼ä¼ é€’ï¼Œå¯è§†åŒ–ç†è§£

### âŒ åŠ£åŠ¿ (Cons)

1. **ä¸²è¡Œè®¡ç®—**ï¼šæ— æ³•å¹¶è¡Œå¤„ç†åºåˆ—ï¼Œè®­ç»ƒæ…¢ï¼ˆç›¸æ¯” Transformerï¼‰
2. **é•¿åºåˆ—å›°éš¾**ï¼šå³ä½¿ LSTMï¼Œå¤ªé•¿åºåˆ—ï¼ˆ>1000ï¼‰ä»æ¢¯åº¦æ¶ˆå¤±
3. **BPTT è®¡ç®—å¤æ‚**ï¼šéœ€ä»è¾“å‡ºåå‘è®¡ç®—åˆ°è¾“å…¥ï¼Œå†…å­˜å ç”¨å¤§
4. **æ¢¯åº¦çˆ†ç‚¸**ï¼šRNN ç‰¹æœ‰ï¼ˆCNN/Transformer æ— ï¼‰ï¼Œéœ€æ¢¯åº¦è£å‰ª
5. **å·²è¢« Transformer è¶…è¶Š**ï¼šåŒç­‰å‚æ•°ä¸‹ Transformer ç²¾åº¦æ›´é«˜

### ğŸ¯ é€‚ç”¨åœºæ™¯

| åœºæ™¯ | é€‚ç”¨åº¦ | åŸå›  |
|---|---|---|
| **è¯­è¨€å»ºæ¨¡** | â­â­â­â­ | ç»å…¸ç”¨é€”ï¼Œè™½ Transformer æ›´ä¼˜ä½†ä»ç”¨ |
| **æœºå™¨ç¿»è¯‘** | â­â­â­ | seq2seq æ ‡é…ï¼Œä½†ç°åœ¨ Transformer æ›´ä½³ |
| **åºåˆ—æ ‡æ³¨ï¼ˆNERã€POSï¼‰** | â­â­â­â­ | åŒå‘ LSTM æ ‡å‡†ï¼Œé¢„è®­ç»ƒ Transformer è¶…è¶Š |
| **æƒ…æ„Ÿåˆ†æ** | â­â­â­â­ | å¯ç”¨ï¼Œä½† BERT æ›´ä¼˜ |
| **æ—¶é—´åºåˆ—é¢„æµ‹** | â­â­â­â­â­ | æ“…é•¿ï¼Œæ•æ‰æ—¶é—´ä¾èµ–ï¼›æœ‰äº›æ—¶åºä»»åŠ¡ LSTM ä»æœ€ä¼˜ |
| **æ–‡æœ¬ç”Ÿæˆ** | â­â­â­ | å¯ç”¨ï¼Œä½† Transformer/GPT æ›´å¼º |
| **æ–‡æœ¬åˆ†ç±»** | â­â­â­ | å¯ç”¨ï¼Œä½†ä¸å¦‚ BERT/Transformer |
| **å¯¹è¯ç³»ç»Ÿ** | â­â­â­ | seq2seq æ›¾ç»ä¸»æµï¼Œç° Transformer + å¾®è°ƒæ›´ä¼˜ |

---

## ğŸ’¬ é¢è¯•å¿…é—®

> [!question] Q1: æ¨å¯¼ RNN çš„æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼ŒLSTM æ˜¯å¦‚ä½•è§£å†³çš„ï¼Ÿ
>
> **ç­”æ¡ˆæ¡†æ¶**ï¼š
>
> **æ¢¯åº¦æ¶ˆå¤±åˆ†æ**ï¼š
>
> RNN çš„å‚æ•° $W_{hh}$ åœ¨æ‰€æœ‰æ—¶åˆ»å…±äº«ã€‚ä»æ—¶åˆ» 1 åˆ°æ—¶åˆ» $T$ çš„æ¢¯åº¦é“¾å¼ï¼š
>
> $$\frac{\partial h_T}{\partial h_1} = \prod_{t=2}^{T} \frac{\partial h_t}{\partial h_{t-1}} = \prod_{t=2}^{T} \text{tanh}'(z_t) \cdot W_{hh}$$
>
> å…¶ä¸­ $\text{tanh}'(z) = 1 - \tanh^2(z) \leq 1$ï¼Œæ‰€ä»¥ï¼š
>
> $$\left|\frac{\partial h_T}{\partial h_1}\right| \leq \|W_{hh}\|^{T-1} \cdot \prod_t (1 - \tanh^2(z_t))$$
>
> å½“ $T$ å¾ˆå¤§æ—¶ï¼ˆå¦‚ 100 æ­¥ï¼‰ï¼Œè‹¥ $\|W_{hh}\| < 1$ï¼Œæ¢¯åº¦å‘ˆæŒ‡æ•°è¡°å‡ï¼š$0.9^{99} \approx 10^{-5}$
>
> **æ¢¯åº¦çˆ†ç‚¸**ï¼šè‹¥ $\|W_{hh}\| > 1$ï¼Œåè€Œæ¢¯åº¦æŒ‡æ•°å¢é•¿ï¼Œå¯¼è‡´æ•°å€¼æº¢å‡ºã€‚
>
> **LSTM çš„è§£å†³**ï¼š
>
> LSTM çš„å•å…ƒçŠ¶æ€ $c_t$ æœ‰ä¸“é—¨çš„"é«˜é€Ÿé€šé“"ï¼š
>
> $$c_t = f_t \odot c_{t-1} + i_t \odot \tilde{h}_t$$
>
> åå‘ä¼ æ’­æ—¶çš„æ¢¯åº¦ï¼š
>
> $$\frac{\partial L}{\partial c_{t-1}} = \frac{\partial L}{\partial c_t} \odot f_t$$
>
> è¿™é‡Œæ˜¯ **element-wise ä¹˜æ³•è€ŒéçŸ©é˜µä¹˜æ³•**ï¼Œä¸” $f_t$ é€šå¸¸æ¥è¿‘ 1ï¼ˆé—å¿˜é—¨ï¼‰ï¼Œæ‰€ä»¥æ¢¯åº¦ä¸ä¼šæŒ‡æ•°è¡°å‡ã€‚å¯è¯æ˜LSTM æ¢¯åº¦ $\leq 1 + O(\text{gateå˜åŒ–})$ï¼Œä¸éš $T$ æŒ‡æ•°è¡°å‡ã€‚
>
> **å¯¹æ¯”**ï¼šRNN æ¢¯åº¦ $\propto 0.9^T$ï¼ŒLSTM æ¢¯åº¦ä¿æŒ $O(1)$

> [!question] Q2: LSTM çš„ä¸‰ä¸ªé—¨ï¼ˆè¾“å…¥é—¨ã€é—å¿˜é—¨ã€è¾“å‡ºé—¨ï¼‰å„è‡ªçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿèƒ½å¦ç”¨æ›´å°‘é—¨æ•°ä»£æ›¿ï¼Ÿ
>
> **ç­”æ¡ˆæ ¸å¿ƒ**ï¼š
>
> **ä¸‰ä¸ªé—¨çš„è§’è‰²åˆ†å·¥**ï¼š
> - **é—å¿˜é—¨** $f_t$ï¼šæ§åˆ¶è¿‡å»ä¿¡æ¯çš„ä¿ç•™ç¨‹åº¦ï¼ˆ$\approx 1$ æ—¶ä¿ç•™ï¼Œ$\approx 0$ æ—¶é—å¿˜ï¼‰
> - **è¾“å…¥é—¨** $i_t$ï¼šæ§åˆ¶æ–°ä¿¡æ¯çš„åŠ å…¥é‡ï¼ˆ$\approx 1$ æ—¶æ¥çº³æ–°ä¿¡æ¯ï¼Œ$\approx 0$ æ—¶å¿½è§†ï¼‰
> - **è¾“å‡ºé—¨** $o_t$ï¼šæ§åˆ¶éšçŠ¶æ€è¾“å‡ºé‡ï¼ˆ$\approx 1$ æ—¶å®Œå…¨è¾“å‡ºï¼Œ$\approx 0$ æ—¶éšè—ï¼‰
>
> **ç›´è§‚ä¾‹å­**ï¼šåœ¨è¯­è¨€å»ºæ¨¡ä¸­ï¼Œé•¿è·ç¦»çš„ä»£è¯å›æŒ‡
> - "æˆ‘èµ°è¿›äº†ä¸€å®¶å’–å•¡å…...ï¼ˆè®¸å¤šè¯åï¼‰...æˆ‘ç‚¹äº†ä¸€æ¯å’–å•¡"
> - ç¬¬ä¸€ä¸ª"æˆ‘"å¯¹åº”æœ€åçš„"æˆ‘"ï¼Œä¸­é—´è®¸å¤šä¸ç›¸å…³çš„è¯éœ€é—å¿˜ï¼ˆé—å¿˜é—¨ï¼‰
> - å…³é”®è¯ï¼ˆå¦‚åŠ¨è¯"ç‚¹"ï¼‰éœ€å½“æ—¶è¾“å‡ºå½±å“å†³ç­–ï¼ˆè¾“å‡ºé—¨ï¼‰
>
> **èƒ½å¦ç”¨æ›´å°‘é—¨æ•°**ï¼š
> - **å•é—¨ï¼ˆGRUï¼‰**ï¼šåˆå¹¶è¾“å…¥é—¨å’Œé—å¿˜é—¨ä¸ºæ›´æ–°é—¨ï¼Œå‚æ•°å°‘ ~33%ï¼Œæ•ˆæœé€šå¸¸ç›¸è¿‘
> - **æ— é—¨ï¼ˆæ ‡å‡† RNNï¼‰**ï¼šä¸è¡Œï¼Œä¼šæ¢¯åº¦æ¶ˆå¤±ï¼ˆå·²è¯æ˜ï¼‰
> - **ç†è®ºä¸‹ç•Œ**ï¼šéœ€è‡³å°‘æŸç§å½¢å¼çš„"é€‰æ‹©æœºåˆ¶"ï¼Œä¸‰é—¨æ˜¯å®Œæ•´è®¾è®¡ï¼Œå°‘äºä¸‰é—¨å¿…ç„¶æŸå¤±è¡¨è¾¾èƒ½åŠ›

> [!question] Q3: RNN vs Transformer çš„æœ¬è´¨å·®å¼‚æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆ Transformer åœ¨ NLP ä¸­é€æ¸å–ä»£äº† RNNï¼Ÿ
>
> **ç­”æ¡ˆæ ¸å¿ƒ**ï¼š
>
> **æœ¬è´¨å·®å¼‚**ï¼š
>
> | ç»´åº¦ | RNN | Transformer |
> |---|---|---|
> | **ä¾èµ–å…³ç³»** | **é¡ºåºä¾èµ–**ï¼ˆé€æ­¥ï¼‰| **å…¨è¿æ¥ä¾èµ–**ï¼ˆä¸€æ­¥ï¼‰ |
> | **è®¡ç®—å¤æ‚åº¦** | $O(T \cdot d^2)$ | $O(T^2 \cdot d)$ |
> | **æ¢¯åº¦è·¯å¾„é•¿åº¦** | $O(T)$ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰ | $O(1)$ï¼ˆç›´æ¥ï¼‰ |
> | **å¹¶è¡Œæ€§** | æ— ï¼ˆé€æ­¥ï¼‰ | å®Œå…¨ï¼ˆä¸€æ­¥ï¼‰ |
> | **ä½ç½®åå¥½** | å¤©ç„¶ï¼ˆæ—¶é—´æ­¥ï¼‰| éœ€æ˜¾å¼ç¼–ç  |
>
> **æ¢¯åº¦æµå¯¹æ¯”**ï¼š
> - RNNï¼šæœ€åä¸€æ­¥çš„æ¢¯åº¦å›ä¼ åˆ°ç¬¬ä¸€æ­¥ï¼Œéœ€ä¹˜ä»¥ $T$ ä¸ªä¸­é—´æ¢¯åº¦ï¼ŒæŒ‡æ•°è¡°å‡
> - Transformerï¼šä»»æ„ä¸¤ä½ç½®é—´çš„æ¢¯åº¦è·¯å¾„é•¿åº¦ä¸º 1ï¼ˆself-attentionï¼‰ï¼Œç›´æ¥æ— è¡°å‡
>
> **ä¸ºä»€ä¹ˆ Transformer å–ä»£ RNN**ï¼š
> 1. **è®­ç»ƒå¿«**ï¼šå®Œå…¨å¹¶è¡Œï¼Œç›¸åŒè®¡ç®—é¢„ç®—ä¸‹ epoch æ•°æ›´å¤š
> 2. **ç²¾åº¦é«˜**ï¼šæ¢¯åº¦æ›´å¥½ï¼Œç›¸åŒå‚æ•°ä¸‹ç²¾åº¦æ›´ä¼˜
> 3. **é•¿åºåˆ—å‹å¥½**ï¼šè™½ç„¶ $O(T^2)$ å†…å­˜ï¼Œä½†ç›´æ¥ä¾èµ–æ— æ¢¯åº¦æ¶ˆå¤±ï¼Œå®é™…å¯å¤„ç†æ›´é•¿åºåˆ—
> 4. **é¢„è®­ç»ƒå‹å¥½**ï¼šå¤§è§„æ¨¡é¢„è®­ç»ƒï¼ˆBERTã€GPTï¼‰æ›´æ˜“æ”¶æ•›å’Œæ‰©å±•
> 5. **ç¼ºç‚¹**ï¼ˆç›¸å¯¹ï¼‰ï¼š$O(T^2)$ å†…å­˜å¯¹è¶…é•¿åºåˆ—å›°éš¾ï¼Œä½†å·¥ç¨‹ä¸Šå¯ç”¨åˆ†å—ã€ç¨€ç–ç­‰ä¼˜åŒ–
>
> **RNN çš„åšæŒåœºæ™¯**ï¼š
> - åœ¨çº¿/æµå¼æ¨ç†ï¼ˆTransformer éœ€ç¼“å­˜æ‰€æœ‰å†å²ï¼‰
> - è¶…é•¿åºåˆ—ï¼ˆ>100K tokensï¼‰
> - å®æ—¶æ€§è¦æ±‚ï¼ˆé€è¯è¾“å‡º Transformer æœ‰å»¶è¿Ÿï¼‰
> - æŸäº›ä¸“ä¸šé¢†åŸŸï¼ˆå¦‚é‡‘èæ—¶åºé¢„æµ‹ä¸­ LSTM ä»é¢†å…ˆï¼‰