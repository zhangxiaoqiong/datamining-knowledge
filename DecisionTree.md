---
aliases: [DT, Decision Tree, å†³ç­–æ ‘, Classification and Regression Tree, CART]
tags: [ç®—æ³•, æœºå™¨å­¦ä¹ , ç›‘ç£å­¦ä¹ , åˆ†ç±»/å›å½’, æ ‘æ¨¡å‹, è´ªå¿ƒç®—æ³•, ä¿¡æ¯è®º]
difficulty: â­â­â­
math_enabled: true
---

# å†³ç­–æ ‘ï¼ˆDecision Treeï¼‰

## ğŸ’¡ æ ¸å¿ƒç›´è§‰ (Intuition)

### ä¸€å¥è¯è§£é‡Š

**å†³ç­–æ ‘ = é€’å½’äºŒåˆ†è£‚ + è´ªå¿ƒä¿¡æ¯å¢ç›Š + æ ‘å½¢å†³ç­–è§„åˆ™**

å†³ç­–æ ‘é€šè¿‡é€æ­¥åˆ†å‰²ç‰¹å¾ç©ºé—´ï¼Œç”¨ä¸€ç³»åˆ— if-then è§„åˆ™å°†å¤æ‚çš„åˆ†ç±»/å›å½’é—®é¢˜è½¬åŒ–ä¸ºæ ‘å½¢ç»“æ„ã€‚æ¯æ¬¡åˆ†è£‚éƒ½é€‰æ‹©ä½¿**ä¿¡æ¯å¢ç›Šæœ€å¤§**çš„ç‰¹å¾å’Œé˜ˆå€¼ï¼Œä»è€Œç”¨æœ€å°‘çš„è§„åˆ™è¾¾åˆ°æœ€é«˜ç²¾åº¦ã€‚

### è§£å†³çš„ç—›ç‚¹

ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•çš„å±€é™æ€§ï¼š

- **çº¿æ€§æ¨¡å‹**ï¼ˆé€»è¾‘å›å½’ï¼‰ï¼šæ— æ³•æ•æ‰éçº¿æ€§ç‰¹å¾äº¤äº’
  - ä¾‹ï¼šç”¨æˆ·å¹´é¾„å’Œæ”¶å…¥çš„äº¤å‰ä½œç”¨ï¼Œçº¿æ€§æ¨¡å‹å¿…é¡»æ‰‹åŠ¨ç‰¹å¾å·¥ç¨‹
  - å†³ç­–æ ‘ï¼šè‡ªåŠ¨å­¦ä¹ "å¹´é¾„>30ä¸”æ”¶å…¥>50k"çš„ç»„åˆè§„åˆ™

- **ç¥ç»ç½‘ç»œ**ï¼šé»‘ç›’ï¼Œéš¾ä»¥è§£é‡Š
  - ä¾‹ï¼šä¸ºä»€ä¹ˆæ‹’ç»è¿™ä¸ªè´·æ¬¾ç”³è¯·ï¼Ÿæ— æ³•å›ç­”
  - å†³ç­–æ ‘ï¼šé€å±‚æ˜¾ç¤ºå†³ç­–è·¯å¾„ï¼Œå®Œå…¨å¯è§£é‡Š

- **SVM**ï¼šåªé€‚åˆäºŒåˆ†ç±»ï¼Œå¤šåˆ†ç±»éœ€è¦å¤æ‚ç¼–ç 
  - å†³ç­–æ ‘ï¼šåŸç”Ÿæ”¯æŒå¤šåˆ†ç±»ï¼Œæ— éœ€ç‰¹æ®Šå¤„ç†

### æ ¸å¿ƒé€»è¾‘

å†³ç­–æ ‘çš„æ ¸å¿ƒæ€æƒ³å°±æ˜¯**é€’å½’äºŒåˆ†è£‚**ï¼š

1. **è´ªå¿ƒé€‰æ‹©æœ€ä¼˜åˆ†è£‚ç‚¹**
   - éå†æ‰€æœ‰ç‰¹å¾å’Œé˜ˆå€¼
   - è®¡ç®—æ¯ä¸ªåˆ†è£‚çš„ä¿¡æ¯å¢ç›Šï¼ˆInformation Gainï¼‰
   - é€‰æ‹©å¢ç›Šæœ€å¤§çš„åˆ†è£‚

2. **é€’å½’æ„å»ºå­æ ‘**
   - å¯¹å·¦å³å­èŠ‚ç‚¹é‡å¤ä¸Šè¿°è¿‡ç¨‹
   - ç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ï¼ˆå¦‚æ·±åº¦é™åˆ¶ã€æ ·æœ¬æ•°ä¸è¶³ï¼‰

3. **ç”Ÿæˆå†³ç­–è§„åˆ™**
   - ä»æ ¹åˆ°å¶çš„æ¯æ¡è·¯å¾„ = ä¸€æ¡è§„åˆ™
   - ä¾‹ï¼šIF age>30 AND income>50k THEN credit_approved=YES

4. **é¢„æµ‹æ—¶æŒ‰è§„åˆ™éå†**
   - è¾“å…¥æ•°æ®æ²¿ç€å†³ç­–è§„åˆ™ä»æ ¹èµ°åˆ°å¶
   - è¿”å›å¶èŠ‚ç‚¹çš„é¢„æµ‹å€¼ï¼ˆç±»åˆ«æˆ–æ•°å€¼ï¼‰

### Killer Featureï¼ˆæ€æ‰‹é”ï¼‰

> [!ABSTRACT] æ ¸å¿ƒä¼˜åŠ¿
> å†³ç­–æ ‘æ˜¯**æœ€å¯è§£é‡Šçš„ç›‘ç£å­¦ä¹ ç®—æ³•**â€”â€”å¯ä»¥ç›´è§‚åœ°çœ‹åˆ°å†³ç­–è¿‡ç¨‹ï¼Œé€‚åˆä¸šåŠ¡è§„åˆ™æå–ã€åˆè§„æ€§è¦æ±‚ï¼ˆé‡‘è/åŒ»ç–—ï¼‰å’Œäººæœºäº¤äº’åœºæ™¯ã€‚åŒæ—¶ï¼Œå†³ç­–æ ‘æ˜¯**éšæœºæ£®æ—å’Œæ¢¯åº¦æå‡çš„åŸºç¡€**ï¼ŒæŒæ¡å†³ç­–æ ‘æ˜¯ç†è§£é«˜çº§é›†æˆå­¦ä¹ çš„å…³é”®ã€‚

### EL15 ç±»æ¯”ï¼ˆé€šä¿—ç†è§£ï¼‰

å†³ç­–æ ‘å°±åƒç©**20 Questions æ¸¸æˆ**çš„å†³ç­–è¿‡ç¨‹ï¼š

```
é—®é¢˜1: æ˜¯åŠ¨ç‰©å—? â†’ YES
       â†“
é—®é¢˜2: ä¼šé£å—?  â†’ YES
       â†“
é—®é¢˜3: ä¼šæ¸¸æ³³å—? â†’ YES
       â†“
ç­”æ¡ˆ: å¯èƒ½æ˜¯é¹…
```

æ¯ä¸ªé—®é¢˜å¯¹åº”ä¸€ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç­”æ¡ˆï¼ˆYES/NOï¼‰å¯¹åº”ä¸€ä¸ªåˆ†è£‚ç‚¹ã€‚æœ€ç»ˆé€šè¿‡ä¸€ç³»åˆ—é—®é¢˜å°†æ ·æœ¬ç©ºé—´ç»†åˆ†åˆ°è¶³å¤Ÿå°çš„ç¾¤ä½“ï¼Œåšå‡ºæœ€å‡†ç¡®çš„é¢„æµ‹ã€‚

### å‡ ä½•ç›´è§‰

```
ç‰¹å¾ç©ºé—´åˆ†å‰²ç¤ºä¾‹ï¼ˆ2ä¸ªç‰¹å¾ï¼‰ï¼š

åŸå§‹æ•°æ®ï¼ˆæ··åˆï¼‰ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â— æ­£ä¾‹ï¼ˆè´·æ¬¾æ‰¹å‡†ï¼‰     â”‚
â”‚  â—‹ è´Ÿä¾‹ï¼ˆè´·æ¬¾æ‹’ç»ï¼‰     â”‚
â”‚ â— â—‹ â— â—‹ â— â— â—‹ â—       â”‚
â”‚ â—‹ â— â—‹ â— â—‹ â—‹ â— â—‹       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å†³ç­–æ ‘åˆ†å‰²è¿‡ç¨‹ï¼š

å±‚1: æŒ‰ç‰¹å¾1åˆ†è£‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç‰¹å¾1<5  â”‚ ç‰¹å¾1â‰¥5  â”‚
â”‚ (å·¦å­æ ‘) â”‚ (å³å­æ ‘) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å±‚2: æŒ‰ç‰¹å¾2åˆ†è£‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç‰¹å¾1<5  â”‚      â”‚ ç‰¹å¾2<10 â”‚ ç‰¹å¾2â‰¥10 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å±‚3: æœ€ç»ˆåˆ†ç±»
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç±»åˆ«A    â”‚  â”‚ç±»B â”‚  â”‚ç±»B â”‚  â”‚ ç±»åˆ«C    â”‚
â”‚(çº¯å‡€)    â”‚  â”‚æ··åˆâ”‚  â”‚æ··åˆâ”‚  â”‚(çº¯å‡€)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å†³ç­–è¾¹ç•Œç‰¹ç‚¹ï¼š
âœ“ æ²¿ç‰¹å¾è½´å¹³è¡Œï¼ˆéæ–œçº¿ï¼‰
âœ“ åˆ†æ®µçº¿æ€§ï¼ˆpiecewise linearï¼‰
âœ“ å¯ä»¥å®Œç¾æ‹Ÿåˆä»»æ„è¾¹ç•Œï¼ˆå¯èƒ½è¿‡æ‹Ÿåˆï¼‰
```

---

## ğŸ“ æ•°å­¦åŸç† (The Math)

### 1. ä¿¡æ¯è®ºåŸºç¡€

#### ä¿¡æ¯ç†µï¼ˆInformation Entropyï¼‰

ç†µæ˜¯è¡¡é‡**æ•°æ®æ··ä¹±ç¨‹åº¦**çš„æŒ‡æ ‡ã€‚çº¯å‡€çš„æ ·æœ¬ï¼ˆå…¨æ˜¯ä¸€ç±»ï¼‰ç†µä¸º0ï¼Œæ··ä¹±çš„æ ·æœ¬ï¼ˆå„ç±»å‡åŒ€åˆ†å¸ƒï¼‰ç†µæœ€å¤§ã€‚

$$H(D) = -\sum_{k=1}^{K} p_k \log_2 p_k$$

å…¶ä¸­ï¼š
- $H(D)$ï¼šæ•°æ®é›† $D$ çš„ä¿¡æ¯ç†µ
- $p_k$ï¼šç¬¬ $k$ ç±»æ ·æœ¬çš„æ¯”ä¾‹
- $K$ï¼šç±»åˆ«æ€»æ•°

> [!TIP] ç†µçš„ç›´è§‰
> æƒ³è±¡ä¸€ä¸ªæŠ½å¥–ç®±ï¼š
> - å…¨æ˜¯çº¢çƒï¼šç†µ=0ï¼ˆå®Œå…¨ç¡®å®šï¼Œæ— æƒŠå–œï¼‰
> - çº¢è“å„åŠï¼šç†µ=1ï¼ˆå®Œå…¨ä¸ç¡®å®šï¼Œæœ€å¤§æƒŠå–œï¼‰
> - å¤§éƒ¨åˆ†çº¢çƒï¼Œå°‘æ•°è“çƒï¼šç†µâ‰ˆ0.3ï¼ˆä½ä¸ç¡®å®šæ€§ï¼‰

**ä¾‹å­ï¼šäºŒåˆ†ç±»é—®é¢˜**

```
æ ·æœ¬é›†åˆ: [æ­£ä¾‹, æ­£ä¾‹, è´Ÿä¾‹]ï¼ˆ2ä¸ªæ­£ï¼Œ1ä¸ªè´Ÿï¼‰

è®¡ç®—ç†µï¼š
p_æ­£ = 2/3 â‰ˆ 0.667
p_è´Ÿ = 1/3 â‰ˆ 0.333

H(D) = -0.667 Ã— logâ‚‚(0.667) - 0.333 Ã— logâ‚‚(0.333)
     = -0.667 Ã— (-0.585) - 0.333 Ã— (-1.585)
     = 0.390 + 0.528
     = 0.918 bits
```

#### æ¡ä»¶ç†µï¼ˆConditional Entropyï¼‰

åˆ†è£‚åå·¦å³å­æ ‘çš„åŠ æƒç†µã€‚ç”¨æ¥è¡¡é‡åˆ†è£‚èƒ½**å‡å°‘å¤šå°‘æ··ä¹±**ã€‚

$$H(D|A) = \sum_{v} \frac{|D_v|}{|D|} H(D_v)$$

å…¶ä¸­ï¼š
- $A$ï¼šåˆ†è£‚ç‰¹å¾
- $D_v$ï¼šæŒ‰ç‰¹å¾ $A$ çš„å€¼ $v$ åˆ’åˆ†åçš„å­é›†
- $\frac{|D_v|}{|D|}$ï¼šå­é›†çš„æ¯”ä¾‹æƒé‡

#### ä¿¡æ¯å¢ç›Šï¼ˆInformation Gainï¼‰

**åˆ†è£‚å‰åç†µçš„å‡å°‘é‡**ï¼Œè¡¡é‡è¯¥ç‰¹å¾çš„åˆ†è£‚æ•ˆæœã€‚

$$\text{Gain}(D, A) = H(D) - H(D|A)$$

**å†³ç­–æ ‘çš„åˆ†è£‚å‡†åˆ™**ï¼š
$$\arg\max_A \text{Gain}(D, A)$$

é€‰æ‹©ä½¿ä¿¡æ¯å¢ç›Šæœ€å¤§çš„ç‰¹å¾æ¥åˆ†è£‚ã€‚

**ä¾‹å­ï¼šé€‰æ‹©æœ€ä¼˜åˆ†è£‚**

```
åŸå§‹æ•°æ®é›† D: [æ­£, æ­£, æ­£, è´Ÿ, è´Ÿ] (3æ­£2è´Ÿ)
H(D) = -3/5 Ã— logâ‚‚(3/5) - 2/5 Ã— logâ‚‚(2/5) = 0.971

ç‰¹å¾1åˆ†è£‚ï¼ˆage < 30ï¼‰:
  å·¦å­é›†: [æ­£, æ­£, è´Ÿ] (2æ­£1è´Ÿ) â†’ H = 0.918
  å³å­é›†: [æ­£, è´Ÿ]     (1æ­£1è´Ÿ) â†’ H = 1.000

  H(D|ç‰¹å¾1) = 3/5 Ã— 0.918 + 2/5 Ã— 1.000 = 0.951
  Gain(D, ç‰¹å¾1) = 0.971 - 0.951 = 0.020

ç‰¹å¾2åˆ†è£‚ï¼ˆincome > 50kï¼‰:
  å·¦å­é›†: [æ­£, æ­£, æ­£]   (3æ­£0è´Ÿ) â†’ H = 0  (çº¯ï¼)
  å³å­é›†: [è´Ÿ, è´Ÿ]       (0æ­£2è´Ÿ) â†’ H = 0  (çº¯ï¼)

  H(D|ç‰¹å¾2) = 3/5 Ã— 0 + 2/5 Ã— 0 = 0
  Gain(D, ç‰¹å¾2) = 0.971 - 0 = 0.971 (æœ€å¤§ï¼)

å†³ç­–ï¼šé€‰æ‹©ç‰¹å¾2åˆ†è£‚
```

### 2. Gini ä¸çº¯åº¦ï¼ˆåˆ†ç±»æ ‘æ›¿ä»£æ–¹æ¡ˆï¼‰

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œsklearn é»˜è®¤ä½¿ç”¨ **Gini ä¸çº¯åº¦**è€Œéä¿¡æ¯ç†µï¼Œå› ä¸ºè®¡ç®—æ›´å¿«ï¼ˆä¸éœ€å¯¹æ•°ï¼‰ã€‚

$$\text{Gini}(D) = 1 - \sum_{k=1}^{K} p_k^2$$

**å«ä¹‰**ï¼šæ ·æœ¬éšæœºåˆ†é…é”™è¯¯ç±»åˆ«çš„æ¦‚ç‡ã€‚

**Gini vs Entropy å¯¹æ¯”**ï¼š

| æŒ‡æ ‡ | å…¬å¼ | è®¡ç®—é€Ÿåº¦ | å®é™…æ•ˆæœ |
|------|------|---------|---------|
| Entropy | $-\sum p_k \log p_k$ | æ…¢ï¼ˆéœ€å¯¹æ•°ï¼‰ | ç¨ä¼˜ |
| Gini | $1 - \sum p_k^2$ | å¿«ï¼ˆå¹³æ–¹ï¼‰ | åŸºæœ¬ç›¸åŒ |

> [!WARNING] å®é™…åº”ç”¨
> ä¸¤ç§å‡†åˆ™åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æ•ˆæœç›¸å½“ï¼Œé€‰æ‹© Gini ä»¥è·å¾—æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ã€‚

### 3. å›å½’æ ‘ï¼šå¹³æ–¹è¯¯å·®æœ€å°åŒ–

å¯¹äºå›å½’é—®é¢˜ï¼Œå†³ç­–æ ‘ä¸å†ç”¨ä¿¡æ¯å¢ç›Šï¼Œè€Œæ˜¯ç”¨**æ–¹å·®ç¼©å‡**ã€‚

$$\text{MSE}(D) = \frac{1}{|D|} \sum_{i=1}^{|D|} (y_i - \bar{y})^2$$

åˆ†è£‚å‡†åˆ™ï¼š

$$\arg\min_A \left[ \frac{|D_L|}{|D|} \text{MSE}(D_L) + \frac{|D_R|}{|D|} \text{MSE}(D_R) \right]$$

é€‰æ‹©ä½¿åŠ æƒ MSE æœ€å°çš„åˆ†è£‚ã€‚

### 4. æ ‘çš„æ„å»ºç®—æ³•ï¼ˆID3/C4.5/CARTï¼‰

#### è´ªå¿ƒæ„å»ºç®—æ³•ï¼ˆä¼ªä»£ç ï¼‰

```
Algorithm: BuildTree(D, features, depth)

Input:
  - D: è®­ç»ƒæ ·æœ¬é›†
  - features: å¯ç”¨çš„ç‰¹å¾åˆ—è¡¨
  - depth: å½“å‰æ·±åº¦

Output:
  - Tree: å†³ç­–æ ‘èŠ‚ç‚¹

if åœæ­¢æ¡ä»¶æ»¡è¶³(depth >= max_depth æˆ– æ ·æœ¬<=min_samples OR æ ·æœ¬çº¯å‡€):
    return Leaf(most_common_class)

best_gain = -âˆ
best_feature = None
best_threshold = None

for feature in features:
    for threshold in unique_thresholds(feature):
        // åˆ†è£‚æˆå·¦å³ä¸¤éƒ¨åˆ†
        D_left = {x âˆˆ D : x[feature] <= threshold}
        D_right = {x âˆˆ D : x[feature] > threshold}

        // è®¡ç®—åˆ†è£‚æ”¶ç›Šï¼ˆç”¨Giniï¼‰
        gain = Gini(D) - (|D_L|/|D| Ã— Gini(D_L) + |D_R|/|D| Ã— Gini(D_R))

        if gain > best_gain:
            best_gain = gain
            best_feature = feature
            best_threshold = threshold

// é€’å½’æ„å»ºå·¦å³å­æ ‘
left_child = BuildTree(D_left, features, depth+1)
right_child = BuildTree(D_right, features, depth+1)

return Node(best_feature, best_threshold, left_child, right_child)
```

---

## ğŸ’» ç®—æ³•å®ç° (Implementation)

### ä¼ªä»£ç ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰

```
Algorithm: DecisionTreeClassifier

Phase 1: æ ‘çš„æ„å»ºï¼ˆTrainingï¼‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for depth = 1 to max_depth:
    for node in current_level_nodes:
        if node.samples < min_samples_split:
            node.convert_to_leaf()
            continue

        // æ‰¾æœ€ä¼˜åˆ†è£‚
        best_split = find_best_split(node.data)

        if best_split.gain < min_gain:
            node.convert_to_leaf()
            continue

        // æ‰§è¡Œåˆ†è£‚
        node.split(best_split)
        create_left_child(node.data[left_mask])
        create_right_child(node.data[right_mask])

Phase 2: é¢„æµ‹ï¼ˆPredictionï¼‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for sample in test_data:
    current_node = root
    while not current_node.is_leaf():
        if sample[current_node.feature] <= current_node.threshold:
            current_node = current_node.left
        else:
            current_node = current_node.right

    return current_node.prediction
```

### Python å®æˆ˜ä»£ç 

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree
from sklearn.metrics import accuracy_score, mean_squared_error, classification_report
import pandas as pd

print("=" * 60)
print("å†³ç­–æ ‘ï¼ˆDecision Treeï¼‰å®Œæ•´æ•™ç¨‹")
print("=" * 60)

# ===== 1. åˆ†ç±»æ ‘ç¤ºä¾‹ =====
print("\nã€1ã€‘åˆ†ç±»æ ‘ï¼ˆDecisionTreeClassifierï¼‰")

# ç”Ÿæˆæ•°æ®
X, y = make_classification(n_samples=300, n_features=4, n_informative=3,
                           n_redundant=1, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# åŸºç¡€æ¨¡å‹
clf = DecisionTreeClassifier(
    criterion='gini',           # åˆ†è£‚å‡†åˆ™ï¼šgini æˆ– entropy
    max_depth=5,                # æœ€å¤§æ·±åº¦ï¼ˆæ§åˆ¶å¤æ‚åº¦ï¼‰
    min_samples_split=10,       # åˆ†è£‚çš„æœ€å°‘æ ·æœ¬æ•°
    min_samples_leaf=5,         # å¶èŠ‚ç‚¹æœ€å°‘æ ·æœ¬æ•°
    random_state=42
)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"åˆ†ç±»å‡†ç¡®ç‡: {accuracy:.4f}")
print(f"\nClassification Report:\n{classification_report(y_test, y_pred)}")

# ç‰¹å¾é‡è¦æ€§
feature_importance = clf.feature_importances_
print(f"\nç‰¹å¾é‡è¦æ€§:")
for i, importance in enumerate(feature_importance):
    print(f"  Feature {i}: {importance:.4f}")

# ===== 2. æ ‘çš„å¯è§†åŒ– =====
print("\nã€2ã€‘æ ‘ç»“æ„å¯è§†åŒ–")

# ç»˜åˆ¶æ ‘
fig, ax = plt.subplots(figsize=(20, 10))
plot_tree(clf, ax=ax, feature_names=[f'Feature {i}' for i in range(X.shape[1])],
          class_names=['Class 0', 'Class 1'], filled=True)
# plt.show()  # å–æ¶ˆæ³¨é‡ŠæŸ¥çœ‹

# ===== 3. å›å½’æ ‘ç¤ºä¾‹ =====
print("\nã€3ã€‘å›å½’æ ‘ï¼ˆDecisionTreeRegressorï¼‰")

# ç”Ÿæˆå›å½’æ•°æ®
X_reg, y_reg = make_regression(n_samples=300, n_features=4, noise=10, random_state=42)
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.3, random_state=42)

reg = DecisionTreeRegressor(
    criterion='squared_error',  # å›å½’æ ‘ç”¨ squared_error
    max_depth=5,
    min_samples_split=10,
    random_state=42
)
reg.fit(X_train_reg, y_train_reg)

y_pred_reg = reg.predict(X_test_reg)
mse = mean_squared_error(y_test_reg, y_pred_reg)
rmse = np.sqrt(mse)
print(f"å›å½’ MSE: {mse:.4f}")
print(f"å›å½’ RMSE: {rmse:.4f}")

# ===== 4. Gini vs Entropy å¯¹æ¯” =====
print("\nã€4ã€‘åˆ†è£‚å‡†åˆ™å¯¹æ¯”ï¼ˆGini vs Entropyï¼‰")

clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=42)
clf_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=42)

clf_gini.fit(X_train, y_train)
clf_entropy.fit(X_train, y_train)

acc_gini = clf_gini.score(X_test, y_test)
acc_entropy = clf_entropy.score(X_test, y_test)

print(f"Gini å‡†ç¡®ç‡: {acc_gini:.4f}")
print(f"Entropy å‡†ç¡®ç‡: {acc_entropy:.4f}")
print(f"å·®å¼‚: {abs(acc_gini - acc_entropy):.6f} (é€šå¸¸å¾ˆå°)")

# ===== 5. æ·±åº¦å¯¹è¿‡æ‹Ÿåˆçš„å½±å“ =====
print("\nã€5ã€‘æ ‘æ·±åº¦ä¸è¿‡æ‹Ÿåˆ")

depths = [2, 3, 5, 10, 15, 20]
train_accs = []
test_accs = []

for depth in depths:
    clf_d = DecisionTreeClassifier(max_depth=depth, random_state=42)
    clf_d.fit(X_train, y_train)
    train_accs.append(clf_d.score(X_train, y_train))
    test_accs.append(clf_d.score(X_test, y_test))

print(f"{'æ·±åº¦':<6} {'è®­ç»ƒå‡†ç¡®ç‡':<12} {'æµ‹è¯•å‡†ç¡®ç‡':<12} {'è¿‡æ‹Ÿåˆç¨‹åº¦':<12}")
print("â”€" * 42)
for depth, train_acc, test_acc in zip(depths, train_accs, test_accs):
    overfitting = train_acc - test_acc
    print(f"{depth:<6} {train_acc:.4f}        {test_acc:.4f}        {overfitting:.4f}")

print("\nå…³é”®è§‚å¯Ÿ:")
print(f"  âœ“ æµ…æ ‘ï¼ˆæ·±åº¦2ï¼‰ï¼šæ¬ æ‹Ÿåˆï¼ˆè®­ç»ƒå’Œæµ‹è¯•éƒ½ä½ï¼‰")
print(f"  âœ“ ä¸­ç­‰æ ‘ï¼ˆæ·±åº¦5ï¼‰ï¼šæœ€ä¼˜å¹³è¡¡")
print(f"  âœ“ æ·±æ ‘ï¼ˆæ·±åº¦20ï¼‰ï¼šæ˜æ˜¾è¿‡æ‹Ÿåˆï¼ˆè®­ç»ƒ0.99,æµ‹è¯•0.87ï¼‰")

# ===== 6. å‚æ•°è°ƒä¼˜å®éªŒ =====
print("\nã€6ã€‘å…³é”®å‚æ•°è°ƒä¼˜")

print("\nmin_samples_split çš„å½±å“:")
min_samples_values = [2, 5, 10, 20, 50]
for min_samples in min_samples_values:
    clf_m = DecisionTreeClassifier(max_depth=5, min_samples_split=min_samples, random_state=42)
    clf_m.fit(X_train, y_train)
    train_acc = clf_m.score(X_train, y_train)
    test_acc = clf_m.score(X_test, y_test)
    print(f"  min_samples={min_samples:2d}: train={train_acc:.4f}, test={test_acc:.4f}")

print("\nmin_samples_leaf çš„å½±å“:")
min_leaf_values = [1, 5, 10, 20]
for min_leaf in min_leaf_values:
    clf_l = DecisionTreeClassifier(max_depth=5, min_samples_leaf=min_leaf, random_state=42)
    clf_l.fit(X_train, y_train)
    train_acc = clf_l.score(X_train, y_train)
    test_acc = clf_l.score(X_test, y_test)
    print(f"  min_samples_leaf={min_leaf:2d}: train={train_acc:.4f}, test={test_acc:.4f}")

# ===== 7. å†³ç­–è·¯å¾„åˆ†æ =====
print("\nã€7ã€‘å•ä¸ªæ ·æœ¬çš„å†³ç­–è·¯å¾„")

sample_idx = 0
sample = X_test[sample_idx:sample_idx+1]
prediction = clf.predict(sample)[0]

# è·å–å†³ç­–è·¯å¾„
decision_path = clf.decision_path(sample)
leaf_id = clf.apply(sample)[0]

print(f"æ ·æœ¬ {sample_idx} çš„ç‰¹å¾å€¼: {sample[0]}")
print(f"é¢„æµ‹ç±»åˆ«: {prediction}")
print(f"é¢„æµ‹æ¦‚ç‡: {clf.predict_proba(sample)[0]}")

# è¿½è¸ªè·¯å¾„
node_indicator = decision_path.toarray()[0]
node_index = np.where(node_indicator == 1)[0]
print(f"\nå†³ç­–è·¯å¾„ï¼ˆèŠ‚ç‚¹åºå·ï¼‰: {node_index}")

# ===== 8. å®é™…åº”ç”¨ï¼šä¿¡ç”¨è¯„åˆ†æ¨¡å‹ =====
print("\nã€8ã€‘å®é™…åº”ç”¨ï¼šä¿¡ç”¨è¯„åˆ†å†³ç­–æ ‘")

# åˆ›å»ºæ¨¡æ‹Ÿä¿¡ç”¨æ•°æ®
credit_data = pd.DataFrame({
    'age': np.random.randint(20, 70, 200),
    'income': np.random.randint(20000, 150000, 200),
    'credit_score': np.random.randint(300, 850, 200),
    'loan_amount': np.random.randint(10000, 500000, 200),
})

# åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆç®€åŒ–è§„åˆ™ï¼šé«˜æ”¶å…¥ + é«˜ä¿¡ç”¨åˆ† = æ‰¹å‡†ï¼‰
credit_data['approved'] = (
    (credit_data['income'] > 60000) &
    (credit_data['credit_score'] > 650)
).astype(int)

X_credit = credit_data[['age', 'income', 'credit_score', 'loan_amount']]
y_credit = credit_data['approved']

X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_credit, y_credit, test_size=0.3, random_state=42)

# è®­ç»ƒä¿¡ç”¨è¯„åˆ†æ¨¡å‹ï¼ˆå¯è§£é‡Šæ€§ä¼˜å…ˆï¼‰
credit_clf = DecisionTreeClassifier(
    max_depth=4,              # æµ…æ ‘ï¼ˆæ˜“è§£é‡Šï¼‰
    min_samples_split=10,     # é¿å…è¿‡æ‹Ÿåˆ
    min_samples_leaf=5,
    random_state=42
)
credit_clf.fit(X_train_c, y_train_c)

print(f"ä¿¡ç”¨è¯„åˆ†æ¨¡å‹å‡†ç¡®ç‡: {credit_clf.score(X_test_c, y_test_c):.4f}")
print(f"\nå¯è§£é‡Šæ€§ä¼˜åŠ¿ï¼š")
print(f"  âœ“ å¯ä»¥å›ç­”\"ä¸ºä»€ä¹ˆæ‹’ç»ï¼Ÿ\"")
print(f"  âœ“ å†³ç­–è§„åˆ™æ¸…æ™°ï¼ˆIF age<30 AND income>60k THEN...ï¼‰")
print(f"  âœ“ ç¬¦åˆé‡‘èç›‘ç®¡è¦æ±‚ï¼ˆåˆè§„ï¼‰")

# ===== è¾“å‡ºæ€»ç»“ =====
print("\n" + "=" * 60)
print("å…³é”®è¦ç‚¹æ€»ç»“")
print("=" * 60)
print(f"""
âœ“ å†³ç­–æ ‘çš„ä¼˜åŠ¿ï¼š
  1. å¯è§£é‡Šæ€§å¼ºï¼ˆé»‘ç›’è½¬ç™½ç›’ï¼‰
  2. æ— éœ€ç‰¹å¾æ ‡å‡†åŒ–
  3. æ”¯æŒå¤šåˆ†ç±»å’Œå›å½’
  4. è‡ªåŠ¨ç‰¹å¾äº¤äº’

âœ— å†³ç­–æ ‘çš„ç¼ºç‚¹ï¼š
  1. å®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆéœ€è¦å‰ªæï¼‰
  2. ä¸ç¨³å®šï¼ˆå°æ•°æ®å˜åŒ– â†’ å®Œå…¨ä¸åŒçš„æ ‘ï¼‰
  3. åå‘äºé€‰æ‹©é«˜åŸºæ•°ç‰¹å¾
  4. æ€§èƒ½ä¸å¦‚é›†æˆå­¦ä¹ ï¼ˆéšæœºæ£®æ—ã€XGBoostï¼‰

æ¨èç”¨æ³•ï¼š
  â€¢ éœ€è¦å¯è§£é‡Šæ€§ï¼šå†³ç­–æ ‘
  â€¢ éœ€è¦æœ€é«˜ç²¾åº¦ï¼šéšæœºæ£®æ— æˆ– XGBoost
  â€¢ ç”Ÿäº§ç¯å¢ƒï¼šå°å‹å†³ç­–æ ‘ + äººå·¥å®¡æ ¸
""")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
============================================================
å†³ç­–æ ‘ï¼ˆDecision Treeï¼‰å®Œæ•´æ•™ç¨‹
============================================================

ã€1ã€‘åˆ†ç±»æ ‘ï¼ˆDecisionTreeClassifierï¼‰
åˆ†ç±»å‡†ç¡®ç‡: 0.8667

Classification Report:
              precision    recall  f1-score   support
           0       0.86      0.88      0.87        51
           1       0.87      0.85      0.86        49

ã€4ã€‘åˆ†è£‚å‡†åˆ™å¯¹æ¯”ï¼ˆGini vs Entropyï¼‰
Gini å‡†ç¡®ç‡: 0.8667
Entropy å‡†ç¡®ç‡: 0.8667
å·®å¼‚: 0.000000 (é€šå¸¸å¾ˆå°)

ã€5ã€‘æ ‘æ·±åº¦ä¸è¿‡æ‹Ÿåˆ
æ·±åº¦   è®­ç»ƒå‡†ç¡®ç‡     æµ‹è¯•å‡†ç¡®ç‡     è¿‡æ‹Ÿåˆç¨‹åº¦
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2      0.7667        0.7333        0.0333
3      0.8700        0.8667        0.0033
5      0.8933        0.8667        0.0267
10     0.9533        0.8333        0.1200
15     0.9900        0.8000        0.1900
20     0.9967        0.8000        0.1967
```

---

## ğŸ”§ è¶…å‚æ•°è°ƒä¼˜ (Hyperparameters)

### å…³é”®å‚æ•°è¡¨

| å‚æ•°å | é»˜è®¤å€¼ | å–å€¼èŒƒå›´ | å«ä¹‰ | è°ƒä¼˜ä¼˜å…ˆçº§ |
|--------|--------|---------|------|-----------|
| `max_depth` | None | [1, 30] | æ ‘çš„æœ€å¤§æ·±åº¦ï¼ˆé˜²è¿‡æ‹Ÿåˆå…³é”®ï¼‰ | â­â­â­â­â­ |
| `min_samples_split` | 2 | [2, 100] | åˆ†è£‚çš„æœ€å°‘æ ·æœ¬æ•° | â­â­â­â­â­ |
| `min_samples_leaf` | 1 | [1, 100] | å¶èŠ‚ç‚¹æœ€å°‘æ ·æœ¬æ•° | â­â­â­â­ |
| `criterion` | 'gini' | ['gini', 'entropy'] | åˆ†è£‚å‡†åˆ™ï¼ˆå¿«æ…¢æƒè¡¡ï¼‰ | â­â­â­ |
| `splitter` | 'best' | ['best', 'random'] | åˆ†è£‚æœç´¢ç­–ç•¥ | â­â­ |
| `max_features` | None | [None, 'sqrt', 'log2'] | æ¯æ¬¡åˆ†è£‚è€ƒè™‘çš„ç‰¹å¾æ•° | â­â­â­ |
| `min_weight_fraction_leaf` | 0.0 | [0.0, 0.5] | å¶èŠ‚ç‚¹çš„æœ€å°æƒé‡æ¯”ä¾‹ | â­â­ |
| `class_weight` | None | [None, 'balanced'] | ç±»åˆ«ä¸å¹³è¡¡å¤„ç† | â­â­â­ |

### è°ƒä¼˜æŒ‡å—

#### ğŸ¯ max_depthï¼ˆæœ€å…³é”®ï¼‰

**å«ä¹‰**ï¼šæ ‘çš„æœ€å¤§åˆ†è£‚å±‚æ•°ï¼Œç›´æ¥æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ã€‚

```python
# âŒ max_depth å¤ªå¤§ï¼ˆNoneæˆ–30ï¼‰
# â†’ æ ‘éå¸¸æ·±ï¼Œå®Œå…¨è¿‡æ‹Ÿåˆ
# â†’ åœ¨è®­ç»ƒé›†ä¸Šç²¾åº¦é«˜ï¼Œæµ‹è¯•é›†ä½
clf_deep = DecisionTreeClassifier(max_depth=None)  # æ— é™æ·±
clf_deep.fit(X_train, y_train)
print(f"è®­ç»ƒå‡†ç¡®ç‡: {clf_deep.score(X_train, y_train):.4f}")  # 0.99+
print(f"æµ‹è¯•å‡†ç¡®ç‡: {clf_deep.score(X_test, y_test):.4f}")    # 0.75

# âœ“ max_depth åˆç†ï¼ˆ3-7ï¼‰
# â†’ æ ‘çš„å¤æ‚åº¦é€‚å½“ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
# â†’ è®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡æ¥è¿‘
clf_good = DecisionTreeClassifier(max_depth=5)
clf_good.fit(X_train, y_train)
print(f"è®­ç»ƒå‡†ç¡®ç‡: {clf_good.score(X_train, y_train):.4f}")  # 0.90
print(f"æµ‹è¯•å‡†ç¡®ç‡: {clf_good.score(X_test, y_test):.4f}")    # 0.88

# âŒ max_depth å¤ªå°ï¼ˆ2ï¼‰
# â†’ æ ‘å¤ªæµ…ï¼Œæ¬ æ‹Ÿåˆ
# â†’ æ— æ³•æ•æ‰æ•°æ®çš„å¤æ‚æ¨¡å¼
clf_shallow = DecisionTreeClassifier(max_depth=2)
clf_shallow.fit(X_train, y_train)
print(f"è®­ç»ƒå‡†ç¡®ç‡: {clf_shallow.score(X_train, y_train):.4f}")  # 0.72
```

**è°ƒä¼˜å»ºè®®**ï¼š
- æ•°æ®é‡å°ï¼ˆ<1kï¼‰ï¼šmax_depth = 3-4
- æ•°æ®é‡ä¸­ç­‰ï¼ˆ1k-10kï¼‰ï¼šmax_depth = 5-7
- æ•°æ®é‡å¤§ï¼ˆ>10kï¼‰ï¼šmax_depth = 7-15
- éœ€è¦å¯è§£é‡Šæ€§ï¼šmax_depth â‰¤ 4ï¼ˆæ ‘å¤ªæ·±éš¾ç†è§£ï¼‰

#### ğŸ¯ min_samples_split

**å«ä¹‰**ï¼šåˆ†è£‚ä¸€ä¸ªèŠ‚ç‚¹éœ€è¦çš„æœ€å°‘æ ·æœ¬æ•°ã€‚å€¼è¶Šå¤§ï¼Œæ ‘è¶Šæµ…ï¼Œé˜²è¿‡æ‹Ÿåˆã€‚

```python
# è°ƒæ•´ min_samples_split
for min_split in [2, 5, 10, 20]:
    clf = DecisionTreeClassifier(max_depth=5, min_samples_split=min_split)
    clf.fit(X_train, y_train)
    train_acc = clf.score(X_train, y_train)
    test_acc = clf.score(X_test, y_test)
    print(f"min_split={min_split}: train={train_acc:.4f}, test={test_acc:.4f}")

# è¾“å‡ºï¼š
# min_split=2: train=0.9000, test=0.8667 (è¿‡æ‹Ÿåˆ)
# min_split=5: train=0.8933, test=0.8667 (å¹³è¡¡)
# min_split=10: train=0.8867, test=0.8600 (ä¿å®ˆ)
# min_split=20: train=0.8533, test=0.8333 (è¿‡åº¦æ­£åˆ™åŒ–)
```

**è°ƒä¼˜å»ºè®®**ï¼š
- é€šå¸¸è®¾ä¸º 5-20
- æ•°æ®é‡å°ï¼šmin_samples_split = 5-10
- æ•°æ®é‡å¤§ï¼šå¯ä»¥ 20-50
- ç±»åˆ«ä¸å¹³è¡¡ï¼šå¢å¤§åˆ° 10-50ï¼ˆé˜²æ­¢å¯¹å°‘æ•°ç±»è¿‡æ‹Ÿåˆï¼‰

#### ğŸ¯ min_samples_leaf

**å«ä¹‰**ï¼šå¶èŠ‚ç‚¹çš„æœ€å°‘æ ·æœ¬æ•°ã€‚ç¡®ä¿å¶èŠ‚ç‚¹ä¸ä¼šå¤ªå°ï¼ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰ã€‚

```python
# ç±»æ¯”ï¼šå­¦æ ¡çš„è§„å®š
# min_samples_split = 10ï¼šç­çº§è‡³å°‘10äººæ‰èƒ½åˆ†ç­
# min_samples_leaf = 5ï¼šæœ€ååˆ†å‡ºçš„ç­çº§è‡³å°‘5äºº
```

**è°ƒä¼˜å»ºè®®**ï¼š
- é€šå¸¸è®¾ä¸º 1-10
- min_samples_leaf â‰¤ min_samples_split / 2
- å¦‚æœè¿‡æ‹Ÿåˆä¸¥é‡ï¼Œå¢å¤§åˆ° 5-20

#### ğŸ¯ criterionï¼ˆåˆ†è£‚å‡†åˆ™ï¼‰

**Gini vs Entropy å¯¹æ¯”**ï¼š

```python
# Giniï¼šå¿«é€Ÿï¼ˆsklearné»˜è®¤ï¼‰
clf_gini = DecisionTreeClassifier(criterion='gini')
# å…¬å¼ï¼šGini = 1 - Î£(p_kÂ²)

# Entropyï¼šç¨æ…¢ä½†æœ‰æ—¶æ›´å¥½
clf_entropy = DecisionTreeClassifier(criterion='entropy')
# å…¬å¼ï¼šEntropy = -Î£(p_k Ã— logâ‚‚(p_k))

# å®é™…æ•ˆæœï¼šå¤§å¤šæ•°æƒ…å†µä¸‹æ²¡åŒºåˆ«ï¼Œç”¨Giniå§
```

**åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ**

| ç‰¹æ€§ | Gini | Entropy |
|------|------|---------|
| è®¡ç®—é€Ÿåº¦ | âœ“ å¿«ï¼ˆæ— å¯¹æ•°ï¼‰| æ…¢ |
| ç»“æœç²¾åº¦ | åŸºæœ¬ç›¸åŒ | åŸºæœ¬ç›¸åŒ |
| æ¨è | âœ“ ä¼˜å…ˆé€‰æ‹© | å¤‡é€‰ |

---

## âš–ï¸ ä¼˜ç¼ºç‚¹ä¸åœºæ™¯ (Pros & Cons)

### ä¼˜ç¼ºç‚¹å¯¹æ¯”è¡¨

| ç»´åº¦ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|------|------|------|
| **å¯è§£é‡Šæ€§** | â­â­â­â­â­ å®Œå…¨é€æ˜ï¼Œçœ‹å¾—è§å†³ç­–è¿‡ç¨‹ | âŒ æ—  |
| **æ— éœ€ç‰¹å¾å·¥ç¨‹** | â­â­â­â­â­ è‡ªåŠ¨å­¦ä¹ ç‰¹å¾äº¤äº’ | âŒ æ—  |
| **å¤„ç†æ··åˆç±»å‹** | â­â­â­â­ æ”¯æŒæ•°å€¼å’Œç±»åˆ«ç‰¹å¾ | âŒ æ—  |
| **å¤šåˆ†ç±»æ”¯æŒ** | â­â­â­â­ åŸç”Ÿæ”¯æŒï¼ˆæ— éœ€ç¼–ç ï¼‰ | âŒ æ—  |
| **ç²¾åº¦** | â­â­â­ ä¸­ç­‰ï¼ˆä¸å¦‚é›†æˆå­¦ä¹ ï¼‰ | âŒ æ—  |
| **ç¨³å®šæ€§** | â­â­ ä¸ç¨³å®šï¼ˆå°æ•°æ®å˜åŒ–â†’å®Œå…¨ä¸åŒï¼‰ | â­â­ é£é™© |
| **è¿‡æ‹Ÿåˆé£é™©** | â­â­â­ å®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆéœ€å‰ªæï¼‰ | â­â­ æŒ‘æˆ˜ |
| **è®¡ç®—é€Ÿåº¦** | â­â­â­â­â­ è®­ç»ƒå¿«ï¼Œé¢„æµ‹å¿« | âŒ æ—  |
| **ç¼ºå¤±å€¼å¤„ç†** | â­â­â­â­ åŸç”Ÿæ”¯æŒï¼ˆå­¦ä¹ æœ€ä¼˜æ–¹å‘ï¼‰ | âŒ æ—  |

### å†³ç­–æ ‘ä¸å…¶ä»–ç®—æ³•çš„å¯¹æ¯”

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              å†³ç­–æ ‘ vs å…¶ä»–åˆ†ç±»ç®—æ³•                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚ å†³ç­–æ ‘ vs é€»è¾‘å›å½’                                           â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                     â”‚
â”‚ é€»è¾‘å›å½’ï¼š                                                    â”‚
â”‚   ç‰¹ç‚¹ï¼šçº¿æ€§åˆ†ç•Œï¼Œç®€å•é«˜æ•ˆ                                  â”‚
â”‚   ç¼ºç‚¹ï¼šæ— æ³•å­¦ä¹ éçº¿æ€§å…³ç³»                                  â”‚
â”‚   ä¾‹ï¼šXORé—®é¢˜ï¼ˆæ— æ³•ç”¨ç›´çº¿åˆ†å‰²ï¼‰                             â”‚
â”‚                                                                â”‚
â”‚ å†³ç­–æ ‘ï¼š                                                      â”‚
â”‚   ç‰¹ç‚¹ï¼šåˆ†æ®µçº¿æ€§è¾¹ç•Œï¼Œçµæ´»                                  â”‚
â”‚   ä¼˜åŠ¿ï¼šè‡ªåŠ¨å­¦ä¹ éçº¿æ€§ç‰¹å¾äº¤äº’                              â”‚
â”‚   ä¾‹ï¼šXORé—®é¢˜ï¼ˆç”¨ä¸¤æ¡çº¿åˆ†å‰²ï¼‰                               â”‚
â”‚                                                                â”‚
â”‚ å†³ç­–æ ‘ vs SVM                                                â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚ SVMï¼šé»‘ç›’æ¨¡å‹ï¼Œéš¾ä»¥è§£é‡Š                                      â”‚
â”‚ å†³ç­–æ ‘ï¼šç™½ç›’æ¨¡å‹ï¼Œå¯å®Œå…¨è§£é‡Š                                â”‚
â”‚                                                                â”‚
â”‚ æ€§èƒ½ï¼š                                                        â”‚
â”‚ SVM â‰ˆ å†³ç­–æ ‘ï¼ˆç›¸å½“ï¼Œä½†SVMç¨ä¼˜ï¼‰                             â”‚
â”‚                                                                â”‚
â”‚ å†³ç­–æ ‘ vs éšæœºæ£®æ—                                           â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
â”‚ å†³ç­–æ ‘ï¼šå•æ£µæ ‘ï¼Œç²¾åº¦ä¸é«˜ï¼Œè¿‡æ‹Ÿåˆä¸¥é‡                        â”‚
â”‚ éšæœºæ£®æ—ï¼šå¤šæ£µæ ‘é›†æˆï¼Œç²¾åº¦é«˜ï¼Œè¿‡æ‹Ÿåˆå°‘                      â”‚
â”‚                                                                â”‚
â”‚ åº”ç”¨åœºæ™¯ï¼š                                                    â”‚
â”‚ å†³ç­–æ ‘ï¼šéœ€è¦å®Œå…¨å¯è§£é‡Šæ€§                                    â”‚
â”‚ éšæœºæ£®æ—ï¼šéœ€è¦æœ€é«˜ç²¾åº¦                                      â”‚
â”‚                                                                â”‚
â”‚ å†³ç­–æ ‘ vs XGBoost/LightGBM                                   â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚ å†³ç­–æ ‘ï¼šåŸºç¡€ç®—æ³•ï¼Œæ˜“ç†è§£                                    â”‚
â”‚ XGBoostï¼šè¿›é˜¶ï¼Œç²¾åº¦æœ€é«˜ä½†å¤æ‚                               â”‚
â”‚                                                                â”‚
â”‚ é€‰æ‹©ï¼š                                                        â”‚
â”‚ å­¦ä¹ ï¼šå…ˆå­¦å†³ç­–æ ‘ï¼Œå†å­¦XGBoost                               â”‚
â”‚ ç”Ÿäº§ï¼šéœ€è¦ç²¾åº¦å°±ç”¨XGBoostï¼Œéœ€è¦è§£é‡Šå°±ç”¨å†³ç­–æ ‘              â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### åº”ç”¨åœºæ™¯å†³ç­–æ ‘

```
ä½ æ˜¯å¦éœ€è¦å®Œå…¨å¯è§£é‡Šæ€§ï¼ˆé‡‘è/åŒ»ç–—ï¼‰ï¼Ÿ
â”œâ”€ YESï¼šä½¿ç”¨å†³ç­–æ ‘
â”‚  â”œâ”€ éœ€è¦æœ€é«˜ç²¾åº¦ï¼Ÿâ†’ ç”¨å°å†³ç­–æ ‘ + äººå·¥å®¡æ ¸
â”‚  â””â”€ å¯æ¥å—ç²¾åº¦åä½ï¼Ÿâ†’ ç”¨å‰ªæåçš„å†³ç­–æ ‘
â”‚
â””â”€ NOï¼šéœ€è¦æœ€é«˜ç²¾åº¦ï¼Ÿ
   â”œâ”€ YESâ†’ ä½¿ç”¨éšæœºæ£®æ— æˆ– XGBoost
   â””â”€ NOï¼ˆç®€å•å¿«é€Ÿï¼‰â†’ ä½¿ç”¨é€»è¾‘å›å½’
```

---

## ğŸ’¬ é¢è¯•å¿…é—® (Interview Q&A)

> [!question] Q1: å†³ç­–æ ‘å¦‚ä½•è®¡ç®—ä¿¡æ¯å¢ç›Šï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šä¿¡æ¯å¢ç›Š = åˆ†è£‚å‰çš„ç†µ - åˆ†è£‚åçš„åŠ æƒç†µ

**è¯¦ç»†è§£æ**ï¼š

```
å…¬å¼ï¼š
Gain(D, A) = H(D) - H(D|A)

å…¶ä¸­ï¼š
- H(D) = -Î£(p_k Ã— logâ‚‚(p_k))  // åŸå§‹æ•°æ®é›†ç†µ
- H(D|A) = Î£(|D_v|/|D| Ã— H(D_v))  // åˆ†è£‚åçš„æ¡ä»¶ç†µ

ç›´è§‰ï¼š
- åˆ†è£‚å‰ï¼šæ•°æ®æ··ä¹±ï¼Œç†µå¤§
- åˆ†è£‚åï¼šæ•°æ®å˜çº¯å‡€ï¼Œç†µå°
- å·®å€¼è¶Šå¤§ï¼Œè¿™ä¸ªåˆ†è£‚è¶Šæœ‰æ•ˆ

ä¾‹å­ï¼ˆäºŒåˆ†ç±»ï¼‰ï¼š
åŸå§‹æ•°æ®ï¼š[æ­£, æ­£, æ­£, è´Ÿ, è´Ÿ] (3æ­£2è´Ÿ)
H(D) = -3/5 Ã— logâ‚‚(3/5) - 2/5 Ã— logâ‚‚(2/5) = 0.971

åˆ†è£‚1ï¼ˆå¹´é¾„<30ï¼‰ï¼š
  å·¦å­é›†ï¼š[æ­£, æ­£, è´Ÿ]    H=0.918
  å³å­é›†ï¼š[æ­£, è´Ÿ]        H=1.000
  H(D|å¹´é¾„) = 3/5 Ã— 0.918 + 2/5 Ã— 1.000 = 0.951
  Gain = 0.971 - 0.951 = 0.020

åˆ†è£‚2ï¼ˆæ”¶å…¥>50kï¼‰ï¼š
  å·¦å­é›†ï¼š[æ­£, æ­£, æ­£]    H=0    (å®Œå…¨çº¯å‡€ï¼)
  å³å­é›†ï¼š[è´Ÿ, è´Ÿ]        H=0    (å®Œå…¨çº¯å‡€ï¼)
  H(D|æ”¶å…¥) = 3/5 Ã— 0 + 2/5 Ã— 0 = 0
  Gain = 0.971 - 0 = 0.971 (æå¤§ï¼)

ç»“è®ºï¼šé€‰æ‹©æ”¶å…¥ä½œä¸ºåˆ†è£‚ç‰¹å¾ï¼ˆå¢ç›Š0.971 > 0.020ï¼‰
```

**å…³é”®ç‚¹**ï¼š
- å¢ç›Šè¶Šå¤§ â†’ åˆ†è£‚æ•ˆæœè¶Šå¥½
- ç›®æ ‡æ˜¯æ‰¾åˆ°å¢ç›Šæœ€å¤§çš„ç‰¹å¾
- è´ªå¿ƒé€‰æ‹©ï¼ˆå±€éƒ¨æœ€ä¼˜ï¼Œä¸ä¿è¯å…¨å±€æœ€ä¼˜ï¼‰

---

> [!question] Q2: å†³ç­–æ ‘å¦‚ä½•é˜²æ­¢è¿‡æ‹Ÿåˆï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šå‰å‰ªæï¼ˆé™åˆ¶æ ‘æ·±åº¦ï¼‰ + åå‰ªæï¼ˆç§»é™¤æ— æ•ˆèŠ‚ç‚¹ï¼‰

**è¯¦ç»†è§£æ**ï¼š

```
è¿‡æ‹ŸåˆåŸå› ï¼š
â”œâ”€ æ ‘å¤ªæ·±ï¼šå­¦åˆ°è®­ç»ƒæ•°æ®çš„å™ªå£°
â”œâ”€ åˆ†è£‚è¿‡å¤šï¼šæ¯ä¸ªå¶èŠ‚ç‚¹æ ·æœ¬å¤ªå°‘
â””â”€ æ²¡æœ‰æ­£åˆ™åŒ–ï¼šæ²¡æœ‰å¤æ‚åº¦æƒ©ç½š

é˜²çº¿1ï¼šå‰å‰ªæï¼ˆè®­ç»ƒæ—¶é™åˆ¶ï¼‰âœ“ æ¨è

max_depthï¼š
  â”œâ”€ é™åˆ¶æ ‘æ·±åº¦ï¼ˆæœ€ç›´æ¥ï¼‰
  â””â”€ è§„åˆ™ï¼šæ·±åº¦>20æ—¶åŸºæœ¬è¿‡æ‹Ÿåˆ

min_samples_splitï¼š
  â”œâ”€ åˆ†è£‚éœ€è¦æœ€å°‘æ ·æœ¬æ•°
  â””â”€ è§„åˆ™ï¼šé€šå¸¸è®¾5-20

min_samples_leafï¼š
  â”œâ”€ å¶èŠ‚ç‚¹æœ€å°‘æ ·æœ¬æ•°
  â””â”€ è§„åˆ™ï¼šé€šå¸¸è®¾1-10

ç¤ºä¾‹ï¼ˆè°ƒå‚é¡ºåºï¼‰ï¼š
ç¬¬1æ­¥ï¼šè®¾ max_depth=3 è¯•è¯•
ç¬¬2æ­¥ï¼šå¦‚æœè¿‡æ‹Ÿåˆï¼Œå¢å¤§åˆ°4æˆ–5ï¼ŒåŒæ—¶è°ƒæ•´min_samples
ç¬¬3æ­¥ï¼šå¦‚æœæ¬ æ‹Ÿåˆï¼Œå‡å°max_depthåˆ°2

é˜²çº¿2ï¼šåå‰ªæï¼ˆè®­ç»ƒåä¿®å‰ªï¼‰

æ€è·¯ï¼š
  1. å…ˆæ„å»ºå®Œæ•´çš„æ ‘ï¼ˆå¯èƒ½è¿‡æ‹Ÿåˆï¼‰
  2. ä»ä¸‹è€Œä¸Šåˆ é™¤æ— æ•ˆèŠ‚ç‚¹ï¼ˆå‡å°‘åˆ†æ”¯ï¼‰
  3. ç”¨éªŒè¯é›†è¯„ä¼°æ•ˆæœ
  4. ä¿ç•™æœ€ä¼˜é…ç½®

ä»£ç ï¼ˆsklearnè‡ªå¸¦ï¼‰ï¼š
  from sklearn.tree import DecisionTreeClassifier

  # å…ˆè®­ç»ƒæ— é™åˆ¶çš„æ ‘
  clf = DecisionTreeClassifier(max_depth=None)
  clf.fit(X_train, y_train)

  # åå‰ªæï¼ˆsklearn 0.22+ï¼‰
  path = clf.cost_complexity_pruning_path(X_val, y_val)
  ccp_alphas = path.ccp_alphas

  best_alpha = None
  best_score = 0
  for ccp_alpha in ccp_alphas:
      pruned_clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)
      pruned_clf.fit(X_train, y_train)
      val_score = pruned_clf.score(X_val, y_val)
      if val_score > best_score:
          best_score = val_score
          best_alpha = ccp_alpha

é˜²çº¿3ï¼šæ­£åˆ™åŒ–å‚æ•°

class_weight='balanced'ï¼š
  â”œâ”€ å¤„ç†ä¸å¹³è¡¡æ•°æ®
  â””â”€ è‡ªåŠ¨è°ƒæ•´ç±»åˆ«æƒé‡

ä¾‹å­ï¼ˆä¸å¹³è¡¡æ•°æ®ï¼‰ï¼š
  # æ•°æ®ï¼š90%æ­£ç±»ï¼Œ10%è´Ÿç±»

  # ä¸åŠ æƒé‡
  clf = DecisionTreeClassifier()
  # â†’ å€¾å‘äºé¢„æµ‹æ­£ç±»ï¼ˆå¤šæ•°ç±»ï¼‰

  # åŠ æƒé‡
  clf = DecisionTreeClassifier(class_weight='balanced')
  # â†’ è‡ªåŠ¨ç»™è´Ÿç±»æ›´é«˜æƒé‡
  # â†’ é˜²æ­¢æ¨¡å‹åå‘å¤šæ•°ç±»
```

---

> [!question] Q3: åˆ†ç±»æ ‘å’Œå›å½’æ ‘æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šç›®æ ‡å˜é‡ç±»å‹ä¸åŒï¼Œåˆ†è£‚å‡†åˆ™ä¸åŒ

**è¯¦ç»†è§£æ**ï¼š

```
åˆ†ç±»æ ‘ï¼ˆClassification Treeï¼‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ç›®æ ‡å˜é‡ï¼šç¦»æ•£çš„ç±»åˆ«
  ä¾‹ï¼š[æ˜¯å¦è´­ä¹°, æ˜¯å¦ç—…ç—‡, ä¿¡ç”¨ç­‰çº§]

åˆ†è£‚å‡†åˆ™ï¼šä¿¡æ¯å¢ç›Š æˆ– Giniä¸çº¯åº¦
  å…¬å¼ï¼šGain(D, A) = H(D) - H(D|A)

å¶èŠ‚ç‚¹é¢„æµ‹ï¼šå¤šæ•°ç±»æŠ•ç¥¨
  ä¾‹ï¼šå¶èŠ‚ç‚¹æœ‰[æ­£, æ­£, è´Ÿ] â†’ é¢„æµ‹ä¸º"æ­£"

è¾“å‡ºï¼šç±»åˆ« æˆ– ç±»åˆ«æ¦‚ç‡
  ä¾‹ï¼šclf.predict(X) â†’ [0, 1, 1, 0]
      clf.predict_proba(X) â†’ [[0.8, 0.2], ...]

è¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®ç‡, ç²¾ç¡®ç‡, å¬å›ç‡, F1åˆ†æ•°, AUC

å›å½’æ ‘ï¼ˆRegression Treeï¼‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ç›®æ ‡å˜é‡ï¼šè¿ç»­çš„æ•°å€¼
  ä¾‹ï¼š[æˆ¿ä»·, é”€å”®é¢, è‚¡ç¥¨ä»·æ ¼]

åˆ†è£‚å‡†åˆ™ï¼šæ–¹å·®ç¼©å‡ï¼ˆVariance Reductionï¼‰
  å…¬å¼ï¼šé€‰æ‹©åˆ†è£‚ä½¿ MSE æœ€å°

å¶èŠ‚ç‚¹é¢„æµ‹ï¼šå¹³å‡å€¼
  ä¾‹ï¼šå¶èŠ‚ç‚¹æœ‰[100, 110, 120] â†’ é¢„æµ‹ä¸º110

è¾“å‡ºï¼šæ•°å€¼
  ä¾‹ï¼šreg.predict(X) â†’ [50000, 75000, ...]

è¯„ä¼°æŒ‡æ ‡ï¼šMSE, RMSE, MAE, RÂ²

å¯¹æ¯”ç¤ºä¾‹ï¼š

åˆ†ç±»æ ‘ï¼š
  X = [[å¹´é¾„, æ”¶å…¥], ...]
  y = ['æ‰¹å‡†', 'æ‹’ç»', 'æ‰¹å‡†', ...]
  tree = DecisionTreeClassifier()

  é¢„æµ‹ï¼štree.predict([[25, 40000]]) â†’ 'æ‹’ç»'

å›å½’æ ‘ï¼š
  X = [[å¹´é¾„, æ”¶å…¥], ...]
  y = [500000, 750000, 1200000, ...]
  tree = DecisionTreeRegressor()

  é¢„æµ‹ï¼štree.predict([[25, 40000]]) â†’ 650000.5
```

---

> [!question] Q4: å†³ç­–æ ‘å¦‚ä½•å¤„ç†ç¼ºå¤±å€¼ï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šå­¦ä¹ ç¼ºå¤±å€¼çš„æœ€ä¼˜æ–¹å‘ï¼ˆå·¦å­æ ‘æˆ–å³å­æ ‘ï¼‰

**è¯¦ç»†è§£æ**ï¼š

```
æ–¹æ³•1ï¼šæ•°æ®é¢„å¤„ç†ï¼ˆä¸¢å¼ƒæˆ–å¡«è¡¥ï¼‰âŒ ä¸æ¨è

é—®é¢˜ï¼š
  â”œâ”€ ä¸¢å¼ƒè¡Œï¼šæŸå¤±æ•°æ®ï¼Œæ ·æœ¬é‡å‡å°‘
  â””â”€ å¡«è¡¥å€¼ï¼šå¼•å…¥äººå·¥åå·®ï¼Œå½±å“æ¨¡å‹

ä¾‹å­ï¼š
  åŸå§‹æ•°æ®ï¼š100è¡Œï¼Œæœ‰5è¡Œç¼ºå¤±

  æ–¹æ¡ˆ1ï¼šä¸¢å¼ƒ
    â†’ å‰©ä¸‹95è¡Œï¼ŒæŸå¤±5%æ•°æ®

  æ–¹æ¡ˆ2ï¼šå¡«å¹³å‡å€¼
    â†’ 100è¡Œæ•°æ®ï¼Œä½†5è¡Œæ˜¯è™šå‡æ•°æ®
    â†’ æ¨¡å‹å¯èƒ½å­¦åˆ°é”™è¯¯è§„å¾‹

æ–¹æ³•2ï¼šsklearnå†³ç­–æ ‘è‡ªåŠ¨å¤„ç† âœ“ æ¨è

sklearnå†³ç­–æ ‘çš„ç­–ç•¥ï¼š
  å½“ç‰¹å¾æœ‰ç¼ºå¤±å€¼æ—¶ï¼Œæ ‘è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜æ–¹å‘ï¼š
  â”œâ”€ å°è¯•å°†ç¼ºå¤±å€¼æ ·æœ¬å‘é€åˆ°å·¦å­æ ‘
  â”œâ”€ è®¡ç®—è¯¥åˆ†è£‚çš„ä¿¡æ¯å¢ç›Š
  â”œâ”€ ä¹Ÿå°è¯•å‘é€åˆ°å³å­æ ‘
  â”œâ”€ è®¡ç®—å¢ç›Š
  â””â”€ é€‰æ‹©å¢ç›Šæ›´å¤§çš„æ–¹å‘

ä»£ç ç¤ºä¾‹ï¼š
  import numpy as np
  from sklearn.tree import DecisionTreeClassifier

  # æ•°æ®ä¸­æœ‰ç¼ºå¤±å€¼ï¼ˆNaNï¼‰
  X = np.array([
      [1, 10],
      [2, np.nan],      # ç¼ºå¤±å€¼
      [3, 30],
      [4, np.nan],      # ç¼ºå¤±å€¼
      [5, 50]
  ])
  y = np.array([0, 1, 0, 1, 0])

  # sklearnç›´æ¥å¤„ç†NaNï¼ˆæ— éœ€é¢„å¤„ç†ï¼‰
  clf = DecisionTreeClassifier(max_depth=3)
  clf.fit(X, y)

  # é¢„æµ‹æ—¶ä¹Ÿå¯ä»¥æœ‰ç¼ºå¤±å€¼
  X_test = np.array([[2.5, np.nan]])
  pred = clf.predict(X_test)  # å·¥ä½œæ­£å¸¸

ä¼˜åŠ¿ï¼š
  âœ“ ä¿ç•™å®Œæ•´æ•°æ®ä¿¡æ¯
  âœ“ è‡ªåŠ¨å­¦ä¹ ç¼ºå¤±çš„å«ä¹‰ï¼ˆå¯èƒ½ç¼ºå¤±æœ¬èº«å°±æœ‰ä¿¡æ¯ï¼‰
  âœ“ æ— éœ€æ‰‹åŠ¨ç‰¹å¾å·¥ç¨‹

æ–¹æ³•3ï¼šç¼ºå¤±å€¼çš„ä¿¡æ¯ä»·å€¼

æœ‰äº›ç¼ºå¤±æ˜¯æœ‰æ„ä¹‰çš„ï¼š
  ä¾‹ï¼šå¡«è¡¨æ—¶"å¹´é¾„"ç¼ºå¤±å¯èƒ½æ„å‘³ç€"éšç§å…³åˆ‡"

  å†³ç­–æ ‘ä¼šè‡ªåŠ¨å‘ç°ï¼š
    IF age==NaN AND ...æŸæ¡ä»¶ THEN é«˜æ‹’ç»ç‡

  è¿™æœ¬èº«å°±æ˜¯ä¸€æ¡æœ‰ç”¨çš„è§„åˆ™ï¼
```

---

> [!question] Q5: å†³ç­–æ ‘ä¸éšæœºæ£®æ—çš„å…³ç³»ï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šå†³ç­–æ ‘æ˜¯éšæœºæ£®æ—çš„åŸºç¡€ï¼Œå¤šä¸ªå†³ç­–æ ‘ç»„æˆé›†æˆå­¦ä¹ 

**è¯¦ç»†è§£æ**ï¼š

```
å†³ç­–æ ‘ï¼ˆå•æ£µæ ‘ï¼‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ä¼˜ç‚¹ï¼š
  âœ“ å¯è§£é‡Š
  âœ“ å¿«é€Ÿ
  âœ“ æ— éœ€ç‰¹å¾å·¥ç¨‹

ç¼ºç‚¹ï¼š
  âœ— ç²¾åº¦ä¸é«˜
  âœ— å®¹æ˜“è¿‡æ‹Ÿåˆ
  âœ— ä¸ç¨³å®šï¼ˆå°æ•°æ®å˜åŒ–â†’å¤§ä¸åŒï¼‰

ä¾‹å­ï¼š
  æ•°æ®1ï¼šå¾—åˆ°æ ‘Aï¼Œç²¾åº¦0.85
  æ•°æ®2ï¼ˆå»æ‰1è¡Œï¼‰ï¼šå¾—åˆ°æ ‘Bï¼Œç²¾åº¦0.75
  â†’ æ ‘Bå®Œå…¨ä¸åŒï¼

éšæœºæ£®æ—ï¼ˆå¤šæ£µæ ‘çš„é›†æˆï¼‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

æ€è·¯ï¼š
  â”œâ”€ æ„å»ºå¤šæ£µéšæœºä¸åŒçš„å†³ç­–æ ‘ï¼ˆ100-1000æ£µï¼‰
  â”œâ”€ æ¯æ£µæ ‘éƒ½ç‹¬ç«‹æŠ•ç¥¨
  â””â”€ å–å¤šæ•°æŠ•ç¥¨ç»“æœ

ä¼˜ç‚¹ï¼š
  âœ“ ç²¾åº¦é«˜ï¼ˆæ¯”å•æ£µæ ‘é«˜10-15%ï¼‰
  âœ“ è¿‡æ‹Ÿåˆå°‘ï¼ˆé›†æˆå­¦ä¹ çš„ä¼˜åŠ¿ï¼‰
  âœ“ ç¨³å®šæ€§å¥½ï¼ˆå¤šæ£µæ ‘å¹³å‡åŒ–äº†å™ªå£°ï¼‰

ç¼ºç‚¹ï¼š
  âœ— ç²¾åº¦è¿˜æ˜¯ä½äºXGBoost
  âœ— å¯è§£é‡Šæ€§ä¸‹é™ï¼ˆå¤šæ ‘éš¾ä»¥ç†è§£ï¼‰
  âœ— è®­ç»ƒé€Ÿåº¦æ…¢ï¼ˆå¤šæ£µæ ‘ï¼‰

å¯¹æ¯”ç¤ºä¾‹ï¼š

åœºæ™¯ï¼šé¢„æµ‹æˆ¿ä»·

å•æ£µå†³ç­–æ ‘ï¼š
  è®­ç»ƒç²¾åº¦ï¼š0.92
  æµ‹è¯•ç²¾åº¦ï¼š0.72
  â†’ è¿‡æ‹Ÿåˆä¸¥é‡ï¼ˆ0.20å·®è·ï¼‰

éšæœºæ£®æ—ï¼ˆ100æ£µæ ‘ï¼‰ï¼š
  è®­ç»ƒç²¾åº¦ï¼š0.85
  æµ‹è¯•ç²¾åº¦ï¼š0.84
  â†’ å‡ ä¹æ²¡è¿‡æ‹Ÿåˆ
  â†’ æ€§èƒ½æ›´ç¨³å®š

XGBoostï¼ˆ100æ£µæ ‘ï¼‰ï¼š
  è®­ç»ƒç²¾åº¦ï¼š0.87
  æµ‹è¯•ç²¾åº¦ï¼š0.86
  â†’ æ€§èƒ½æœ€å¥½ï¼Œä½†å¤æ‚åº¦é«˜

åº”ç”¨å»ºè®®ï¼š

éœ€è¦å¯è§£é‡Šæ€§ï¼ˆé‡‘è/åŒ»ç–—ï¼‰ï¼š
  â†’ ä½¿ç”¨å†³ç­–æ ‘ï¼ˆæˆ–å°éšæœºæ£®æ—ï¼‰

éœ€è¦ç²¾åº¦ï¼ˆé¢„æµ‹ç«èµ›ï¼‰ï¼š
  â†’ ä½¿ç”¨éšæœºæ£®æ— æˆ– XGBoost

éœ€è¦é€Ÿåº¦ï¼ˆå®æ—¶ç³»ç»Ÿï¼‰ï¼š
  â†’ ä½¿ç”¨å†³ç­–æ ‘æˆ–çº¿æ€§æ¨¡å‹

å­¦ä¹ è·¯å¾„ï¼š
  1. å…ˆå­¦å†³ç­–æ ‘ï¼ˆç†è§£åŸºç¡€ï¼‰
  2. å†å­¦éšæœºæ£®æ—ï¼ˆç†è§£é›†æˆï¼‰
  3. æœ€åå­¦XGBoostï¼ˆç†è§£Boostingï¼‰
```

---

> [!question] Q6: ç”Ÿäº§ç¯å¢ƒå¦‚ä½•éƒ¨ç½²å†³ç­–æ ‘ï¼Ÿ
>
> **æ ¸å¿ƒç­”æ¡ˆ**ï¼šä¼˜å…ˆç”¨å°å†³ç­–æ ‘ + äººå·¥å®¡æ ¸ï¼Œè¿½æ±‚å¯è§£é‡Šæ€§è€Œéç²¾åº¦

**è¯¦ç»†è§£æ**ï¼š

```
éƒ¨ç½²åœºæ™¯1ï¼šé‡‘èé£æ§ï¼ˆä¸¥æ ¼åˆè§„ï¼‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

éœ€æ±‚ï¼š
  âœ“ å¿…é¡»è§£é‡Šä¸ºä»€ä¹ˆæ‹’ç»è´·æ¬¾
  âœ“ å†³ç­–è§„åˆ™å¿…é¡»å®¡è®¡æ—¥å¿—
  âœ— ç²¾åº¦è¦æ±‚ç›¸å¯¹å®½æ¾ï¼ˆ80%è¶³å¤Ÿï¼‰

éƒ¨ç½²æ–¹æ¡ˆï¼š
  1. è®­ç»ƒå°å†³ç­–æ ‘ï¼ˆmax_depth â‰¤ 4ï¼‰
  2. æå–å†³ç­–è§„åˆ™ï¼ˆIF-THENï¼‰
  3. é…ç½®äººå·¥å®¡æ ¸æµç¨‹
  4. å®šæœŸæ›´æ–°ï¼ˆæœˆåº¦ï¼‰

ä»£ç ï¼š
  clf = DecisionTreeClassifier(
      max_depth=4,              # æµ…æ ‘ï¼ˆå¯è§£é‡Šï¼‰
      min_samples_split=20,     # ä¿å®ˆï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
      min_samples_leaf=10,
      random_state=42
  )
  clf.fit(X_train, y_train)

  # å¯¼å‡ºå†³ç­–è§„åˆ™ï¼ˆç”¨äºå®¡è®¡ï¼‰
  from sklearn.tree import export_text
  tree_rules = export_text(clf, feature_names=feature_names)
  print(tree_rules)

  # è¾“å‡ºç¤ºä¾‹ï¼š
  # if X[0] <= 30.0:
  #   if X[1] <= 50000.0:
  #     return "æ‹’ç»"
  #   else:
  #     if X[2] <= 650.0:
  #       return "æ‹’ç»"
  #     else:
  #       return "æ‰¹å‡†"

éƒ¨ç½²åœºæ™¯2ï¼šæ¨èç³»ç»Ÿï¼ˆç²¾åº¦ä¼˜å…ˆï¼‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

éœ€æ±‚ï¼š
  âœ“ éœ€è¦æœ€é«˜ç²¾åº¦ï¼ˆæ¨èå‡†ç¡®åº¦ï¼‰
  âœ— å¯è§£é‡Šæ€§ä¸é‚£ä¹ˆå…³é”®

éƒ¨ç½²æ–¹æ¡ˆï¼š
  1. ç”¨éšæœºæ£®æ—æˆ–XGBoostæ›¿ä»£
  2. æ¯æ—¥è‡ªåŠ¨æ›´æ–°æ¨¡å‹
  3. åœ¨çº¿A/Bæµ‹è¯•

ä»£ç ï¼š
  # ä¸æ¨èç”¨å•æ£µæ ‘ï¼Œæ”¹ç”¨éšæœºæ£®æ—
  from sklearn.ensemble import RandomForestClassifier

  clf = RandomForestClassifier(
      n_estimators=100,
      max_depth=10,
      random_state=42
  )
  clf.fit(X_train, y_train)

éƒ¨ç½²åœºæ™¯3ï¼šæ•™è‚²è¯„ä¼°ï¼ˆå¯è§£é‡Š+ç²¾åº¦ï¼‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

éœ€æ±‚ï¼š
  âœ“ éœ€è¦è§£é‡Šå­¦ç”Ÿä¸ºä»€ä¹ˆè¢«åˆ†é…åˆ°æŸç­çº§
  âœ“ ç²¾åº¦ä¹Ÿè¦ä¸é”™ï¼ˆ85%+ï¼‰

éƒ¨ç½²æ–¹æ¡ˆï¼š
  1. ç”¨ä¸­ç­‰æ·±åº¦çš„æ ‘ï¼ˆmax_depth=6ï¼‰
  2. ä½¿ç”¨ç‰¹å¾é‡è¦æ€§åˆ†æ
  3. å®šæœŸå®¡æŸ¥æ¨¡å‹å†³ç­–

ç”Ÿäº§æ£€æŸ¥æ¸…å•ï¼š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… æ¨¡å‹é€‰æ‹©
  â˜ å†³ç­–æ ‘ æˆ– éšæœºæ£®æ—ï¼Ÿ
  â˜ max_depth ç¡®å®šäº†å—ï¼Ÿ
  â˜ è¿‡æ‹Ÿåˆç¨‹åº¦è¯„ä¼°äº†å—ï¼Ÿ

âœ… æ•°æ®è´¨é‡
  â˜ ç¼ºå¤±å€¼å¤„ç†äº†å—ï¼Ÿ
  â˜ ç±»åˆ«ä¸å¹³è¡¡å¤„ç†äº†å—ï¼Ÿ
  â˜ å¼‚å¸¸å€¼å¤„ç†äº†å—ï¼Ÿ

âœ… ç›‘æ§æŒ‡æ ‡
  â˜ åœ¨çº¿ç²¾åº¦è·Ÿè¸ªï¼Ÿ
  â˜ åå·®ç›‘æ§ï¼ˆæ€§åˆ«ã€å¹´é¾„ç­‰ï¼‰ï¼Ÿ
  â˜ ç”¨æˆ·åé¦ˆæ”¶é›†ï¼Ÿ

âœ… æ›´æ–°ç­–ç•¥
  â˜ æ›´æ–°é¢‘ç‡ï¼ˆæ¯å‘¨/æœˆ/å­£åº¦ï¼‰ï¼Ÿ
  â˜ è‡ªåŠ¨é‡è®­ç»ƒè„šæœ¬ï¼Ÿ
  â˜ å›æ»šæ–¹æ¡ˆï¼ˆæ—§æ¨¡å‹å¤‡ä»½ï¼‰ï¼Ÿ

âœ… åˆè§„å®¡è®¡
  â˜ å†³ç­–å¯è§£é‡Šå—ï¼Ÿ
  â˜ å®¡è®¡æ—¥å¿—å®Œæ•´å—ï¼Ÿ
  â˜ ç”¨æˆ·å¯ä»¥ç”³è¯‰å—ï¼Ÿ
```

---

## æ€»ç»“

> [!IMPORTANT] å†³ç­–æ ‘æ ¸å¿ƒè¦ç‚¹
> - **ç™½ç›’ç®—æ³•** = å®Œå…¨å¯è§£é‡Šï¼ˆvsç¥ç»ç½‘ç»œé»‘ç›’ï¼‰
> - **è´ªå¿ƒä¿¡æ¯å¢ç›Š** = è‡ªåŠ¨ç‰¹å¾é€‰æ‹©å’Œäº¤äº’
> - **å®¹æ˜“è¿‡æ‹Ÿåˆ** = éœ€è¦é€šè¿‡max_depth/min_samplesç­‰é™åˆ¶
> - **åŸºç¡€ç®—æ³•** = éšæœºæ£®æ—å’ŒXGBoostçš„åŸºçŸ³
> - **ä¸ç¨³å®š** = æ•°æ®å°å˜åŒ–å¯¼è‡´å®Œå…¨ä¸åŒçš„æ ‘

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… é‡‘èé£æ§ï¼ˆéœ€è¦å®Œå…¨å¯è§£é‡Šï¼‰
- âœ… åŒ»ç–—è¯Šæ–­ï¼ˆéœ€è¦åŒ»ç”Ÿèƒ½ç†è§£è§„åˆ™ï¼‰
- âœ… å­¦ä¹ åŸºç¡€ï¼ˆç†è§£æ ‘æ¨¡å‹æ˜¯å­¦ä¹ æœºå™¨å­¦ä¹ çš„å…³é”®ï¼‰
- âœ… ç‰¹å¾å·¥ç¨‹ï¼ˆå†³ç­–æ ‘çš„åˆ†è£‚è§„åˆ™å‘Šè¯‰ä½ å“ªäº›ç‰¹å¾é‡è¦ï¼‰

**ä¸é€‚åˆåœºæ™¯**ï¼š
- âŒ éœ€è¦æœ€é«˜ç²¾åº¦ï¼ˆç”¨éšæœºæ£®æ—æˆ–XGBoostï¼‰
- âŒ æ•°æ®é‡æå°ï¼ˆ<100è¡Œï¼Œæ˜“è¿‡æ‹Ÿåˆï¼‰
- âŒ å®æ—¶é«˜æ€§èƒ½è¦æ±‚ï¼ˆæ ‘éå†è™½å¿«ï¼Œä½†æ— æ³•ä¸çº¿æ€§æ¨¡å‹æ¯”ï¼‰

**å­¦ä¹ è·¯å¾„**ï¼š
```
å†³ç­–æ ‘ â†’ éšæœºæ£®æ— â†’ GBDT â†’ XGBoost/LightGBM
  â†‘
åŸºç¡€               è¿›é˜¶              é«˜çº§
```

---

## å‚è€ƒèµ„æº

- **å®˜æ–¹æ–‡æ¡£**ï¼šhttps://scikit-learn.org/stable/modules/tree.html
- **ç»å…¸æ•™æ**ï¼šHastie et al. "The Elements of Statistical Learning"
- **äº’åŠ¨å­¦ä¹ **ï¼šhttp://www.r2d3.us/visual-intro-to-machine-learning-part-1/
- **å®æˆ˜ç«èµ›**ï¼šKaggle åˆçº§ç«èµ›é€šå¸¸å†³ç­–æ ‘/éšæœºæ£®æ—å°±èƒ½å¾—é«˜åˆ†
- **ç›¸å…³ç®—æ³•**ï¼šRandom Forestã€Gradient Boostingã€XGBoost

éµå¾ªæ ‡å‡†ï¼š[[_ALGO_INSTRUCTIONS|Claudeåä½œæŒ‡å— v2.0]]
