---
tags: [数据挖掘, 机器学习, 知识体系, 导航]
math: true
difficulty: 入门
---

# 数据挖掘知识体系完全导览

## 🧭 什么是数据挖掘？

**数据挖掘 (Data Mining)** 是从大规模数据中自动发现隐藏的、非平凡的、有潜在价值的知识的过程。

核心要素：
- **数据**：规模大、维度多、结构复杂
- **算法**：自动学习模式和规则
- **知识**：可解释、可应用、有价值的洞见

---

## 📚 知识体系全景图

```
                    数据挖掘 (Data Mining)
                           |
        ___________________|___________________
       |                   |                   |
    模式识别          预测与分类          聚类与分组
   (Pattern)          (Prediction)      (Clustering)
       |                   |                   |
       ├─关联规则         ├─线性回归        ├─K-Means
       ├─序列模式         ├─逻辑回归        ├─层次聚类
       ├─频繁项集         ├─决策树          ├─DBSCAN
       └─异常检测         ├─随机森林        ├─高斯混合
                          ├─SVM              └─自组织映射
                          ├─神经网络
                          ├─RNN/LSTM
                          ├─CNN
                          └─Transformer
```

---

## 1️⃣ 监督学习 (Supervised Learning)

### 定义
有**标签数据**：$(x_i, y_i)$ 对，目标是学习映射 $f: x \to y$。

### 分类问题（Classification）

**目标**：预测**离散类别**（如：垃圾邮件 vs 正常邮件）

#### 传统方法

| 算法 | 优点 | 缺点 | 最佳场景 |
|---|---|---|---|
| **逻辑回归** | 快速、可解释、概率输出 | 只处理线性可分 | 小数据、需要概率 |
| **决策树** | 可解释、处理非线性、快速 | 易过拟合、不稳定 | 交互特征、混合数据类型 |
| **SVM** | 理论坚实、高维有效、泛化好 | 参数调试难、黑盒 | 高维小数据、二分类 |
| **朴素贝叶斯** | 快速、概率清晰、稀疏数据有效 | 特征独立假设强 | 文本分类、垃圾邮件 |
| **K-近邻** | 无需训练、局部决策 | 慢、参数敏感、维度诅咒 | 小数据原型 |

#### 集成学习

| 算法 | 机制 | 优势 | 推荐度 |
|---|---|---|---|
| **随机森林** ✅ | Bagging + 投票 | 参数少、泛化强、自动特征选择 | ⭐⭐⭐⭐⭐ 表格数据必选 |
| **GBDT** | Boosting（梯度提升） | 精度极高、处理不平衡 | ⭐⭐⭐⭐⭐ 竞赛首选 |
| **XGBoost** | GBDT 改进（正则化、并行） | 快、精、稳、参数丰富 | ⭐⭐⭐⭐⭐ Kaggle 神器 |
| **LightGBM** | 叶节点分裂、直方图优化 | 超快、内存省、大数据优化 | ⭐⭐⭐⭐ 100M+ 行数据 |
| **AdaBoost** | 加权样本、串行弱学习器 | 简单、理论清晰 | ⭐⭐⭐ 教学用 |

**➡️ 推荐学习路线**：逻辑回归 → 决策树 → **随机森林** → XGBoost

#### 深度学习

| 模型 | 输入 | 优势 | 劣势 | 适用场景 |
|---|---|---|---|---|
| **ANN** ✅ | 数值向量 | 通用、可微 | 需大数据、调参多 | 小-中数据集 |
| **CNN** ✅ | 图像 | 参数高效、平移不变 | 需大数据 | **计算机视觉** |
| **RNN/LSTM** ✅ | 序列 | 捕捉时间依赖、可变长 | 串行慢、梯度消失 | **时间序列、NLP** |
| **Transformer** ✅ | 序列 | 完全并行、长距离依赖 | O(n²) 复杂度、需大数据 | **现代 NLP、LLM** |

**➡️ 推荐学习路线**：ANN → CNN（视觉任务）或 RNN/Transformer（NLP 任务）

---

### 回归问题（Regression）

**目标**：预测**连续值**（如：房价、温度、库存）

| 算法 | 特点 | 适用 |
|---|---|---|
| **线性回归** | 简单、快速、可解释 | 线性关系、小数据 |
| **多项式回归** | 捕捉非线性 | 中等复杂度 |
| **岭回归 / Lasso** | 正则化防过拟合 | 多重共线性 |
| **SVM 回归** | 鲁棒、非线性 | 非线性关系 |
| **随机森林回归** | 处理非线性、特征交互 | **中等数据、混合特征** ⭐⭐⭐⭐ |
| **梯度提升回归** | 精度最高 | 大数据、需精度 ⭐⭐⭐⭐⭐ |
| **神经网络回归** | 极强表达力 | 大数据、复杂非线性 |

---

## 2️⃣ 无监督学习 (Unsupervised Learning)

### 定义
**无标签数据**：只有 $x_i$，目标是发现数据的内在结构。

### 聚类 (Clustering)

**目标**：将数据分组，同组内相似，组间差异大。

| 算法 | 原理 | 优点 | 缺点 | 最佳场景 |
|---|---|---|---|---|
| **K-Means** | 迭代重心更新 | 快速、直观、可扩展 | 需知道 K、球形聚类、异常值敏感 | 中等数据、球形簇 |
| **DBSCAN** | 密度连通性 | 任意形状、发现异常点 | 参数敏感 (eps, minpts)、稀疏数据困难 | **非球形、异常检测** ⭐⭐⭐⭐ |
| **层次聚类** | 树形分解 | 丰富的聚类信息、可视化树 | 慢 (O(n²))、难以修正错误 | 小数据、需树形结构 |
| **高斯混合模型** | EM 算法、概率模型 | 概率输出、理论清晰 | 需知道簇数、计算慢 | 小-中数据、需概率 |
| **谱聚类** | 图论、特征向量 | 处理任意形状、全局最优 | 内存占用大、参数选择难 | 非凸簇、图结构数据 |

**➡️ 推荐学习路线**：K-Means → DBSCAN → 高斯混合模型

### 降维 (Dimensionality Reduction)

**目标**：从高维数据提取低维表示，同时保留关键信息。

| 算法 | 类型 | 优点 | 用途 |
|---|---|---|---|
| **PCA** | 线性、无监督 | 快速、理论清晰、方差最大 | 可视化、去噪、加速训练 |
| **t-SNE** | 非线性、无监督 | 可视化效果好、保留局部结构 | **高维数据可视化** ⭐⭐⭐⭐ |
| **UMAP** | 非线性、无监督 | 保留全局结构、快速 | 可视化、特征学习 |
| **自编码器** | 神经网络、有监督 | 学习层次特征、端到端 | 特征提取、数据压缩 |

---

### 关联规则挖掘 (Association Rule Mining)

**目标**：发现数据间的隐藏关联（如：买 A 的人常买 B）。

| 算法 | 方法 | 复杂度 | 用途 |
|---|---|---|---|
| **Apriori** | 逐层增长频繁项集 | O(n²) | 小数据、规则挖掘 |
| **Eclat** | 深度优先搜索 | O(n) | 中等数据 |
| **FP-Growth** | 频繁模式树 | O(n log n) | **大数据、快速** ⭐⭐⭐⭐⭐ |

---

## 3️⃣ 强化学习 (Reinforcement Learning)

### 定义
**交互学习**：智体与环境交互，通过奖励信号优化决策。

| 概念 | 说明 |
|---|---|
| **状态 (State)** | 环境的当前配置 |
| **动作 (Action)** | 智体的选择 |
| **奖励 (Reward)** | 环境的反馈 |
| **策略 (Policy)** | 状态到动作的映射 |

| 算法 | 思路 | 应用 |
|---|---|---|
| **Q-Learning** | 学习最优动作值函数 | 游戏 AI（Atari） |
| **Policy Gradient** | 直接学习策略网络 | 连续控制（机器人） |
| **Actor-Critic** | 结合两者 | AlphaGo 基础 |
| **深度 Q 学习 (DQN)** | 用神经网络近似 Q 函数 | AlphaGo、自动驾驶 |

---

## 4️⃣ 特殊问题

### 异常检测 (Anomaly Detection)

**目标**：识别"不寻常"的数据点。

| 方法 | 原理 | 优点 | 适用 |
|---|---|---|---|
| **孤立森林** | 随机分割空间 | 快速、高维有效 | 高维、无标签 ⭐⭐⭐⭐⭐ |
| **局部离群因子** | 局部密度比较 | 任意形状 | 基于密度的异常 |
| **One-Class SVM** | 支撑向量机的单类版本 | 理论清晰 | 边界清晰的异常 |
| **自编码器** | 重构误差 | 深度学习、非线性 | 图像、复杂数据 |
| **隔离森林** | 基于随机森林 | 快速可扩展 | **大数据** ⭐⭐⭐⭐ |

### 不平衡分类 (Imbalanced Classification)

**问题**：正样本极少（如欺诈检测）。

| 方案 | 方法 | 优缺点 |
|---|---|---|
| **重采样** | 过采样少数类/欠采样多数类 | 灵活但易过拟合或丢失信息 |
| **加权** | 给少数类更高权重 | `class_weight='balanced'` |
| **阈值移动** | 改变分类决策边界 | 无需改模型 |
| **SMOTE** | 合成少数类样本 | 高级采样技术 |
| **Ensemble** | Balanced Bagging、EasyEnsemble | 强大且稳定 ⭐⭐⭐⭐ |

### 时间序列预测 (Time Series Forecasting)

| 模型 | 特点 | 场景 |
|---|---|---|
| **ARIMA** | 统计模型、可解释 | 线性时序、短期预测 |
| **指数平滑** | 加权历史 | 趋势和季节性 |
| **Prophet** | Facebook、可处理缺失值 | 商业预测、假期效应 |
| **LSTM** | 深度学习、长期依赖 | **长序列、非线性** ⭐⭐⭐⭐⭐ |
| **Transformer** | 完全并行、自注意力 | 超长序列、多变量 |
| **VAR** | 多元自回归 | 多变量耦合预测 |

### 推荐系统 (Recommendation Systems)

| 方法 | 原理 | 优缺点 |
|---|---|---|
| **协同过滤** | 相似用户/物品推荐 | 简单但冷启动问题 |
| **内容过滤** | 基于物品特征 | 需特征工程 |
| **矩阵分解** | SVD、NMF 分解用户-物品矩阵 | 高效但仍有冷启动 |
| **深度学习** | 端到端学习用户-物品交互 | 表达强但需大数据 |
| **混合系统** | 协同 + 内容 + 图神经网络 | **最优实践** ⭐⭐⭐⭐⭐ |

---

## 5️⃣ 自然语言处理 (NLP)

### 经典方法（统计 NLP）

| 任务 | 算法 | 性能 |
|---|---|---|
| **文本分类** | TF-IDF + 逻辑回归/SVM | 可用但过时 |
| **序列标注** | 条件随机场 (CRF) | 可靠但慢 |
| **文本相似度** | 余弦相似度、编辑距离 | 表面化 |

### 📜 现代替代方案

| 经典方法 | 现代替代 | 优势 | 推荐工具 |
|---|---|---|---|
| TF-IDF + SVM | BERT/RoBERTa 微调 | 精度高 30-50%，上下文理解 | HuggingFace Transformers ✅ |
| CRF 序列标注 | BERT Token Classification | 精度提升，端到端学习 | HuggingFace Token-Classification Pipeline ✅ |
| Word2Vec / GloVe | Sentence-BERT / Contextual Embeddings | 语义更强，处理多义词 | SBERT / BGE-M3 ✅ |
| 余弦相似度 | 预训练模型相似度 | 语义级别匹配 | Sentence-BERT ✅ |

### 神经 NLP（深度学习）

| 模型 | 用途 | 性能 |
|---|---|---|
| **Word2Vec / GloVe** | 词向量 | 基础但过时 |
| **RNN / LSTM** | 序列建模、机器翻译 | 好，但慢 |
| **Transformer** | **所有 NLP 任务** | 无敌 ⭐⭐⭐⭐⭐ |
| **BERT** | 文本理解、分类、相似度 | SOTA ⭐⭐⭐⭐⭐ |
| **GPT** | 文本生成、对话、推理 | 超级智能 ⭐⭐⭐⭐⭐ |

---

## 6️⃣ 计算机视觉 (Computer Vision)

| 任务 | 算法 | 工具库 |
|---|---|---|
| **图像分类** | CNN / ResNet / Vision Transformer | PyTorch, TensorFlow |
| **目标检测** | YOLO / Faster R-CNN / RetinaNet | OpenCV, TensorFlow |
| **语义分割** | FCN / U-Net / DeepLab | PyTorch Segmentation |
| **实例分割** | Mask R-CNN / DETR | torchvision |
| **人脸识别** | FaceNet / VGGFace / ArcFace | Dlib, InsightFace |
| **3D 视觉** | PointNet / NeRF | Open3D |
| **视频理解** | 3D-CNN / SlowFast | PySlowFast |

### 📜 现代替代方案

| 经典/早期方法 | 现代替代 | 优势 | 推荐工具 |
|---|---|---|---|
| SIFT + SVM 分类 | Vision Transformer / EfficientNet | 精度高，参数高效 | TIMM / HuggingFace ✅ |
| Faster R-CNN | YOLO-v8 / DETR | 实时性好，精度高 | Ultralytics ✅ |
| FCN / U-Net | SegFormer / DINOv2 | 精度提升，训练快 | Transformers ✅ |
| Mask R-CNN | Mask2Former / DETR+Masks | 精度更高，灵活性强 | MMDetection ✅ |
| VGGFace / FaceNet | ArcFace / CosFace | 人脸识别精度 99%+ | InsightFace ✅ |
| 光流 + 手工特征 | Transformer / TimeSformer | 长期依赖捕捉，并行化 | PySlowFast ✅ |

---

## 📊 学习路线图

### 初级（3-6 个月）
```
数学基础
  ↓
  ├─ 线性代数（矩阵、特征值）
  ├─ 概率统计（分布、假设检验）
  └─ 微积分（导数、梯度）

数据预处理
  ├─ 数据清洗
  ├─ 特征工程
  └─ 数据可视化

传统机器学习（浅学习）
  ├─ 逻辑回归 ✅
  ├─ 决策树 ✅
  ├─ 随机森林 ✅
  └─ SVM
```

### 中级（6-12 个月）
```
深度学习基础
  ├─ 神经网络 (ANN) ✅
  ├─ 反向传播、优化
  └─ 正则化、批归一化

领域专项
  ├─ 计算机视觉 → CNN ✅
  ├─ 自然语言处理 → RNN/Transformer ✅
  └─ 时间序列 → LSTM ✅

Kaggle 实战
  └─ 竞赛、特征工程、集成
```

### 高级（12+ 个月）
```
前沿模型
  ├─ Transformer ✅（NLP 统治）
  ├─ Vision Transformer（视觉新范式）
  ├─ 多模态（CLIP、LLaVA）
  └─ 大语言模型（微调 GPT、BERT）

特殊领域
  ├─ 图神经网络 (GNN)
  ├─ 强化学习 (RL)
  ├─ 生成模型 (VAE, GAN, Diffusion)
  └─ 因果推断 (Causal Inference)

生产化
  ├─ 模型部署 (ONNX, TensorRT)
  ├─ MLOps (DVC, Airflow)
  └─ 监控和维护
```

---

## 🗂️ 本知识库已有文档

| 文档 | 深度 | 复杂度 | 适用领域 |
|---|---|---|---|
| **随机森林** ✅ | Bagging + 特征重要性 | ⭐⭐ | **表格数据** |
| **ANN** ✅ | 反向传播 + 正则化 | ⭐⭐⭐ | 通用深度学习 |
| **CNN** ✅ | 卷积 + 残差网络 | ⭐⭐⭐⭐ | **计算机视觉** |
| **RNN** ✅ | LSTM + 梯度消失 | ⭐⭐⭐⭐ | **序列建模、时间序列** |
| **Transformer** ✅ | 自注意力 + 并行化 | ⭐⭐⭐⭐⭐ | **现代 NLP、LLM** |

---

## 🎯 算法选择决策树

```
┌─ 有标签数据？
│
├─ YES → 监督学习
│  │
│  ├─ 分类问题？
│  │  ├─ 表格数据？
│  │  │  ├─ 小数据(<10K) → 随机森林 ✅
│  │  │  └─ 大数据(>100K) → XGBoost/LightGBM
│  │  │
│  │  ├─ 图像？
│  │  │  └─ CNN ✅
│  │  │
│  │  └─ 文本？
│  │     └─ Transformer (BERT) ✅
│  │
│  └─ 回归问题？
│     ├─ 表格数据 → 随机森林 / XGBoost ✅
│     ├─ 时间序列 → LSTM / Transformer ✅
│     └─ 图像回归 → CNN
│
└─ NO → 无监督学习
   │
   ├─ 聚类？
   │  ├─ 已知簇数？ → K-Means
   │  └─ 未知？ → DBSCAN ✅
   │
   ├─ 降维可视化？
   │  └─ t-SNE / UMAP
   │
   ├─ 异常检测？
   │  └─ 隔离森林 ✅
   │
   └─ 关联规则？
      └─ FP-Growth
```

---

## 💡 实践建议

### 数据集规模选择

| 规模 | 推荐算法 | 原因 |
|---|---|---|
| **<1000 行** | 逻辑回归、SVM、决策树 | 小数据过拟合风险，需简单模型 |
| **1K-100K 行** | **随机森林** ✅ | 最佳平衡，无需调参 |
| **100K-1M 行** | XGBoost / LightGBM | 集成优势明显，精度最高 |
| **>1M 行** | LightGBM / 深度学习 | 需要高效算法和大规模计算 |

### 特征工程的重要性

```
模型精度 = f(数据质量, 特征工程, 算法, 超参数)
          = 40%      + 40%         + 10% + 10%
```

**教训**：再好的算法也抵不过差的特征。花 50% 时间在特征工程上。

### 常见陷阱

❌ 数据泄漏（用测试集的统计量）
❌ 不进行 cross-validation
❌ 只用准确率评估不平衡数据（用 F1, AUC）
❌ 过度调参导致过拟合
❌ 忽视特征工程，一味堆叠模型

✅ 推荐做法：基础特征工程 → 简单模型 baseline → 逐步优化

---

## 📖 继续学习资源

### 教材
- 《统计学习方法》- 李航（经典）
- 《机器学习》- 周志华（教科书）
- 《深度学习》- Goodfellow（理论深度）

### 实战平台
- Kaggle（竞赛、数据集、讨论）
- Hugging Face（预训练模型、数据集）
- Papers with Code（论文实现、SOTA）

### 开源库
- scikit-learn（传统算法）
- PyTorch / TensorFlow（深度学习）
- XGBoost / LightGBM（梯度提升）
- Hugging Face Transformers（NLP 预训练模型）

---

## 🚀 总结

数据挖掘是**最新实践 + 深厚理论**的结合：

1. **选对方向**：先理解问题（分类/聚类/异常检测），再选算法
2. **重视数据**：数据质量 > 算法复杂度
3. **循序渐进**：从简到繁，从浅到深
4. **实战第一**：理论指导实践，实践反馈理论
5. **紧跟前沿**：Transformer 已成 NLP/多模态标配，需关注新论文

**最后忠告**：没有银弹。每个问题都需要：
- 📊 深入理解数据
- 🔍 领域知识
- 🧪 多种方法实验
- 📈 持续监控和改进