---
tags: [ç®—æ³•, æ·±åº¦å­¦ä¹ , è®¡ç®—æœºè§†è§‰, å·ç§¯ç¥ç»ç½‘ç»œ, CNN]
math: true
difficulty: å›°éš¾
---

# å·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Network, CNN)

## ğŸ’¡ æ ¸å¿ƒç›´è§‰

- **ä¸€å¥è¯å®šä¹‰**ï¼šé€šè¿‡å±€éƒ¨è¿æ¥ã€æƒé‡å…±äº«å’Œå±‚æ¬¡ç‰¹å¾æå–ï¼Œå°†åŸå§‹åƒç´ çº§è¾“å…¥é€å±‚æŠ½è±¡ä¸ºé«˜çº§è¯­ä¹‰ç‰¹å¾çš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚

- **è§£å†³é—®é¢˜**ï¼šè§£å†³äº†å…¨è¿æ¥ç½‘ç»œåœ¨å›¾åƒå¤„ç†ä¸­çš„ä¸¤å¤§éš¾é¢˜ï¼š(1) å‚æ•°çˆ†ç‚¸ï¼ˆå¦‚ 224Ã—224 RGB å›¾åƒè¿›å…¨è¿æ¥éœ€ 150M å‚æ•°ï¼‰ï¼Œ(2) å±€éƒ¨ç›¸å…³æ€§è¢«ç ´åï¼ˆå…¨è¿æ¥å¿½è§†åƒç´ çš„ç©ºé—´ç›¸é‚»æ€§ï¼‰ã€‚CNN åˆ©ç”¨å·ç§¯å’Œæ± åŒ–çš„**å±€éƒ¨æ€§å’Œå¹³ç§»ä¸å˜æ€§**ï¼Œå‚æ•°é«˜æ•ˆä¸”ç²¾åº¦æ›´ä¼˜ã€‚

- **æ ¸å¿ƒé€»è¾‘**ï¼šCNN = å·ç§¯å±‚ï¼ˆå±€éƒ¨ç‰¹å¾æå–ï¼‰+ æ± åŒ–å±‚ï¼ˆä¸‹é‡‡æ ·ï¼‰+ å…¨è¿æ¥å±‚ï¼ˆåˆ†ç±»ï¼‰ã€‚å·ç§¯æ ¸åœ¨ç©ºé—´ä¸Šæ»‘åŠ¨ï¼Œæƒé‡å…±äº«ä½¿å‚æ•°å¤§å¹…é™ä½ï¼›å¤šå±‚å †å å®ç°ä»ä½çº§è¾¹ç¼˜ç‰¹å¾ â†’ ä¸­çº§çº¹ç† â†’ é«˜çº§è¯­ä¹‰çš„æ¸è¿›æŠ½è±¡ã€‚

- **å‡ ä½•æ„ä¹‰**ï¼š
  - **å·ç§¯**ï¼šåœ¨ç‰¹å¾å›¾ä¸Šæ»‘åŠ¨å·ç§¯æ ¸ï¼Œè¾“å‡ºæ¯ä¸ªä½ç½®çš„å±€éƒ¨æ¨¡å¼å“åº”ï¼ˆå¯è§†åŒ–ä¸ºç‰¹å¾å›¾çš„æ¿€æ´»ï¼‰
  - **æ± åŒ–**ï¼šä¸‹é‡‡æ ·å’Œéçº¿æ€§å¤„ç†ï¼Œæå‡ç‰¹å¾çš„æŠ—å™ªæ€§å’Œå¹³ç§»ä¸å˜æ€§
  - **å¤šå±‚å †å **ï¼šæ„Ÿå—é‡é€å±‚æ‰©å¤§ï¼Œåº•å±‚å­¦ä¹ è¾¹ç¼˜ï¼Œé¡¶å±‚å­¦ä¹ æ•´ä½“ç»“æ„

- **æ€æ‰‹é” (Killer Feature)**ï¼š**å‚æ•°å…±äº«** + **å±€éƒ¨è¿æ¥** + **å±‚æ¬¡ç‰¹å¾**ã€‚å¥ å®šäº†ç°ä»£è®¡ç®—æœºè§†è§‰çš„åŸºç¡€ï¼šImageNetã€è‡ªåŠ¨é©¾é©¶è§†è§‰ç³»ç»Ÿã€åŒ»å­¦å½±åƒè¯Šæ–­ç­‰ã€‚ä¸€å¼ å›¾ç‰‡ 150M å‚æ•°çš„å…¨è¿æ¥å˜æˆ 60M å‚æ•°çš„ ResNet-50ï¼Œç²¾åº¦åè€Œæ›´é«˜ã€‚

> [!TIP] CNN çš„ç‰¹å¾æå–è¿‡ç¨‹
>
> ```
> åŸå§‹å›¾åƒ (224Ã—224Ã—3)
>    â†“
> å·ç§¯ + ReLU â†’ ç‰¹å¾å›¾1 (112Ã—112Ã—64)  [ä½çº§ï¼šè¾¹ç¼˜ã€è§’]
>    â†“
> æ± åŒ– â†’ ä¸‹é‡‡æ · (56Ã—56Ã—64)
>    â†“
> å·ç§¯ + ReLU â†’ ç‰¹å¾å›¾2 (56Ã—56Ã—128)   [ä¸­çº§ï¼šçº¹ç†ã€å½¢çŠ¶]
>    â†“
> æ± åŒ– â†’ ä¸‹é‡‡æ · (28Ã—28Ã—128)
>    â†“
> ... (å¤šå±‚å †å )
>    â†“
> å…¨å±€å¹³å‡æ± åŒ– (1Ã—1Ã—512)
>    â†“
> å…¨è¿æ¥ â†’ logits (num_classes)
>    â†“
> è¾“å‡ºç±»åˆ«æ¦‚ç‡
> ```

---

## ğŸ“ æ•°å­¦åŸç†

### 1. å·ç§¯æ“ä½œ (Convolution)

**2D å·ç§¯çš„æ ¸å¿ƒå…¬å¼**ï¼š

$$y[i, j] = \sum_{m=0}^{k_h-1} \sum_{n=0}^{k_w-1} w[m, n] \cdot x[i+m, j+n] + b$$

å…¶ä¸­ï¼š
- $x$ï¼šè¾“å…¥ç‰¹å¾å›¾ï¼Œå½¢çŠ¶ $(H, W)$
- $w$ï¼šå·ç§¯æ ¸ï¼ˆæƒé‡ï¼‰ï¼Œå½¢çŠ¶ $(k_h, k_w)$ï¼Œé€šå¸¸ 3Ã—3 æˆ– 5Ã—5
- $b$ï¼šåç½®ï¼ˆæ ‡é‡æˆ–å‘é‡ï¼‰
- $y$ï¼šè¾“å‡ºç‰¹å¾å›¾
- $i, j$ï¼šè¾“å‡ºä½ç½®çš„è¡Œåˆ—ç´¢å¼•

**è¾“å‡ºå¤§å°è®¡ç®—**ï¼ˆä¸è€ƒè™‘ batch å’Œé€šé“ï¼‰ï¼š

$$H_{out} = \frac{H_{in} - k_h + 2p}{s} + 1$$

$$W_{out} = \frac{W_{in} - k_w + 2p}{s} + 1$$

å…¶ä¸­ï¼š
- $p$ï¼špaddingï¼ˆå¡«å……ï¼‰
- $s$ï¼šstrideï¼ˆæ­¥é•¿ï¼‰
- $k_h, k_w$ï¼šå·ç§¯æ ¸å¤§å°

> [!ABSTRACT] ä¸ºä»€ä¹ˆå·ç§¯æ¯”å…¨è¿æ¥æ›´ä¼˜ï¼Ÿ
>
> **å‚æ•°å¯¹æ¯”**ï¼ˆä»¥ 3Ã—3 å·ç§¯å’Œå…¨è¿æ¥ä¸ºä¾‹ï¼‰ï¼š
> - å…¨è¿æ¥ï¼šä» (32Ã—32Ã—3) â†’ (32Ã—32Ã—64) éœ€ $(32 \times 32 \times 3) \times (32 \times 32 \times 64) = 192M$ å‚æ•°
> - å·ç§¯ï¼š3Ã—3Ã—3Ã—64 = 1728 ä¸ªå‚æ•°ï¼ˆæƒé‡å…±äº«ï¼‰ï¼Œ**å‚æ•°å°‘ 100,000 å€**
>
> **å½’çº³åç½®**ï¼ˆInductive Biasï¼‰ï¼š
> - å·ç§¯å‡è®¾ç‰¹å¾æ˜¯å±€éƒ¨ç›¸å…³çš„ï¼ˆç›¸é‚»åƒç´ æ›´ç›¸å…³ï¼‰
> - å·ç§¯å‡è®¾æ¨¡å¼åœ¨å›¾åƒå„å¤„å¹³ç§»ä¸å˜ï¼ˆè¾¹ç¼˜æ£€æµ‹å™¨åœ¨ä»»ä½•ä½ç½®éƒ½é€‚ç”¨ï¼‰
> - è¿™ä¸¤ä¸ªå‡è®¾å¯¹è‡ªç„¶å›¾åƒé«˜åº¦æœ‰æ•ˆ

### 2. å¤šé€šé“å·ç§¯

å®é™…å·ç§¯å¤„ç†å¤šé€šé“è¾“å…¥å’Œè¾“å‡ºï¼š

$$y_{out}[i, j, c_{out}] = \sum_{c_{in}=0}^{C_{in}-1} \sum_{m=0}^{k_h-1} \sum_{n=0}^{k_w-1} w[m, n, c_{in}, c_{out}] \cdot x[i+m, j+n, c_{in}] + b[c_{out}]$$

å½¢çŠ¶ç»Ÿè®¡ï¼š
- è¾“å…¥ï¼š$(H_{in}, W_{in}, C_{in})$
- å·ç§¯æ ¸ï¼š$(k_h, k_w, C_{in}, C_{out})$
- è¾“å‡ºï¼š$(H_{out}, W_{out}, C_{out})$
- **å‚æ•°æ•°**ï¼š$k_h \times k_w \times C_{in} \times C_{out}$ï¼ˆç›¸æ¯”å…¨è¿æ¥çš„ $H \times W \times C_{in} \times H \times W \times C_{out}$ å‡å°‘ $H \times W$ å€ï¼‰

### 3. æ± åŒ–æ“ä½œ (Pooling)

**æœ€å¤§æ± åŒ–**ï¼š
$$y[i, j] = \max_{m \in [0, k_h), n \in [0, k_w)} x[i \cdot s + m, j \cdot s + n]$$

**å¹³å‡æ± åŒ–**ï¼š
$$y[i, j] = \frac{1}{k_h \times k_w} \sum_{m=0}^{k_h-1} \sum_{n=0}^{k_w-1} x[i \cdot s + m, j \cdot s + n]$$

**ä½œç”¨**ï¼š
1. **ä¸‹é‡‡æ ·**ï¼šå‡å°‘ç‰¹å¾å›¾å¤§å°ï¼Œé™ä½è®¡ç®—é‡å’Œå†…å­˜ï¼ˆé€šå¸¸ 2Ã—2 stride=2 ä½¿å¤§å°å‡åŠï¼‰
2. **å¹³ç§»ä¸å˜æ€§**ï¼šå°çš„åƒç´ ç§»åŠ¨ä¸æ”¹å˜ max pooling çš„ç»“æœ
3. **ç‰¹å¾é€‰æ‹©**ï¼šmax pooling é€‰æ‹©æœ€å¼ºçš„ç‰¹å¾å“åº”

> [!TIP] æ± åŒ–çš„æ¢¯åº¦æµ
>
> - **Max Pooling**ï¼šæ¢¯åº¦åªå›ä¼ åˆ°æœ€å¤§å€¼ä½ç½®ï¼Œå…¶ä»–ä½ç½®æ¢¯åº¦ä¸º 0
> - **Average Pooling**ï¼šæ¢¯åº¦å‡åŒ€åˆ†æ•£åˆ°æ‰€æœ‰ä½ç½®
> - **Global Average Pooling**ï¼šå¯¹æ•´ä¸ªç‰¹å¾å›¾å–å¹³å‡ï¼Œé€šå¸¸ç”¨äºæœ€åä¸€å±‚ï¼ˆæ— å‚æ•°ï¼Œé¿å…è¿‡æ‹Ÿåˆï¼‰

### 4. åå‘ä¼ æ’­ï¼ˆå·ç§¯æ¢¯åº¦ï¼‰

å¯¹äºå·ç§¯å±‚ï¼Œæ¢¯åº¦è®¡ç®—ï¼š

$$\frac{\partial L}{\partial w[m, n]} = \sum_{i, j} \frac{\partial L}{\partial y[i,j]} \cdot x[i+m, j+n]$$

$$\frac{\partial L}{\partial x[i, j]} = \sum_{m, n} \frac{\partial L}{\partial y[i-m, j-n]} \cdot w[m, n]$$

å…³é”®è§‚å¯Ÿï¼š
- å¯¹æƒé‡çš„æ¢¯åº¦ï¼šè¾“å…¥ç‰¹å¾å›¾ä¸æ¢¯åº¦çš„"å·ç§¯"
- å¯¹è¾“å…¥çš„æ¢¯åº¦ï¼šæ¢¯åº¦ä¸å·ç§¯æ ¸çš„"è½¬ç½®å·ç§¯"ï¼ˆåå·ç§¯ï¼‰
- æƒé‡å…±äº«ä½¿ä¸åŒä½ç½®çš„æ¢¯åº¦ç´¯ç§¯æ±‚å’Œ

---

## ğŸ’» ç®—æ³•å®ç°

### PyTorch å®Œæ•´å®ç°ï¼ˆä»é›¶æ„å»º CNNï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10, MNIST
import matplotlib.pyplot as plt

# ===== æ‰‹å†™å·ç§¯å±‚ï¼ˆæ¼”ç¤ºï¼‰=====
class Conv2dManual(nn.Module):
    """æ‰‹å†™ 2D å·ç§¯å±‚ï¼ˆæ¼”ç¤ºç”¨ï¼Œå®é™…ä½¿ç”¨ nn.Conv2dï¼‰"""

    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super(Conv2dManual, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)
        self.stride = stride
        self.padding = padding

        # å‚æ•°ï¼šå·ç§¯æ ¸å’Œåç½®
        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, *self.kernel_size))
        self.bias = nn.Parameter(torch.randn(out_channels))

        # Kaiming åˆå§‹åŒ–
        nn.init.kaiming_normal_(self.weight, mode='fan_out', nonlinearity='relu')
        nn.init.constant_(self.bias, 0)

    def forward(self, x):
        """
        Args:
            x: (batch_size, in_channels, height, width)
        Returns:
            output: (batch_size, out_channels, out_h, out_w)
        """
        # ä½¿ç”¨ PyTorch çš„ F.conv2d å‡½æ•°
        return F.conv2d(x, self.weight, self.bias, stride=self.stride, padding=self.padding)

# ===== æ ‡å‡† CNN æ¶æ„ =====
class SimpleCNN(nn.Module):
    """ç®€å•çš„ CNN åˆ†ç±»å™¨"""

    def __init__(self, num_classes=10, input_channels=3):
        super(SimpleCNN, self).__init__()

        # å·ç§¯å— 1ï¼šè¾“å…¥ â†’ 64 é€šé“
        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 1/2 ä¸‹é‡‡æ ·

        # å·ç§¯å— 2ï¼š64 â†’ 128 é€šé“
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 1/4 ä¸‹é‡‡æ ·

        # å·ç§¯å— 3ï¼š128 â†’ 256 é€šé“
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(256)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 1/8 ä¸‹é‡‡æ ·

        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))

        # å…¨è¿æ¥å±‚
        self.fc = nn.Linear(256, num_classes)

        # Dropout é˜²è¿‡æ‹Ÿåˆ
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        """
        x: (batch_size, 3, 32, 32) for CIFAR-10
        """
        # å·ç§¯å— 1
        x = self.conv1(x)  # (batch, 64, 32, 32)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.pool1(x)  # (batch, 64, 16, 16)

        # å·ç§¯å— 2
        x = self.conv2(x)  # (batch, 128, 16, 16)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.pool2(x)  # (batch, 128, 8, 8)

        # å·ç§¯å— 3
        x = self.conv3(x)  # (batch, 256, 8, 8)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.pool3(x)  # (batch, 256, 4, 4)

        # å…¨å±€å¹³å‡æ± åŒ–
        x = self.global_avg_pool(x)  # (batch, 256, 1, 1)
        x = x.view(x.size(0), -1)  # (batch, 256)

        # å…¨è¿æ¥
        x = self.dropout(x)
        x = self.fc(x)  # (batch, num_classes)

        return x

# ===== æ®‹å·®å—ï¼ˆè§£å†³æ·±ç½‘ç»œæ¢¯åº¦æ¶ˆå¤±ï¼‰=====
class ResidualBlock(nn.Module):
    """æ®‹å·®å—ï¼šå…è®¸æ¢¯åº¦ç›´æ¥å›ä¼ """

    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()

        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # æ®‹å·®è¿æ¥ï¼šè‹¥å°ºå¯¸ä¸åŒ¹é…ï¼Œç”¨ 1Ã—1 å·ç§¯æŠ•å½±
        self.shortcut = nn.Identity()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        residual = self.shortcut(x)

        # ä¸»è·¯å¾„
        out = self.conv1(x)
        out = self.bn1(out)
        out = F.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        # æ®‹å·®è¿æ¥
        out = out + residual
        out = F.relu(out)

        return out

class ResNet(nn.Module):
    """ç®€åŒ–çš„ ResNet"""

    def __init__(self, num_classes=10, input_channels=3):
        super(ResNet, self).__init__()

        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(64)

        # æ®‹å·®å—å †å 
        self.layer1 = self._make_layer(64, 64, num_blocks=2, stride=1)
        self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2)
        self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2)

        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256, num_classes)

    def _make_layer(self, in_channels, out_channels, num_blocks, stride):
        """å †å å¤šä¸ªæ®‹å·®å—"""
        layers = []
        layers.append(ResidualBlock(in_channels, out_channels, stride=stride))
        for _ in range(1, num_blocks):
            layers.append(ResidualBlock(out_channels, out_channels, stride=1))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)

        x = self.global_avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        return x

# ===== è®­ç»ƒå™¨ =====
class CNNTrainer:
    """CNN è®­ç»ƒå™¨"""

    def __init__(self, model, learning_rate=1e-3, device='cpu'):
        self.model = model.to(device)
        self.device = device
        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        self.criterion = nn.CrossEntropyLoss()
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=10)

    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ª epoch"""
        self.model.train()
        epoch_loss = 0
        correct = 0
        total = 0

        for images, labels in train_loader:
            images = images.to(self.device)
            labels = labels.to(self.device)

            # å‰å‘ä¼ æ’­
            outputs = self.model(images)
            loss = self.criterion(outputs, labels)

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            epoch_loss += loss.item()

            # å‡†ç¡®ç‡
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

        self.scheduler.step()
        return epoch_loss / len(train_loader), correct / total

    def validate(self, val_loader):
        """éªŒè¯"""
        self.model.eval()
        val_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(self.device)
                labels = labels.to(self.device)

                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                val_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)

        return val_loss / len(val_loader), correct / total

    def train(self, train_loader, val_loader, epochs=10):
        """å®Œæ•´è®­ç»ƒå¾ªç¯"""
        train_losses = []
        val_losses = []
        train_accs = []
        val_accs = []

        for epoch in range(epochs):
            train_loss, train_acc = self.train_epoch(train_loader)
            val_loss, val_acc = self.validate(val_loader)

            train_losses.append(train_loss)
            val_losses.append(val_loss)
            train_accs.append(train_acc)
            val_accs.append(val_acc)

            print(f"Epoch {epoch+1}/{epochs}, "
                  f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
                  f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

        return train_losses, val_losses, train_accs, val_accs

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    # æ•°æ®é¢„å¤„ç†
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),  # æ•°æ®å¢å¼ºï¼šéšæœºè£å‰ª
        transforms.RandomHorizontalFlip(),     # æ°´å¹³ç¿»è½¬
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    transform_val = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    # åŠ è½½ CIFAR-10 æ•°æ®é›†
    train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    val_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform_val)

    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2)

    # åˆ›å»ºæ¨¡å‹
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # ä½¿ç”¨ç®€å• CNN
    print("===== SimpleCNN =====")
    model = SimpleCNN(num_classes=10, input_channels=3)
    trainer = CNNTrainer(model, learning_rate=1e-3, device=device)

    train_losses, val_losses, train_accs, val_accs = trainer.train(
        train_loader, val_loader, epochs=10
    )

    # ä½¿ç”¨ ResNet
    print("\n===== ResNet =====")
    model_resnet = ResNet(num_classes=10, input_channels=3)
    trainer_resnet = CNNTrainer(model_resnet, learning_rate=1e-3, device=device)

    train_losses_res, val_losses_res, train_accs_res, val_accs_res = trainer_resnet.train(
        train_loader, val_loader, epochs=10
    )

    # æ¯”è¾ƒç»“æœ
    print(f"\nSimpleCNN æœ€ç»ˆéªŒè¯ç²¾åº¦: {val_accs[-1]:.4f}")
    print(f"ResNet æœ€ç»ˆéªŒè¯ç²¾åº¦: {val_accs_res[-1]:.4f}")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='SimpleCNN Train')
    plt.plot(val_losses, label='SimpleCNN Val')
    plt.plot(train_losses_res, label='ResNet Train', linestyle='--')
    plt.plot(val_losses_res, label='ResNet Val', linestyle='--')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Training Loss Comparison')
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 2, 2)
    plt.plot(val_accs, label='SimpleCNN', marker='o')
    plt.plot(val_accs_res, label='ResNet', marker='s')
    plt.xlabel('Epoch')
    plt.ylabel('Validation Accuracy')
    plt.legend()
    plt.title('Validation Accuracy Comparison')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()
```

---

## ğŸ”§ è¶…å‚æ•°è°ƒä¼˜

### å…³é”®å‚æ•°è¯¦è§£

| å‚æ•° | å«ä¹‰ | å¯¹æ€§èƒ½çš„å½±å“ | æ¨èå€¼ |
|---|---|---|---|
| **kernel_size** | å·ç§¯æ ¸å¤§å° | 3Ã—3 â†’ å±€éƒ¨ï¼Œå¿«é€Ÿï¼›5Ã—5 â†’ å…¨å±€æ„Ÿå—é‡å¤§ï¼›å¤ªå¤§ â†’ å‚æ•°çˆ†ç‚¸ | 3Ã—3ï¼ˆæ ‡å‡†ï¼‰ |
| **stride** | å·ç§¯æ­¥é•¿ | å¤§ â†’ å¿«é€Ÿä¸‹é‡‡æ ·ä½†ç‰¹å¾ä¸¢å¤±ï¼›å° â†’ ä¿ç•™ä¿¡æ¯ä½†è®¡ç®—é‡å¤§ | 1ï¼ˆä¿ç•™ï¼‰, 2ï¼ˆä¸‹é‡‡æ ·ï¼‰ |
| **padding** | è¾¹ç•Œå¡«å…… | "same" (padding=1) â†’ ä¿æŒå°ºå¯¸ï¼›"valid" (padding=0) â†’ ç¼©å°å°ºå¯¸ | "same"ï¼ˆé€šå¸¸ï¼‰ |
| **out_channels** | è¾“å‡ºé€šé“æ•° | å¤š â†’ è¡¨è¾¾èƒ½åŠ›å¼ºä½†å‚æ•°å¤šï¼›é€šå¸¸ 64â†’128â†’256â†’512 é€’å¢ | 64-512ï¼ˆé€’å¢ï¼‰ |
| **pooling_size** | æ± åŒ–çª—å£ | 2Ã—2ï¼ˆæ ‡å‡†ï¼‰â†’ 1/4 ä¸‹é‡‡æ ·ï¼›3Ã—3 â†’ æ›´æ¿€è¿›çš„ä¸‹é‡‡æ · | 2ï¼ˆæ ‡å‡†ï¼‰ |
| **depth (num_blocks)** | ç½‘ç»œæ·±åº¦ | è¶Šæ·± â†’ è¶Šå¼ºçš„è¡¨è¾¾ï¼Œä½†æ¢¯åº¦æ¶ˆå¤±ï¼ˆéœ€ BatchNorm/æ®‹å·®ï¼‰ | 4-50ï¼ˆæœ‰æ®‹å·®ï¼‰ |
| **dropout_rate** | Dropout æ¯”ä¾‹ | é˜²è¿‡æ‹Ÿåˆï¼›å¤ªé«˜ â†’ æ¬ æ‹Ÿåˆï¼›å¤ªä½ â†’ æ— æ•ˆ | 0.3-0.5 |
| **batch_size** | æ‰¹é‡å¤§å° | å¤§ â†’ æ¢¯åº¦ä¼°è®¡å‡†ç¡®ä½†å†…å­˜å ç”¨ï¼›å° â†’ å™ªå£°å¤§ä½†æ­£åˆ™åŒ–æ•ˆæœ | 128-256 |
| **learning_rate** | å­¦ä¹ ç‡ | CNN ä¸å¦‚ Transformer æ•æ„Ÿï¼›é€šå¸¸ 1e-3-1e-4 | 1e-3ï¼ˆåˆå§‹ï¼‰|
| **data_augmentation** | æ•°æ®å¢å¼º | éšæœºè£å‰ªã€ç¿»è½¬ã€æ—‹è½¬ â†’ å¢å¼ºé²æ£’æ€§ï¼Œé˜²è¿‡æ‹Ÿåˆ | RandomCrop, Flip, Rotation |

> [!TIP] CNN ç‰¹æœ‰çš„è°ƒä¼˜æŠ€å·§
>
> 1. **æ„Ÿå—é‡åŒ¹é…**ï¼šæ„Ÿå—é‡åº”è¦†ç›–å›¾åƒè¯­ä¹‰å¯¹è±¡
>    - VGGNet ç”¨ 3Ã—3 å †å æ¨¡æ‹Ÿ 7Ã—7ï¼ˆå‚æ•°å°‘ï¼‰
>    - ResNet-50 æœ€åæ„Ÿå—é‡ > 400 åƒç´ ï¼ˆå¯¹ ImageNet è¶³å¤Ÿï¼‰
> 2. **BatchNorm ä½ç½®**ï¼šConv2d â†’ BatchNorm â†’ ReLUï¼ˆæ¨èï¼‰
> 3. **æ•°æ®å¢å¼º**ï¼šå¼ºæ•°æ®å¢å¼ºï¼ˆRandomCropã€ç¿»è½¬ï¼‰èƒ½æ˜¾è‘—é™ä½è¿‡æ‹Ÿåˆ
> 4. **å‚æ•°åˆå§‹åŒ–**ï¼šConv å±‚ç”¨ Kaiming åˆå§‹åŒ–ï¼ˆmode='fan_out', nonlinearity='relu'ï¼‰
> 5. **å­¦ä¹ ç‡è¡°å‡**ï¼šä½™å¼¦è¡°å‡æ¯”å›ºå®š lr æ›´ä¼˜

---

## âš–ï¸ ä¼˜ç¼ºç‚¹ä¸åœºæ™¯

### âœ… ä¼˜åŠ¿ (Pros)

1. **å‚æ•°é«˜æ•ˆ**ï¼šæƒé‡å…±äº«ä½¿å‚æ•°æ•°å¤§å¹…é™ä½ï¼ˆç›¸æ¯”å…¨è¿æ¥ 100-1000 å€ï¼‰
2. **å¹³ç§»ä¸å˜æ€§**ï¼šåŒä¸€æ¨¡å¼åœ¨å›¾åƒå„å¤„è¢«è¯†åˆ«
3. **å±€éƒ¨æ„Ÿå—é‡**ï¼šè‡ªç„¶åˆ©ç”¨å›¾åƒçš„å±€éƒ¨ç›¸å…³æ€§
4. **å±‚æ¬¡ç‰¹å¾**ï¼šæµ…å±‚å­¦è¾¹ç¼˜ï¼Œæ·±å±‚å­¦è¯­ä¹‰ï¼Œå¯è§£é‡Šæ€§å¥½
5. **è®¡ç®—é«˜æ•ˆ**ï¼šå·ç§¯æœ‰é«˜åº¦ä¼˜åŒ–çš„ç¡¬ä»¶å®ç°ï¼ˆGPUã€TPUï¼‰
6. **å®è¯æ•ˆæœå¼º**ï¼šImageNetã€COCO ç­‰ç«èµ›æ•°æ®é›†ä¸Šæœ‰å‹å€’æ€§ä¼˜åŠ¿

### âŒ åŠ£åŠ¿ (Cons)

1. **æ·±åº¦å—é™**ï¼š> 100 å±‚éœ€è¦æ®‹å·®æˆ–å…¶ä»–æŠ€å·§è§£å†³æ¢¯åº¦æ¶ˆå¤±
2. **é•¿è·ç¦»ä¾èµ–å›°éš¾**ï¼šæ„Ÿå—é‡éœ€å¤šå±‚å †å ï¼Œè®¡ç®—å¤æ‚
3. **å›ºå®šè¾“å…¥å¤§å°**ï¼šä¼ ç»Ÿ CNN éœ€å›ºå®šåˆ†è¾¨ç‡ï¼ˆå…¨å·ç§¯å¯å˜ï¼‰
4. **å¹³ç§»ç­‰å˜æ€§ä¸å®Œç¾**ï¼šå°ä½ç§»ä¼šæ”¹å˜ pooling ç»“æœ
5. **å¯¹å¯¹ç§°æ€§åˆ©ç”¨ä¸è¶³**ï¼šéœ€è¦æ•°æ®å¢å¼ºè¡¥å¿
6. **è¿‡æ‹Ÿåˆé£é™©**ï¼šå‚æ•°å¤šï¼Œéœ€å¼ºæ­£åˆ™åŒ–æˆ–å¤§æ•°æ®

### ğŸ¯ é€‚ç”¨åœºæ™¯

| åœºæ™¯ | é€‚ç”¨åº¦ | åŸå›  |
|---|---|---|
| **å›¾åƒåˆ†ç±»** | â­â­â­â­â­ | CNN çš„ä¸»åœºï¼ŒResNet ç­‰æ— æ•Œ |
| **ç›®æ ‡æ£€æµ‹** | â­â­â­â­â­ | Faster R-CNNã€YOLO ç­‰éƒ½åŸºäº CNN éª¨å¹² |
| **è¯­ä¹‰åˆ†å‰²** | â­â­â­â­â­ | FCNã€U-Net ç­‰æ˜¯å·ç§¯çš„è‡ªç„¶å»¶ä¼¸ |
| **å®ä¾‹åˆ†å‰²** | â­â­â­â­â­ | Mask R-CNN æ ‡é… |
| **åŒ»å­¦å½±åƒ** | â­â­â­â­â­ | CTã€MRI å›¾åƒåˆ†æï¼Œ3D-CNN æ ‡å‡† |
| **è‡ªåŠ¨é©¾é©¶è§†è§‰** | â­â­â­â­â­ | è½¦é“çº¿ã€è¡Œäººã€äº¤é€šç¯æ£€æµ‹ç­‰ |
| **äººè„¸è¯†åˆ«** | â­â­â­â­ | VGGFaceã€FaceNet åŸºç¡€ï¼Œå·²è¢« Vision Transformer æŒ‘æˆ˜ |
| **NLPï¼ˆå›¾åƒæ–‡æœ¬ï¼‰** | â­â­â­ | æ–‡æœ¬å›¾åƒåŒ–åç”¨ CNNï¼Œç°å¤šç”¨ Transformer |
| **é•¿åºåˆ—æ–‡æœ¬** | â­â­ | æ„Ÿå—é‡é—®é¢˜ï¼ŒTransformer æ›´ä¼˜ |
| **3D ç‚¹äº‘** | â­â­â­ | å¯ç”¨ä½† PointNet æ›´é«˜æ•ˆ |

---

## ğŸ’¬ é¢è¯•å¿…é—®

> [!question] Q1: æ¨å¯¼å·ç§¯æ“ä½œçš„åå‘ä¼ æ’­ï¼Œä¸ºä»€ä¹ˆæ¢¯åº¦è®¡ç®—æ¶‰åŠ"è½¬ç½®å·ç§¯"ï¼Ÿ
>
> **ç­”æ¡ˆæ¡†æ¶**ï¼š
>
> **æ­£å‘ä¼ æ’­**ï¼š
>
> $$y[i,j] = \sum_{m,n} w[m,n] \cdot x[i+m, j+n] + b$$
>
> **å¯¹è¾“å…¥çš„æ¢¯åº¦**ï¼ˆåå‘ä¼ æ’­ï¼‰ï¼š
>
> $$\frac{\partial L}{\partial x[i,j]} = \sum_{m,n} \frac{\partial L}{\partial y[i-m, j-n]} \cdot w[m,n]$$
>
> è¿™ç­‰ä»·äºç”¨æ¢¯åº¦ $\frac{\partial L}{\partial y}$ ä¸**ç¿»è½¬çš„å·ç§¯æ ¸** $w_{flipped}$ è¿›è¡Œå·ç§¯ï¼š
>
> $$\frac{\partial L}{\partial x} = \text{conv}(\nabla y, w_{flipped}) + \text{padding}$$
>
> **å¯¹æƒé‡çš„æ¢¯åº¦**ï¼š
>
> $$\frac{\partial L}{\partial w[m,n]} = \sum_{i,j} \frac{\partial L}{\partial y[i,j]} \cdot x[i+m, j+n]$$
>
> è¿™æ˜¯**è¾“å…¥ä¸æ¢¯åº¦çš„å·ç§¯**ï¼ˆä¸ç¿»è½¬ï¼‰ã€‚
>
> **ä¸ºä»€ä¹ˆæ¶‰åŠè½¬ç½®å·ç§¯**ï¼š
> - è½¬ç½®å·ç§¯ï¼ˆåå·ç§¯ï¼‰å®ç°çš„æ˜¯**å·ç§¯çš„é€†**ï¼šè‹¥å·ç§¯ç¼©å°å°ºå¯¸ï¼Œåå·ç§¯æ‰©å¤§
> - æ¢¯åº¦å›ä¼ æ—¶ï¼Œ$\frac{\partial L}{\partial x}$ çš„ç©ºé—´å°ºå¯¸ä¸ $y$ ä¸€è‡´ï¼ˆéƒ½æ˜¯å·ç§¯çš„è¾“å…¥è¾“å‡ºå…³ç³»ï¼‰ï¼Œå› æ­¤éœ€è¦"è½¬ç½®"æ“ä½œè¡¥å¿

> [!question] Q2: ä¸ºä»€ä¹ˆæ®‹å·®è¿æ¥ï¼ˆSkip Connectionï¼‰èƒ½è§£å†³æ·±ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±ï¼Ÿ
>
> **ç­”æ¡ˆæ ¸å¿ƒ**ï¼š
>
> **æ— æ®‹å·®çš„æ·±ç½‘ç»œ**ï¼š
>
> æ¯å±‚è¾“å‡ºï¼š$y_l = f_l(y_{l-1})$
>
> æ¢¯åº¦åå‘ä¼ æ’­ï¼š
>
> $$\frac{\partial L}{\partial y_{l-1}} = \frac{\partial L}{\partial y_l} \cdot \frac{\partial f_l}{\partial y_{l-1}}$$
>
> é“¾å¼ä¹˜ç§¯ï¼š$\prod_{l=0}^{L} \frac{\partial f_l}{\partial y_{l-1}}$ï¼Œè‹¥æ¯é¡¹ $< 1$ï¼Œæ¢¯åº¦æŒ‡æ•°è¡°å‡ã€‚
>
> **æœ‰æ®‹å·®çš„ç½‘ç»œ**ï¼ˆResNetï¼‰ï¼š
>
> $$y_l = y_{l-1} + f_l(y_{l-1})$$
>
> æ¢¯åº¦åå‘ä¼ æ’­ï¼š
>
> $$\frac{\partial L}{\partial y_{l-1}} = \frac{\partial L}{\partial y_l} \left(1 + \frac{\partial f_l}{\partial y_{l-1}}\right)$$
>
> å…³é”®ï¼š**åŠ å·é¡¹**ä¿è¯æ¢¯åº¦è‡³å°‘ä¸º 1ï¼ˆå³ä½¿ $\frac{\partial f_l}{\partial y_{l-1}}$ å¾ˆå°ï¼‰ï¼Œæ¢¯åº¦ä¸ä¼šæŒ‡æ•°è¡°å‡ï¼
>
> **æ•°å­¦è¯æ˜**ï¼š
> ä» $y_0$ åˆ° $y_L$ çš„æ¢¯åº¦ï¼š
> $$\frac{\partial L}{\partial y_0} = \frac{\partial L}{\partial y_L} + \sum_{l=1}^{L} \frac{\partial L}{\partial y_L} \cdot \prod_{i=l}^{L-1} \left(1 + \frac{\partial f_i}{\partial y_i}\right)$$
>
> æ¢¯åº¦é€šè¿‡ç›´æ¥è·¯å¾„ä¼ æ’­ï¼Œé¿å…äº†æ¢¯åº¦æ¶ˆå¤±ï¼

> [!question] Q3: CNN ä¸å…¨è¿æ¥ç½‘ç»œç›¸æ¯”ä¸ºä»€ä¹ˆå‚æ•°æ›´å°‘ä½†ç²¾åº¦æ›´é«˜ï¼Ÿä¸ºä»€ä¹ˆ 3Ã—3 å·ç§¯å †å ä¼˜äºå¤§å·ç§¯æ ¸ï¼Ÿ
>
> **ç­”æ¡ˆæ ¸å¿ƒ**ï¼š
>
> **å‚æ•°å¯¹æ¯”**ï¼ˆä»¥ 5 å±‚ç½‘ç»œä¸ºä¾‹ï¼Œè¾“å…¥ 32Ã—32Ã—3 â†’ 32Ã—32Ã—64ï¼‰ï¼š
>
> - **å…¨è¿æ¥**ï¼š$(32 \times 32 \times 3) \times (32 \times 32 \times 64) = 192M$ å‚æ•°
> - **å• 5Ã—5 å·ç§¯**ï¼š$5 \times 5 \times 3 \times 64 = 4.8K$ å‚æ•°
> - **å‚æ•°å°‘ 40,000 å€**
>
> **ä¸ºä»€ä¹ˆç²¾åº¦æ›´é«˜**ï¼š
> 1. **å½’çº³åç½®**ï¼šCNN é€šè¿‡å±€éƒ¨è¿æ¥å’Œæƒé‡å…±äº«ï¼Œéšå¼å‡è®¾ç‰¹å¾å±€éƒ¨ç›¸å…³ï¼ˆè‡ªç„¶å›¾åƒçš„æ€§è´¨ï¼‰
> 2. **æ­£åˆ™åŒ–æ•ˆæœ**ï¼šæƒé‡å…±äº«ç›¸å½“äºå¼ºæ­£åˆ™åŒ–ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ
> 3. **ç‰¹å¾å¯è§£é‡Š**ï¼šåº•å±‚ç‰¹å¾æ˜¯äººç±»å¯ç†è§£çš„ï¼ˆè¾¹ç¼˜ã€çº¹ç†ï¼‰
>
> **3Ã—3 å åŠ  vs å¤§å·ç§¯æ ¸**ï¼š
>
> | å¯¹æ¯”ç»´åº¦ | 5Ã—5 å•æ ¸ | 3Ã—3 å åŠ ï¼ˆ2 å±‚ï¼‰|
> |---|---|---|
> | å‚æ•°æ•° | $5 \times 5 = 25$ | $3 \times 3 + 3 \times 3 = 18$ |
> | æ„Ÿå—é‡ | 5Ã—5 | 5Ã—5ï¼ˆç­‰ä»·ï¼‰ |
> | éçº¿æ€§ | 1 æ¬¡ | 2 æ¬¡ï¼ˆä¸­é—´ ReLUï¼‰ |
> | è®¡ç®—å¤æ‚åº¦ | ä½ | ç•¥é«˜ï¼Œä½†é€šå¸¸æ›´ä¼˜ |
>
> **VGG çš„å‘ç°**ï¼šä¸¤ä¸ª 3Ã—3 å·ç§¯ï¼ˆå‚æ•° 18ï¼‰ä¼˜äºä¸€ä¸ª 5Ã—5ï¼ˆå‚æ•° 25ï¼‰
> - å¤šå±‚éçº¿æ€§æ›´å¼º
> - å‚æ•°æ›´å°‘
> - æ„Ÿå—é‡ç›¸åŒä½†æ›´é«˜æ•ˆ
>
> **ç°ä»£è¶‹åŠ¿**ï¼š1Ã—1 å·ç§¯å¯è¿›ä¸€æ­¥é™å‚ï¼Œå¦‚ MobileNet ç”¨ depthwise separable convolution å‚æ•°å†å‡ 8-9 å€