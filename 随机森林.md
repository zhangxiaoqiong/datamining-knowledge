---
tags: [ç®—æ³•, æœºå™¨å­¦ä¹ , ç›‘ç£å­¦ä¹ , åˆ†ç±»/å›å½’, é›†æˆå­¦ä¹ ]
math: true
difficulty: è¿›é˜¶
---

# éšæœºæ£®æ— (Random Forest)

## ğŸ’¡ æ ¸å¿ƒç›´è§‰

- **ä¸€å¥è¯å®šä¹‰**ï¼šé€šè¿‡æ„å»ºå¤šæ£µå†³ç­–æ ‘ï¼Œå¹¶åœ¨æ•°æ®å’Œç‰¹å¾ç»´åº¦ä¸Šå¼•å…¥éšæœºæ€§ï¼Œæœ€åå¯¹æ‰€æœ‰æ ‘çš„é¢„æµ‹è¿›è¡ŒæŠ•ç¥¨ï¼ˆåˆ†ç±»ï¼‰æˆ–å¹³å‡ï¼ˆå›å½’ï¼‰çš„é›†æˆå­¦ä¹ ç®—æ³•ã€‚

- **è§£å†³é—®é¢˜**ï¼šè§£å†³äº†å•æ£µå†³ç­–æ ‘å®¹æ˜“è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œé€šè¿‡"æ°‘ä¸»æŠ•ç¥¨"æœºåˆ¶å¤§å¹…æå‡æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

- **æ ¸å¿ƒé€»è¾‘**ï¼šéšæœºæ£®æ— = å¤šæ£µå†³ç­–æ ‘ï¼ˆæ ‘çš„å¤šæ ·æ€§æ¥è‡ªæ•°æ®é‡‡æ ·+ç‰¹å¾é‡‡æ ·ï¼‰+ Baggingé›†æˆ + æŠ•ç¥¨èšåˆã€‚

- **å‡ ä½•æ„ä¹‰**ï¼šåœ¨ç‰¹å¾ç©ºé—´ä¸­ï¼Œéšæœºæ£®æ—é€šè¿‡å¤šæ£µæ ‘çš„ç»„åˆå†³ç­–è¾¹ç•Œï¼Œå½¢æˆå¤æ‚çš„éçº¿æ€§åˆ†å‰²é¢ã€‚å•æ£µæ ‘çš„å†³ç­–è¾¹ç•Œæ˜¯è½´å¯¹é½çš„çŸ©å½¢ï¼Œæ£®æ—çš„ç»„åˆè¾¹ç•Œæ˜¯è¿™äº›çŸ©å½¢çš„äº¤é›†ä¸å¹¶é›†ï¼Œå¯ä»¥é€¼è¿‘ä»»æ„æ›²çº¿ã€‚

- **æ€æ‰‹é” (Killer Feature)**ï¼šKaggle ç»“æ„åŒ–æ•°æ®ç«èµ›çš„é¦–é€‰ç®—æ³•ï¼Œå…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€è‡ªåŠ¨ç‰¹å¾é€‰æ‹©èƒ½åŠ›å¼ºã€æ˜“äºå¹¶è¡Œè®¡ç®—ã€‚

> [!TIP] æ ¸å¿ƒå›¾è§£
>
> ```
> åŸå§‹æ•°æ®é›† D
>        |
>        â”œâ”€â†’ Bootstrap Sample 1 â†’ å†³ç­–æ ‘ T1
>        â”œâ”€â†’ Bootstrap Sample 2 â†’ å†³ç­–æ ‘ T2
>        â”œâ”€â†’ Bootstrap Sample 3 â†’ å†³ç­–æ ‘ T3
>        â””â”€â†’ Bootstrap Sample N â†’ å†³ç­–æ ‘ TN
>
> é¢„æµ‹æ—¶ï¼šå¯¹æ‰€æœ‰æ ‘çš„é¢„æµ‹è¿›è¡ŒæŠ•ç¥¨ï¼ˆåˆ†ç±»ï¼‰æˆ–å¹³å‡ï¼ˆå›å½’ï¼‰
> æœ€ç»ˆé¢„æµ‹ = argmax(Vote from T1, T2, ..., TN)  æˆ–  (T1+T2+...+TN)/N
> ```
>
> **å…³é”®ç‚¹**ï¼š
> - æ•°æ®å¤šæ ·æ€§ï¼šé€šè¿‡ Bootstrap æœ‰æ”¾å›é‡‡æ ·ï¼Œæ¯æ£µæ ‘çœ‹åˆ°çš„è®­ç»ƒé›†ç•¥æœ‰ä¸åŒ
> - ç‰¹å¾å¤šæ ·æ€§ï¼šæ¯æ¬¡åˆ†è£‚æ—¶ï¼Œä» $\sqrt{p}$ ä¸ªéšæœºç‰¹å¾ä¸­é€‰æ‹©æœ€ä¼˜åˆ†è£‚ï¼ˆ$p$ æ˜¯æ€»ç‰¹å¾æ•°ï¼‰
> - èšåˆå¤šæ ·æ€§ï¼šå¤šä¸ªæ¨¡å‹çš„ç‹¬ç«‹å†³ç­–é€šè¿‡æŠ•ç¥¨å®ç°ï¼Œé™ä½æ–¹å·®

---

## ğŸ“ æ•°å­¦åŸç†

### 1. å†³ç­–æ ‘åˆ†è£‚å‡†åˆ™ï¼ˆGini ä¸çº¯åº¦ï¼‰

éšæœºæ£®æ—çš„æ¯æ£µæ ‘ä½¿ç”¨**Gini ä¸çº¯åº¦**æ¥è¯„ä¼°åˆ†è£‚è´¨é‡ï¼š

$$\text{Gini}(D) = 1 - \sum_{c=1}^{K} p_c^2$$

å…¶ä¸­ï¼š
- $D$ï¼šæ•°æ®é›†
- $p_c$ï¼šç±»åˆ« $c$ åœ¨æ•°æ®é›†ä¸­çš„æ¯”ä¾‹
- $K$ï¼šç±»åˆ«æ€»æ•°

**ç›´è§‚è§£é‡Š**ï¼šGini è¶Šå°ï¼Œæ ·æœ¬è¶Šçº¯ï¼ˆåŒä¸€ç±»åˆ«å±…å¤šï¼‰ï¼›Gini è¶Šå¤§ï¼Œæ ·æœ¬è¶Šæ‚ï¼ˆå¤šä¸ªç±»åˆ«æ··åˆï¼‰ã€‚

å¯¹äºä¸€ä¸ªåˆ†è£‚ $\text{split}$ï¼Œé€‰æ‹©èƒ½æœ€å¤§åŒ–**åŠ æƒä¿¡æ¯å¢ç›Š**çš„ç‰¹å¾ï¼š

$$\text{Gain}_\text{Gini} = \text{Gini}(D) - \sum_v \frac{|D_v|}{|D|} \text{Gini}(D_v)$$

å…¶ä¸­ï¼š
- $D_v$ï¼šåˆ†è£‚åçš„å­é›†
- $|D|, |D_v|$ï¼šæ ·æœ¬æ•°

**å…³é”®ç‚¹**ï¼šç‰¹å¾éšæœºåŒ–ï¼ˆä» $\sqrt{p}$ ä¸ªç‰¹å¾ä¸­é€‰æ‹©ï¼‰æ„å‘³ç€ä¸æ˜¯æ‰€æœ‰æ ‘éƒ½ä¼˜å…ˆé€‰æ‹©æœ€ä¼˜ç‰¹å¾ï¼Œä»è€Œé™ä½æ ‘é—´ç›¸å…³æ€§ã€‚

> [!ABSTRACT] æ ¸å¿ƒå‡è®¾
>
> éšæœºæ£®æ—å‡è®¾ï¼š
> 1. Bootstrap æ ·æœ¬è¶³å¤Ÿå¤§ï¼Œèƒ½å……åˆ†ä»£è¡¨åŸæ•°æ®åˆ†å¸ƒ
> 2. æ ‘æ·±åº¦è¶³å¤Ÿæ·±ä»¥æ•æ‰éçº¿æ€§äº¤äº’ï¼Œç‰¹å¾éšæœºåŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
> 3. ç‰¹å¾ç‹¬ç«‹é‡‡æ ·ä¸ä¼šå¯¼è‡´æç«¯çš„ç‰¹å¾é—æ¼

### 2. é¢„æµ‹å‡½æ•°

å¯¹äº**å›å½’**é—®é¢˜ï¼š

$$\hat{y}_i = \frac{1}{B} \sum_{b=1}^{B} T_b(x_i)$$

å…¶ä¸­ï¼š
- $T_b(x_i)$ï¼šç¬¬ $b$ æ£µæ ‘å¯¹æ ·æœ¬ $x_i$ çš„é¢„æµ‹å€¼
- $B$ï¼šæ£®æ—ä¸­æ ‘çš„æ€»æ•°

å¯¹äº**åˆ†ç±»**é—®é¢˜ï¼Œä½¿ç”¨å¤šæ•°æŠ•ç¥¨ï¼š

$$\hat{y}_i = \arg\max_{c} \sum_{b=1}^{B} \mathbb{1}[T_b(x_i) = c]$$

å…¶ä¸­ï¼š
- $\mathbb{1}[\cdot]$ï¼šæŒ‡ç¤ºå‡½æ•°

> [!TIP] å¤šæ•°æŠ•ç¥¨ vs æ¦‚ç‡å¹³å‡
>
> è™½ç„¶æˆ‘ä»¬ç”¨å¤šæ•°æŠ•ç¥¨ä½œä¸ºæœ€ç»ˆé¢„æµ‹ï¼Œä½† `predict_proba()` è¿”å›æ¯ä¸ªç±»åˆ«çš„**æ¦‚ç‡å¹³å‡**ï¼š
> $$P(y=c|x_i) = \frac{1}{B} \sum_{b=1}^{B} P(y=c|x_i, T_b)$$
> è¿™é€šå¸¸æ¯”ç¡¬æŠ•ç¥¨æä¾›æ›´å¹³æ»‘çš„å†³ç­–è¾¹ç•Œã€‚

### 3. Bagging çš„æ–¹å·®é™ä½ç†è®º

**æ¨å¯¼è¿‡ç¨‹**ï¼šå‡è®¾ $B$ æ£µæ ‘çš„é¢„æµ‹ $T_1, T_2, \ldots, T_B$ ç›¸äº’ç‹¬ç«‹ä¸”åŒåˆ†å¸ƒï¼Œå„æ ‘æ–¹å·®ä¸º $\sigma^2$ã€‚

æ£®æ—é¢„æµ‹çš„æ–¹å·®ï¼š

$$\text{Var}(\bar{T}) = \text{Var}\left(\frac{1}{B}\sum_{b=1}^{B} T_b\right) = \frac{1}{B^2} \sum_{b=1}^{B} \text{Var}(T_b) = \frac{\sigma^2}{B}$$

ä½†å®é™…ä¸Šæ ‘é—´å­˜åœ¨ç›¸å…³æ€§ï¼ˆå…±äº«ç›¸åŒçš„æ•°æ®åˆ†å¸ƒï¼‰ï¼Œä¿®æ­£å…¬å¼ä¸ºï¼š

$$\text{Var}(\bar{T}) = \frac{\rho \sigma^2}{B} + (1-\rho)\sigma^2$$

å…¶ä¸­ï¼š
- $\rho$ï¼šæ ‘é—´é¢„æµ‹çš„ç›¸å…³ç³»æ•°ï¼ˆ$0 < \rho \leq 1$ï¼‰
- ç¬¬ä¸€é¡¹ï¼šéš $B$ å¢åŠ è€Œå‡å°ï¼ˆBagging çš„æ”¶ç›Šï¼‰
- ç¬¬äºŒé¡¹ï¼šä¸ $B$ æ— å…³ï¼ˆæ ‘é—´ç›¸å…³æ€§é€ æˆçš„ä¸‹é™ï¼‰

**å…³é”®æ´å¯Ÿ**ï¼š
- å½“ $\rho = 1$ï¼ˆå®Œå…¨ç›¸å…³ï¼Œå³æ‰€æœ‰æ ‘ä¸€è‡´ï¼‰ï¼Œæ–¹å·®æ— æ³•é™ä½ï¼š$\text{Var}(\bar{T}) = \sigma^2$
- å½“ $\rho = 0$ï¼ˆå®Œå…¨ç‹¬ç«‹ï¼‰ï¼Œæ–¹å·®é™ä½åˆ° $\sigma^2 / B$
- **ç‰¹å¾éšæœºåŒ–çš„æ ¸å¿ƒä½œç”¨**ï¼šå‡å° $\rho$ï¼Œä½¿ç¬¬ä¸€é¡¹çš„ $1/B$ å› å­èƒ½æœ‰æ•ˆå‘æŒ¥ä½œç”¨

### 4. Bootstrap é‡‡æ ·çš„ç»Ÿè®¡æ€§è´¨

ä»å¤§å°ä¸º $m$ çš„æ•°æ®é›†ä¸­æœ‰æ”¾å›é‡‡æ · $m$ æ¬¡ï¼Œä¸€ä¸ªæ ·æœ¬**æœªè¢«é€‰ä¸­**çš„æ¦‚ç‡ï¼š

$$(1 - 1/m)^m \approx e^{-1} \approx 0.368$$

å› æ­¤çº¦ 36.8% çš„æ ·æœ¬ï¼ˆOOB - Out-of-Bagï¼‰å¯ç”¨äºæ— åè¯¯å·®ä¼°è®¡ï¼Œæ— éœ€é¢å¤–éªŒè¯é›†ã€‚

---

## ğŸ’» ç®—æ³•å®ç°

### æ ¸å¿ƒé€»è¾‘ä¼ªä»£ç ï¼ˆå¸¦è¯¦ç»†æ³¨é‡Šï¼‰

```python
import numpy as np
from scipy.stats import mode

class SimpleRandomForest:
    """NumPy å®ç°çš„éšæœºæ£®æ—ï¼ˆå›å½’ç‰ˆæœ¬ï¼‰"""

    def __init__(self, n_estimators=100, max_depth=10, max_features='sqrt', random_state=42):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.max_features = max_features
        self.random_state = random_state
        self.trees = []
        self.feature_importances_ = None

    def fit(self, X, y):
        """è®­ç»ƒéšæœºæ£®æ—"""
        np.random.seed(self.random_state)
        n_samples, n_features = X.shape

        # ç¡®å®šæ¯æ¬¡åˆ†è£‚ä½¿ç”¨çš„ç‰¹å¾æ•°
        if self.max_features == 'sqrt':
            n_features_split = max(1, int(np.sqrt(n_features)))
        elif self.max_features == 'log2':
            n_features_split = max(1, int(np.log2(n_features)))
        else:
            n_features_split = self.max_features

        # ç´¯ç§¯ç‰¹å¾é‡è¦æ€§
        feature_importance = np.zeros(n_features)

        # è®­ç»ƒ B æ£µæ ‘
        for b in range(self.n_estimators):
            # 1ï¸âƒ£ Bootstrap æœ‰æ”¾å›é‡‡æ ·
            indices = np.random.choice(n_samples, n_samples, replace=True)
            X_boot, y_boot = X[indices], y[indices]

            # 2ï¸âƒ£ è®­ç»ƒä¸€æ£µå†³ç­–æ ‘ï¼ˆé€’å½’åˆ†è£‚ï¼‰
            tree = self._build_tree(X_boot, y_boot, depth=0, n_features_split=n_features_split)
            self.trees.append(tree)

            # æ›´æ–°ç‰¹å¾é‡è¦æ€§
            feature_importance += tree['importance']

        self.feature_importances_ = feature_importance / self.n_estimators
        return self

    def _build_tree(self, X, y, depth, n_features_split):
        """é€’å½’æ„å»ºå•æ£µå†³ç­–æ ‘"""
        n_samples, n_features = X.shape

        # åœæ­¢æ¡ä»¶
        if depth >= self.max_depth or len(np.unique(y)) == 1:
            return {
                'type': 'leaf',
                'value': np.mean(y),
                'importance': np.zeros(n_features)
            }

        # ä» n_features_split ä¸ªéšæœºç‰¹å¾ä¸­æ‰¾æœ€ä¼˜åˆ†è£‚
        best_gain = -np.inf
        best_feature = None
        best_threshold = None

        # éšæœºé€‰æ‹© n_features_split ä¸ªç‰¹å¾
        feature_indices = np.random.choice(n_features, n_features_split, replace=False)

        for feat_idx in feature_indices:
            # å°è¯•è¯¥ç‰¹å¾çš„æ‰€æœ‰é˜ˆå€¼
            thresholds = np.unique(X[:, feat_idx])
            for threshold in thresholds:
                # è®¡ç®— Gini å¢ç›Š
                left_mask = X[:, feat_idx] <= threshold
                right_mask = ~left_mask

                if left_mask.sum() == 0 or right_mask.sum() == 0:
                    continue

                # Gini ä¸çº¯åº¦
                gini_parent = self._gini(y)
                gini_left = self._gini(y[left_mask])
                gini_right = self._gini(y[right_mask])

                # åŠ æƒä¿¡æ¯å¢ç›Š
                gain = gini_parent - (left_mask.sum() / n_samples * gini_left +
                                      right_mask.sum() / n_samples * gini_right)

                if gain > best_gain:
                    best_gain = gain
                    best_feature = feat_idx
                    best_threshold = threshold

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥½çš„åˆ†è£‚ï¼Œè¿”å›å¶å­
        if best_feature is None:
            return {
                'type': 'leaf',
                'value': np.mean(y),
                'importance': np.zeros(n_features)
            }

        # é€’å½’åˆ†è£‚
        left_mask = X[:, best_feature] <= best_threshold
        right_mask = ~left_mask

        left_tree = self._build_tree(X[left_mask], y[left_mask], depth + 1, n_features_split)
        right_tree = self._build_tree(X[right_mask], y[right_mask], depth + 1, n_features_split)

        # ç´¯ç§¯ç‰¹å¾é‡è¦æ€§
        importance = np.zeros(n_features)
        importance[best_feature] = best_gain

        return {
            'type': 'node',
            'feature': best_feature,
            'threshold': best_threshold,
            'left': left_tree,
            'right': right_tree,
            'importance': importance + left_tree['importance'] + right_tree['importance']
        }

    def predict(self, X):
        """é¢„æµ‹"""
        predictions = np.array([self._predict_tree(tree, X) for tree in self.trees])
        return predictions.mean(axis=0)  # å›å½’ï¼šå–å¹³å‡

    def _predict_tree(self, tree, X):
        """å•æ£µæ ‘çš„é¢„æµ‹"""
        if tree['type'] == 'leaf':
            return np.full(X.shape[0], tree['value'])

        left_mask = X[:, tree['feature']] <= tree['threshold']
        result = np.zeros(X.shape[0])

        result[left_mask] = self._predict_tree(tree['left'], X[left_mask])
        result[~left_mask] = self._predict_tree(tree['right'], X[~left_mask])

        return result

    def _gini(self, y):
        """è®¡ç®— Gini ä¸çº¯åº¦"""
        _, counts = np.unique(y, return_counts=True)
        proportions = counts / len(y)
        return 1 - np.sum(proportions ** 2)
```

### Scikit-learn å®æˆ˜ä»£ç 

```python
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.datasets import load_iris, load_wine
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt

# ===== åˆ†ç±»ç¤ºä¾‹ =====
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# å…³é”®å‚æ•°è¯´æ˜
clf = RandomForestClassifier(
    n_estimators=100,          # æ ‘çš„æ•°é‡ï¼šé€šå¸¸ 50-200 è¶³å¤Ÿï¼Œæ”¶ç›Šé€’å‡æ˜æ˜¾
    max_depth=None,            # æ ‘çš„æœ€å¤§æ·±åº¦ï¼šNone è¡¨ç¤ºå®Œå…¨å±•å¼€ï¼Œç”¨ min_samples_leaf æ§åˆ¶
    min_samples_split=2,       # åˆ†è£‚èŠ‚ç‚¹çš„æœ€å°‘æ ·æœ¬æ•°ï¼šå¢å¤§å¯å‡å°‘è¿‡æ‹Ÿåˆ
    min_samples_leaf=1,        # å¶èŠ‚ç‚¹çš„æœ€å°‘æ ·æœ¬æ•°ï¼šé‡è¦çš„è¿‡æ‹Ÿåˆæ§åˆ¶å‚æ•°
    max_features='sqrt',       # åˆ†è£‚æ—¶é‡‡æ ·çš„ç‰¹å¾æ•°ï¼š'sqrt' æœ€å¸¸ç”¨ï¼ˆç‰¹å¾éšæœºåŒ–æ ¸å¿ƒï¼‰
    bootstrap=True,            # ä½¿ç”¨ Bootstrap é‡‡æ ·ï¼šå¿…é¡»ä¸º True
    oob_score=True,            # è®¡ç®— OOB è¯„åˆ†ï¼šå¯ç›´æ¥å¾—åˆ°éªŒè¯è¯¯å·®
    n_jobs=-1,                 # å¹¶è¡Œæ•°ï¼š-1 è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨ CPU
    random_state=42
)

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print(f"å‡†ç¡®ç‡ï¼š{accuracy_score(y_test, y_pred):.3f}")
print(f"OOB è¯„åˆ†ï¼ˆæ— éœ€éªŒè¯é›†ï¼‰ï¼š{clf.oob_score_:.3f}")

# è·å–ç‰¹å¾é‡è¦æ€§
feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']
for name, imp in zip(feature_names, clf.feature_importances_):
    print(f"{name}: {imp:.4f}")
```

---

## ğŸ”§ è¶…å‚æ•°è°ƒä¼˜

### å…³é”®å‚æ•°è¯¦è§£

| å‚æ•° | å«ä¹‰ | å¯¹å†³ç­–è¾¹ç•Œçš„å½±å“ | æ¨èèŒƒå›´ |
|---|---|---|---|
| **`n_estimators`** | æ ‘çš„æ•°é‡ | æ›´å¤šæ ‘ â†’ å‡å°æ–¹å·®ï¼ˆé€šè¿‡å¹³å‡å¤šä¸ªç‹¬ç«‹å†³ç­–ï¼‰ï¼Œå†³ç­–è¾¹ç•Œæ›´å¹³æ»‘ï¼›æ”¶ç›Šé€’å‡ï¼Œé€šå¸¸ 100-200 è¶³å¤Ÿ | 50-500 |
| **`max_depth`** | æ ‘çš„æœ€å¤§æ·±åº¦ | è¿‡æ·± â†’ é«˜æ–¹å·®ï¼ˆå¤æ‚å†³ç­–è¾¹ç•Œï¼Œæ˜“è¿‡æ‹Ÿåˆï¼‰ï¼Œè¿‡æµ… â†’ é«˜åå·®ï¼ˆè¾¹ç•Œè¿‡ç®€å•ï¼Œæ¬ æ‹Ÿåˆï¼‰ã€‚å½±å“å•æ£µæ ‘çš„è¡¨è¾¾èƒ½åŠ›ï¼Œè¿›è€Œå½±å“æ£®æ— | None æˆ– 10-20 |
| **`min_samples_leaf`** | å¶èŠ‚ç‚¹æœ€å°‘æ ·æœ¬ | å¢å¤§ â†’ å¹³æ»‘å†³ç­–è¾¹ç•Œï¼Œå‡å°‘å™ªå£°æ•æ„Ÿæ€§ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼›æ˜¯è¿‡æ‹Ÿåˆæ§åˆ¶çš„é¦–è¦å‚æ•° | 1-5ï¼ˆé»˜è®¤ 1ï¼‰ |
| **`min_samples_split`** | åˆ†è£‚èŠ‚ç‚¹æœ€å°‘æ ·æœ¬ | ä¸ `min_samples_leaf` ç±»ä¼¼æ•ˆæœï¼Œä½†ä½œç”¨ç‚¹æ›´æ—©ï¼›é€šå¸¸æ¯” `min_samples_leaf` çš„æ•ˆæœå¼± | 2-10 |
| **`max_features`** | åˆ†è£‚æ—¶é‡‡æ ·ç‰¹å¾æ•° | **å…³é”®ï¼** æ§åˆ¶æ ‘é—´ç›¸å…³æ€§ $\rho$ã€‚'sqrt' å¾ˆå° â†’ æ ‘å·®å¼‚å¤§ï¼Œ$\rho$ å°ï¼Œæ–¹å·®é™ä½æ˜æ˜¾ï¼›'log2' æˆ–å…¨éƒ¨ â†’ æ ‘ç›¸ä¼¼ï¼Œ$\rho$ å¤§ï¼Œæ–¹å·®é™ä½æœ‰é™ | 'sqrt' âœ…ï¼ˆæ¨èï¼‰ |
| **`bootstrap`** | Bootstrap é‡‡æ · | å¿…é¡»ä¸º Trueã€‚False æ„å‘³ç€æ—  Baggingï¼Œå®Œå…¨å¤±å»æ–¹å·®é™ä½èƒ½åŠ› | True âœ… |

> [!TIP] max_features å¯¹æ–¹å·®çš„å½±å“
>
> å›é¡¾æ–¹å·®å…¬å¼ï¼š$\text{Var}(\bar{T}) = \frac{\rho \sigma^2}{B} + (1-\rho)\sigma^2$
>
> - `max_features='sqrt'`ï¼ˆå°ï¼‰â†’ ç‰¹å¾éšæœºåŒ–å¼º â†’ $\rho$ å° â†’ ç¬¬ä¸€é¡¹ $\frac{\rho \sigma^2}{B}$ å ä¸»å¯¼ â†’ **æ–¹å·®éš $B$ å¿«é€Ÿä¸‹é™**
> - `max_features=None`ï¼ˆå…¨éƒ¨ï¼‰â†’ ç‰¹å¾éšæœºåŒ–å¼± â†’ $\rho$ æ¥è¿‘ 1 â†’ ç¬¬äºŒé¡¹ $(1-\rho)\sigma^2 \approx 0$ï¼Œæ–¹å·®æ¥è¿‘ $\sigma^2$ â†’ **å¢åŠ æ ‘æ— æ³•æ˜¾è‘—å‡å°æ–¹å·®**
>
> è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆ 'sqrt' é€šå¸¸ä¼˜äºå…¨éƒ¨ç‰¹å¾ã€‚

### è°ƒä¼˜å®è·µå»ºè®®

```python
from sklearn.model_selection import GridSearchCV

# æ¨èçš„ä¸¤é˜¶æ®µè°ƒä¼˜
param_grid = {
    'max_depth': [5, 10, 15, None],           # ç¬¬ä¸€é˜¶æ®µï¼šæ ‘æ·±åº¦
    'min_samples_leaf': [1, 2, 5, 10],        # ç¬¬äºŒé˜¶æ®µï¼šå¶å­å¤§å°
    'max_features': ['sqrt', 'log2'],         # å§‹ç»ˆæµ‹è¯•
    'n_estimators': [100, 200]                # æœ€åå¾®è°ƒ
}

grid_search = GridSearchCV(
    RandomForestClassifier(),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid_search.fit(X_train, y_train)
print(f"æœ€ä½³å‚æ•°ï¼š{grid_search.best_params_}")
```

> [!WARNING] å¸¸è§é™·é˜±
>
> 1. **ç›²ç›®å¢åŠ  `n_estimators`**ï¼šæ”¶ç›Šé€’å‡æ˜æ˜¾ï¼Œ100-200 é€šå¸¸å·²è¶³å¤Ÿã€‚ç”¨ OOB è¯¯å·®æ£€æµ‹æ‹ç‚¹è€Œéç›²ç›®å åŠ 
> 2. **`max_depth` è¿‡å¤§**ï¼šå•æ£µæ ‘è¿‡å¤æ‚ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆã€‚ä¼˜å…ˆç”¨ `min_samples_leaf` æ§åˆ¶è€Œé `max_depth`
> 3. **å¿½è§† `max_features`**ï¼šè¿™æ˜¯**ç‰¹å¾éšæœºåŒ–çš„æ ¸å¿ƒ**ï¼Œç›´æ¥æ§åˆ¶æ–¹å·®é™ä½çš„æœ‰æ•ˆæ€§ã€‚'sqrt' é€šå¸¸æœ€ä½³
> 4. **ä¸ç”¨ OOB è¯„åˆ†**ï¼šéšæœºæ£®æ—è‡ªå¸¦ 36.8% OOB æ ·æœ¬ï¼Œè®¾ `oob_score=True` å¯çœå»éªŒè¯é›†ï¼Œæé«˜æ ·æœ¬åˆ©ç”¨ç‡

---

## âš–ï¸ ä¼˜ç¼ºç‚¹ä¸åœºæ™¯

### âœ… ä¼˜åŠ¿ (Pros)

1. **æ³›åŒ–èƒ½åŠ›å¼º**ï¼šé€šè¿‡ Bagging å’ŒæŠ•ç¥¨æœºåˆ¶ï¼Œæ–¹å·®æ˜¾è‘—é™ä½
2. **è‡ªåŠ¨ç‰¹å¾é€‰æ‹©**ï¼šç‰¹å¾é‡è¦æ€§ï¼ˆFeature Importanceï¼‰å¯ä»¥è¯†åˆ«å…³é”®ç‰¹å¾
3. **å¤„ç†æ··åˆç‰¹å¾**ï¼šèƒ½è‡ªç„¶å¤„ç†æ•°å€¼å’Œåˆ†ç±»ç‰¹å¾ï¼ˆé…åˆé¢„å¤„ç†ï¼‰
4. **æ˜“äºå¹¶è¡ŒåŒ–**ï¼šæ ‘ä¹‹é—´ç‹¬ç«‹ï¼Œå¯è½»æ¾åˆ†å¸ƒå¼è®¡ç®—
5. **ä¸éœ€è¦ç‰¹å¾æ ‡å‡†åŒ–**ï¼šæ ‘æ¨¡å‹å¯¹ç‰¹å¾å°ºåº¦ä¸æ•æ„Ÿ
6. **OOB è¯¯å·®**ï¼šå¯ç”¨ Bootstrap çš„ OOB æ ·æœ¬è¿›è¡Œæ— åè¯„ä¼°ï¼Œæ— éœ€é¢å¤–éªŒè¯é›†
7. **é²æ£’æ€§å¥½**ï¼šå¯¹å¼‚å¸¸å€¼å’Œå™ªå£°ç›¸å¯¹ä¸æ•æ„Ÿï¼ˆå¤šæ•°æŠ•ç¥¨æœºåˆ¶ï¼‰

### âŒ åŠ£åŠ¿ (Cons)

1. **å¯è§£é‡Šæ€§å·®**ï¼šå¤§é‡æ ‘ç»„åˆï¼Œéš¾ä»¥ç†è§£å•ä¸ªå†³ç­–è§„åˆ™
2. **è®­ç»ƒé€Ÿåº¦æ…¢**ï¼šéœ€è¦è®­ç»ƒå¤šæ£µæ ‘ï¼Œè™½æ”¯æŒå¹¶è¡Œä½†ä»æ—¶é—´æ¶ˆè€—å¤§
3. **å†…å­˜å ç”¨å¤§**ï¼š$B$ æ£µæ ‘ï¼Œæ¯æ£µéƒ½æ˜¯å®Œæ•´çš„å†³ç­–æ ‘æ¨¡å‹
4. **é¢„æµ‹é€Ÿåº¦**ï¼šè™½æ¯”å•æ ‘æ…¢ï¼Œä½†ä»å¯æ¥å—ï¼ˆæ”¯æŒå¹¶è¡Œé¢„æµ‹ï¼‰
5. **å¯¹å›å½’ä»»åŠ¡æ•ˆæœä¸€èˆ¬**ï¼šç›¸æ¯”æ¢¯åº¦æå‡ï¼ˆGBDTã€XGBoostï¼‰ï¼Œåå·®é™ä½æœ‰é™

### ğŸ¯ æ€æ‰‹é” (Killer Feature)

**Kaggle ç»“æ„åŒ–è¡¨æ ¼æ•°æ®ç«èµ›çš„é¦–é€‰æ–¹æ¡ˆ**ï¼ˆå°¤å…¶æ˜¯ä¸­ç­‰å¤§å°æ•°æ®é›† < 100GBï¼‰ã€‚ç‰¹å¾ä¸éœ€æ ‡å‡†åŒ–ï¼Œè¶…å‚æ•°è°ƒä¼˜ç©ºé—´çµæ´»ï¼Œæ¨¡å‹ç¨³å®šæ€§å¥½ï¼Œæ˜¯å¿«é€Ÿ Baseline å’Œ Ensemble çš„æ ¸å¿ƒç»„ä»¶ã€‚

### å…¸å‹åº”ç”¨åœºæ™¯

| åœºæ™¯ | é€‚ç”¨åº¦ | åŸå›  |
|---|---|---|
| åˆ†ç±»ï¼ˆäºŒåˆ†ç±»/å¤šåˆ†ç±»ï¼‰ | â­â­â­â­â­ | æŠ•ç¥¨æœºåˆ¶å¤©ç„¶ï¼Œæ³›åŒ–å¼º |
| å›å½’ | â­â­â­â­ | å¯ç”¨ï¼Œä½†ä¸å¦‚ GBDT |
| ç‰¹å¾å·¥ç¨‹ | â­â­â­â­â­ | ç‰¹å¾é‡è¦æ€§æ’å |
| å¤§è§„æ¨¡æ•°æ®ï¼ˆ>100M è¡Œï¼‰ | â­â­ | è®­ç»ƒæ…¢ï¼Œè€ƒè™‘ LightGBM |
| é«˜ç»´æ•°æ®ï¼ˆp > 10000ï¼‰ | â­â­â­ | ç‰¹å¾é‡‡æ ·æœ‰å¸®åŠ©ï¼Œä½†éœ€è°ƒæ•´ |
| å®æ—¶é¢„æµ‹ | â­â­â­ | å¯å¹¶è¡Œï¼Œä½†å»¶è¿Ÿå¯èƒ½é«˜ |
| ä¸å¹³è¡¡åˆ†ç±» | â­â­â­ | å¯é…åˆ `class_weight` æˆ–é‡‡æ · |

---

## ğŸ’¬ é¢è¯•å¿…é—®

> [!question] Q1: æ¨å¯¼éšæœºæ£®æ—çš„æ–¹å·®é™ä½å…¬å¼ï¼Œå¹¶è§£é‡Šä¸ºä»€ä¹ˆç‰¹å¾éšæœºåŒ–è‡³å…³é‡è¦ã€‚
>
> **ç­”æ¡ˆæ¡†æ¶**ï¼š
>
> å‡è®¾ $B$ æ£µæ ‘ç›¸äº’ç‹¬ç«‹ï¼Œæ¯æ£µæ ‘æ–¹å·®ä¸º $\sigma^2$ï¼Œé¢„æµ‹å¹³å‡å€¼ä¸ºï¼š
>
> $$\bar{T} = \frac{1}{B}\sum_{b=1}^{B} T_b$$
>
> è‹¥å®Œå…¨ç‹¬ç«‹ï¼š
> $$\text{Var}(\bar{T}) = \frac{\sigma^2}{B}$$
>
> ä½†å®é™…ä¸Šæ ‘é—´ç›¸å…³æ€§ä¸º $\rho$ï¼ˆå…±äº«ç›¸åŒæ•°æ®åˆ†å¸ƒï¼‰ï¼Œä¿®æ­£ä¸ºï¼š
>
> $$\text{Var}(\bar{T}) = \frac{\rho \sigma^2}{B} + (1-\rho)\sigma^2 = \rho\sigma^2\left(\frac{1}{B} - 1\right) + \sigma^2$$
>
> **å…³é”®åˆ†æ**ï¼š
> - å½“ $\rho = 1$ï¼ˆæ‰€æœ‰æ ‘å®Œå…¨ä¸€è‡´ï¼‰ï¼šæ–¹å·® = $\sigma^2$ï¼Œæ— æ³•é™ä½
> - å½“ $\rho = 0$ï¼ˆå®Œå…¨ç‹¬ç«‹ï¼‰ï¼šæ–¹å·® = $\sigma^2/B$ï¼Œæœ€ä¼˜
> - ç‰¹å¾éšæœºåŒ–çš„ä½œç”¨ï¼šé€šè¿‡é™åˆ¶ç‰¹å¾é€‰æ‹©èŒƒå›´ï¼ˆå¦‚ 'sqrt'ï¼‰ï¼Œå¼ºåˆ¶æ ‘é€‰æ‹©ä¸åŒç‰¹å¾ï¼Œå‡å° $\rho$ï¼Œä½¿ç¬¬ä¸€é¡¹ä¸»å¯¼ï¼Œæ–¹å·®éš $B$ å¿«é€Ÿä¸‹é™
>
> **ç›´è§‚ç±»æ¯”**ï¼šå¦‚æœ $B$ ä¸ªæŠ•èµ„é¡¾é—®ç»™å‡ºå®Œå…¨ç›¸åŒçš„å»ºè®®ï¼ˆ$\rho=1$ï¼‰ï¼Œå†å¤šçš„é¡¾é—®ä¹Ÿæ— ç”¨ï¼›ä½†å¦‚æœä»–ä»¬ç‹¬ç«‹æ€è€ƒï¼ˆ$\rho$ å°ï¼‰ï¼Œè¶Šå¤šé¡¾é—®è¶Šå‡†ç¡®ã€‚

> [!question] Q2: ä¸ºä»€ä¹ˆå†³ç­–æ ‘çš„ Gini åˆ†è£‚å‡†åˆ™å¯¹éšæœºæ£®æ—å¾ˆé‡è¦ï¼Ÿèƒ½å¦ä»ä¿¡æ¯è®ºè§’åº¦è§£é‡Šï¼Ÿ
>
> **ç­”æ¡ˆæ ¸å¿ƒ**ï¼š
>
> Gini ä¸çº¯åº¦å®šä¹‰ï¼š$\text{Gini}(D) = 1 - \sum_{c=1}^{K} p_c^2$
>
> é€‰æ‹©åˆ†è£‚æ—¶æœ€å¤§åŒ–å¢ç›Šï¼š
> $$\text{Gain} = \text{Gini}(D) - \sum_v \frac{|D_v|}{|D|} \text{Gini}(D_v)$$
>
> **ä¸ä¿¡æ¯è®ºçš„å…³ç³»**ï¼š
> - Gini åæ˜ "æ‚åº¦"ï¼ˆheterogeneityï¼‰ï¼Œç­‰ä»·äº 1 - æ‹‰æ™®æ‹‰æ–¯ç†µ
> - æœ€å¤§åŒ– Gain = æœ€å°åŒ–åŠ æƒå­é›†çš„åŠ æƒ Gini = è®©åˆ†è£‚åçš„æ ·æœ¬å°½å¯èƒ½çº¯ï¼ˆåŒä¸€ç±»åˆ«å±…å¤šï¼‰
> - è¿™æ˜¯**è´ªå¿ƒçš„æ ‘æ„é€ ç­–ç•¥**ï¼Œæ¯æ­¥é€‰æœ€ä¼˜åˆ†è£‚ï¼Œä½†ä¸ä¿è¯å…¨å±€æœ€ä¼˜ï¼ˆNP-hardï¼‰
>
> **éšæœºæ£®æ—çš„ä¼˜åŒ–**ï¼šè™½ç„¶å•æ£µæ ‘è´ªå¿ƒæ„é€ ï¼Œä½†é€šè¿‡ç‰¹å¾éšæœºåŒ–ï¼ˆå‡å¼±è´ªå¿ƒï¼‰ï¼Œå¤šæ£µæ ‘æŠ•ç¥¨ï¼ˆé›†æˆï¼‰ï¼Œåè€Œè·å¾—æ›´å¥½æ³›åŒ–ã€‚

> [!question] Q3: å¦‚ä½•è§£å†³éšæœºæ£®æ—çš„é«˜åå·®é—®é¢˜ï¼Ÿä¸ºä»€ä¹ˆ XGBoost åœ¨æŸäº›åœºæ™¯ä¼˜äºéšæœºæ£®æ—ï¼Ÿ
>
> **ç­”æ¡ˆæ ¸å¿ƒ**ï¼š
>
> **éšæœºæ£®æ—çš„åå·®åŠ£åŠ¿**ï¼š
> - å›å½’æ—¶ï¼Œä½¿ç”¨ç®€å•å¹³å‡ $\hat{y} = \frac{1}{B}\sum T_b(x)$ï¼Œæ¯æ£µæ ‘çš„è¯¯å·®å¹³ç­‰åŠ æƒ
> - åœ¨**å›å½’ä»»åŠ¡**ä¸­ï¼Œè‹¥æ•°æ®æœ‰æ˜æ˜¾çš„åå·®æˆåˆ†ï¼ˆå¦‚å…¨å±€è¶‹åŠ¿ï¼‰ï¼Œç‹¬ç«‹æ ‘æ— æ³•æœ‰æ•ˆä¿®æ­£è¯¯å·®
>
> **XGBoost çš„ä¼˜åŠ¿**ï¼ˆBoosting vs Baggingï¼‰ï¼š
> - é‡‡ç”¨**æ®‹å·®å­¦ä¹ **ï¼šç¬¬ $(t+1)$ æ£µæ ‘ä¸“æ³¨æ‹Ÿåˆå‰ $t$ æ£µæ ‘çš„é¢„æµ‹æ®‹å·®
> - åŠ æƒæ±‡æ€»ï¼š$\hat{y} = \sum_{b=1}^{B} \eta T_b(x)$ï¼Œåç»­æ ‘æƒé‡æ›´é«˜ï¼ˆé€šè¿‡å­¦ä¹ ç‡ $\eta$ æ§åˆ¶ï¼‰
> - ç»“æœï¼š**åå·®å’Œæ–¹å·®åŒæ—¶é™ä½**ï¼Œæ¯” Bagging æ›´é«˜æ•ˆ
>
> **ä½•æ—¶é€‰éšæœºæ£®æ— vs XGBoost**ï¼š
> | åœºæ™¯ | é€‰æ‹© | åŸå›  |
> |---|---|---|
> | åˆ†ç±»ä»»åŠ¡ | éƒ½å¯ä»¥ï¼Œå€¾å‘ RF | RF æŠ•ç¥¨æœºåˆ¶å¯¹åˆ†ç±»å¤©ç„¶ï¼ŒXGBoost éœ€è°ƒå‚æ•° |
> | å°æ•°æ®é›†ï¼ˆ<10Kï¼‰ | RF | ä¸æ˜“è¿‡æ‹Ÿåˆï¼Œè°ƒå‚ç®€å• |
> | å¤§æ•°æ®é›†ï¼ˆ>100Kï¼‰ + å¤æ‚å…³ç³» | XGBoost | åå·®æ›´å°ï¼Œæ›´é«˜çš„ç²¾åº¦ä¸Šé™ |
> | éœ€è¦å¿«é€Ÿ Baseline | RF | è°ƒå‚ç©ºé—´å°ï¼Œé»˜è®¤å‚æ•°å°±ä¸é”™ |
> | Kaggle ç«èµ› | XGBoost/LightGBM | ç²¾åº¦æœ€é«˜ï¼ŒEnsemble æ•ˆæœå¥½ |

---

## ğŸ“Š å†³ç­–è¾¹ç•Œå¯è§†åŒ–ä¸ç™½ç›’åŒ–ç†è§£

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_moons

# ç”Ÿæˆéçº¿æ€§åˆ†ç±»æ•°æ®
X, y = make_moons(n_samples=300, noise=0.15, random_state=42)
X_train, X_test, y_train, y_test = X[:250], X[250:], y[:250], y[250:]

# å®šä¹‰ç½‘æ ¼ç”¨äºç»˜åˆ¶å†³ç­–è¾¹ç•Œ
h = 0.02
x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# è®­ç»ƒæ¨¡å‹
single_tree = DecisionTreeClassifier(max_depth=5, random_state=42)
single_tree.fit(X_train, y_train)

rf = RandomForestClassifier(n_estimators=100, max_depth=5, max_features='sqrt', random_state=42)
rf.fit(X_train, y_train)

# è·å–å†³ç­–è¾¹ç•Œ
Z_tree = single_tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
Z_rf = rf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

# ç»˜å›¾
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# å•æ£µæ ‘
axes[0].contourf(xx, yy, Z_tree, alpha=0.4, cmap='RdYlBu')
axes[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdYlBu', edgecolor='black')
axes[0].set_title('å•æ£µå†³ç­–æ ‘\nï¼ˆé«˜æ–¹å·®ï¼Œå†³ç­–è¾¹ç•Œå¤æ‚ï¼‰')
axes[0].set_xlabel('Feature 1')
axes[0].set_ylabel('Feature 2')

# éšæœºæ£®æ—
axes[1].contourf(xx, yy, Z_rf, alpha=0.4, cmap='RdYlBu')
axes[1].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdYlBu', edgecolor='black')
axes[1].set_title('éšæœºæ£®æ— (100æ£µæ ‘)\nï¼ˆä½æ–¹å·®ï¼Œå†³ç­–è¾¹ç•Œå¹³æ»‘ï¼‰')
axes[1].set_xlabel('Feature 1')
axes[1].set_ylabel('Feature 2')

plt.tight_layout()
plt.show()

# ç²¾åº¦å¯¹æ¯”
print(f"å•æ£µæ ‘ç²¾åº¦ï¼šè®­ç»ƒ {single_tree.score(X_train, y_train):.3f}ï¼Œæµ‹è¯• {single_tree.score(X_test, y_test):.3f}")
print(f"éšæœºæ£®æ—ç²¾åº¦ï¼šè®­ç»ƒ {rf.score(X_train, y_train):.3f}ï¼Œæµ‹è¯• {rf.score(X_test, y_test):.3f}")
```

**ç›´è§‚è§‚å¯Ÿ**ï¼š
- **å·¦å›¾ï¼ˆå•æ£µæ ‘ï¼‰**ï¼šå†³ç­–è¾¹ç•Œæ˜¯è½´å¯¹é½çš„çŸ©å½¢ç»„åˆï¼Œè¿‡äºå¤æ‚ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆè®­ç»ƒé«˜ï¼Œæµ‹è¯•ä½ï¼‰
- **å³å›¾ï¼ˆéšæœºæ£®æ—ï¼‰**ï¼šå¤šæ£µæ ‘é€šè¿‡æŠ•ç¥¨å¹³æ»‘äº†è¾¹ç•Œï¼Œè™½ç„¶è®­ç»ƒè¯¯å·®ç•¥é«˜ï¼Œä½†æ³›åŒ–æ›´å¥½ï¼ˆæµ‹è¯•è¯¯å·®æ›´ä½ï¼‰

è¿™æ­£æ˜¯**åå·®-æ–¹å·®æƒè¡¡**çš„ä½“ç°ï¼šå•æ£µæ ‘ä½åå·®ä½†é«˜æ–¹å·®ï¼Œéšæœºæ£®æ—é«˜åå·®ä½†ä½æ–¹å·®ï¼Œæ€»ä½“æ³›åŒ–æ›´ä¼˜ã€‚

---

## ğŸ“š æ‹“å±•é˜…è¯»

- **å¯¹æ ‡ç®—æ³•**ï¼š
  - æ¢¯åº¦æå‡æ—ï¼ˆGBDTã€XGBoostã€LightGBMï¼‰ï¼šBoosting è€Œé Baggingï¼Œåå·®æ›´å°ä½†è®­ç»ƒæ…¢
  - ExtraTreesClassifierï¼šåˆ†è£‚é˜ˆå€¼ä¹Ÿéšæœºï¼Œç‰¹å¾éšæœºåŒ–æ›´å¼º

- **å˜ä½“ä¸åº”ç”¨**ï¼š
  - IsolationForestï¼šåŸºäºéšæœºæ£®æ—çš„å¼‚å¸¸æ£€æµ‹
  - RandomTreesEmbeddingï¼šç‰¹å¾æå–ï¼ˆå°†å¶èŠ‚ç‚¹ç¼–ç ä¸ºç‰¹å¾ï¼‰

- **ç†è®ºä¸è®ºæ–‡**ï¼š
  - Leo Breimanã€ŠRandom Forestsã€‹ï¼ˆ2001ï¼‰ï¼šå¥ åŸºæ€§è®ºæ–‡
  - Geurts et al.ã€ŠExtremely Randomized Treesã€‹ï¼ˆ2006ï¼‰ï¼šæç«¯éšæœºæ£®æ—

- **ä¸ XGBoost çš„å¯¹æ ‡**ï¼šå‚è€ƒé¢è¯• Q3 ä¸­çš„è¯¦ç»†å¯¹æ¯”
