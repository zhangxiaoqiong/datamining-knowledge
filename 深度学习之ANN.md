---
tags: [ç®—æ³•, æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, ç›‘ç£å­¦ä¹ , æ¢¯åº¦ä¸‹é™]
math: true
difficulty: å›°éš¾
---

# äººå·¥ç¥ç»ç½‘ç»œ (Artificial Neural Network, ANN)

## ğŸ’¡ æ ¸å¿ƒç›´è§‰

- **ä¸€å¥è¯å®šä¹‰**ï¼šé€šè¿‡å¤šå±‚ç¥ç»å…ƒçš„éçº¿æ€§ç»„åˆï¼Œå­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„å¤æ‚æ˜ å°„å‡½æ•° $f(x)$ çš„é€šç”¨è¿‘ä¼¼å™¨ã€‚

- **è§£å†³é—®é¢˜**ï¼šè§£å†³äº†çº¿æ€§æ¨¡å‹ï¼ˆLRã€SVMï¼‰æ— æ³•æ•æ‰æ•°æ®ä¸­å¤æ‚éçº¿æ€§å…³ç³»çš„é—®é¢˜ã€‚å•å±‚æ„ŸçŸ¥æœºæ— æ³•è¡¨ç¤ºå¼‚æˆ–(XOR)ï¼Œå¤šå±‚ç½‘ç»œå¯ä»¥ã€‚

- **æ ¸å¿ƒé€»è¾‘**ï¼šANN = è¾“å…¥å±‚ â†’ éšå±‚ï¼ˆå¤šä¸ªï¼‰â†’ è¾“å‡ºå±‚ï¼Œé€šè¿‡**åå‘ä¼ æ’­**ä¼˜åŒ–å‚æ•°ï¼Œä½¿é¢„æµ‹é€¼è¿‘çœŸå®å€¼ã€‚

- **å‡ ä½•æ„ä¹‰**ï¼šæ¯ä¸€å±‚çš„éçº¿æ€§æ¿€æ´»å‡½æ•°éƒ½æ˜¯ä¸€æ¬¡**ç‰¹å¾ç©ºé—´çš„éçº¿æ€§å˜æ¢**ï¼Œæ·±å±‚ç½‘ç»œé€šè¿‡å åŠ å¤šä¸ªè¿™æ ·çš„å˜æ¢ï¼Œå°†åŸå§‹ç‰¹å¾ç©ºé—´æ‰­æ›²ã€æŠ˜å ã€åˆ†å‰²ï¼Œæœ€ç»ˆåœ¨é«˜ç»´ç‰¹å¾ç©ºé—´ä¸­çº¿æ€§å¯åˆ†ã€‚

- **æ€æ‰‹é” (Killer Feature)**ï¼šé€šç”¨è¿‘ä¼¼å®šç†ï¼ˆUniversal Approximation Theoremï¼‰ä¿è¯è¶³å¤Ÿå®½çš„å•éšå±‚ç½‘ç»œå¯ä»¥è¿‘ä¼¼ä»»æ„è¿ç»­å‡½æ•°ã€‚å®é™…åº”ç”¨ä¸­ï¼Œæ·±ç½‘ç»œï¼ˆæ·±å±‚ï¼‰æ¯”å®½ç½‘ç»œæ›´é«˜æ•ˆï¼ˆæ ·æœ¬å¤æ‚åº¦ä½ï¼‰ã€‚

> [!TIP] æ ¸å¿ƒæ¶æ„å›¾è§£
>
> ```
> è¾“å…¥å±‚          éšå±‚1         éšå±‚2         è¾“å‡ºå±‚
>   xâ‚  â”€â”                                    Å·â‚
>        â”œâ”€â”€â†’ hâ‚â½Â¹â¾  â”€â”                    â”Œâ”€â†’ Å·â‚‚
>   xâ‚‚  â”€â”¤            â”œâ”€â”€â†’ hâ‚â½Â²â¾  â”€â”€â†’  Ïƒ  â”¤
>   xâ‚ƒ  â”€â”´â”€â”€â†’ hâ‚‚â½Â¹â¾  â”€â”¤          (è¾“å‡º)    â””â”€â†’ Å·â‚ƒ
>                      â””â”€â”€â†’ hâ‚‚â½Â²â¾
>
> å‰å‘ä¼ æ’­ï¼šè®¡ç®— Å· = Ïƒ(Wâ½Ë¡â¾ hâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾)
> åå‘ä¼ æ’­ï¼šè®¡ç®— âˆ‚L/âˆ‚Wï¼Œâˆ‚L/âˆ‚b ç”¨äºæ¢¯åº¦ä¸‹é™
> æ ¸å¿ƒæœºåˆ¶ï¼šæ¯å±‚çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆReLUã€sigmoidï¼‰æ‰“ç ´çº¿æ€§æ€§
> ```

---

## ğŸ“ æ•°å­¦åŸç†

### 1. å‰å‘ä¼ æ’­ (Forward Propagation)

å¯¹äº $L$ å±‚ç½‘ç»œï¼Œç¬¬ $l$ å±‚çš„è®¡ç®—ä¸ºï¼š

**çº¿æ€§å˜æ¢**ï¼š
$$z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$$

**éçº¿æ€§æ¿€æ´»**ï¼š
$$a^{(l)} = \sigma(z^{(l)})$$

å…¶ä¸­ï¼š
- $a^{(l)}$ï¼šç¬¬ $l$ å±‚çš„æ¿€æ´»å‘é‡ï¼ˆè¾“å‡ºï¼‰ï¼Œ$a^{(0)} = x$ï¼ˆè¾“å…¥ï¼‰
- $W^{(l)}$ï¼šæƒé‡çŸ©é˜µï¼Œå½¢çŠ¶ $(n^{(l)}, n^{(l-1)})$
- $b^{(l)}$ï¼šåç½®å‘é‡ï¼Œå½¢çŠ¶ $(n^{(l)}, 1)$
- $\sigma$ï¼šæ¿€æ´»å‡½æ•°ï¼ˆReLUã€sigmoidã€tanhç­‰ï¼‰
- $z^{(l)}$ï¼šçº¿æ€§ç»„åˆç»“æœï¼ˆæœªæ¿€æ´»ï¼‰

**å®Œæ•´å‰å‘ä¼ æ’­**ï¼šä»è¾“å…¥é€’æ¨åˆ°è¾“å‡º
$$\hat{y} = a^{(L)} = \sigma^{(L)}(W^{(L)} \sigma^{(L-1)}(\cdots \sigma^{(1)}(W^{(1)} x + b^{(1)}) \cdots) + b^{(L)})$$

> [!ABSTRACT] æ¿€æ´»å‡½æ•°çš„å¿…è¦æ€§
>
> ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°æ—¶ï¼Œå¤šå±‚ç½‘ç»œé€€åŒ–ä¸ºçº¿æ€§å˜æ¢ï¼š
> $$a^{(L)} = W^{(L)} W^{(L-1)} \cdots W^{(1)} x + \text{(bias terms)}$$
> å…¶ä¸­ $W^{(L)} W^{(L-1)} \cdots W^{(1)}$ ä»æ˜¯çŸ©é˜µï¼Œæ— æ³•è¡¨ç¤ºéçº¿æ€§ã€‚æ¿€æ´»å‡½æ•°å¼•å…¥äº†éçº¿æ€§ï¼Œä½¿æ·±å±‚ç½‘ç»œçœŸæ­£æœ‰è¡¨è¾¾èƒ½åŠ›ã€‚

### 2. æŸå¤±å‡½æ•°ä¸åå‘ä¼ æ’­

**å›å½’ä»»åŠ¡**ï¼ˆMSE æŸå¤±ï¼‰ï¼š
$$L = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2$$

**åˆ†ç±»ä»»åŠ¡**ï¼ˆäº¤å‰ç†µæŸå¤±ï¼‰ï¼š
$$L = -\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} y_{i,k} \log(\hat{y}_{i,k})$$

å…¶ä¸­ï¼š
- $m$ï¼šæ ·æœ¬æ•°
- $K$ï¼šç±»åˆ«æ•°
- $y_{i,k}$ï¼šone-hot ç¼–ç çš„çœŸå®æ ‡ç­¾
- $\hat{y}_{i,k}$ï¼šsoftmax é¢„æµ‹çš„æ¦‚ç‡

### 3. åå‘ä¼ æ’­ (Backpropagation)

åå‘ä¼ æ’­çš„æ ¸å¿ƒæ˜¯**é“¾å¼æ³•åˆ™**ã€‚å¯¹äºç¬¬ $l$ å±‚ï¼Œè®¡ç®—æ¢¯åº¦ï¼š

**è¾“å‡ºå±‚æ¢¯åº¦**ï¼ˆä»¥ MSE ä¸ºä¾‹ï¼‰ï¼š
$$\delta^{(L)} = \frac{\partial L}{\partial z^{(L)}} = (\hat{y} - y) \odot \sigma'(z^{(L)})$$

å…¶ä¸­ï¼š
- $\odot$ï¼šelement-wise ä¹˜ç§¯ï¼ˆHadamardç§¯ï¼‰
- $\sigma'$ï¼šæ¿€æ´»å‡½æ•°çš„å¯¼æ•°

**éšå±‚æ¢¯åº¦**ï¼ˆé“¾å¼æ³•åˆ™é€’æ¨ï¼‰ï¼š
$$\delta^{(l)} = (W^{(l+1)})^T \delta^{(l+1)} \odot \sigma'(z^{(l)})$$

è¿™ä»ç¬¬ $L$ å±‚é€†å‘ä¼ æ’­åˆ°ç¬¬ 1 å±‚ã€‚

**å‚æ•°æ¢¯åº¦**ï¼š
$$\frac{\partial L}{\partial W^{(l)}} = \frac{1}{m} \delta^{(l)} (a^{(l-1)})^T$$

$$\frac{\partial L}{\partial b^{(l)}} = \frac{1}{m} \sum_{i=1}^{m} \delta^{(l)}_i$$

**å‚æ•°æ›´æ–°**ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰ï¼š
$$W^{(l)} \leftarrow W^{(l)} - \eta \frac{\partial L}{\partial W^{(l)}}$$

$$b^{(l)} \leftarrow b^{(l)} - \eta \frac{\partial L}{\partial b^{(l)}}$$

å…¶ä¸­ $\eta$ æ˜¯å­¦ä¹ ç‡ã€‚

> [!TIP] åå‘ä¼ æ’­çš„å‡ ä½•æ„ä¹‰
>
> æ¢¯åº¦ $\nabla L$ æŒ‡å‘æŸå¤±å‡½æ•°å¢åŠ æœ€å¿«çš„æ–¹å‘ã€‚æ¢¯åº¦ä¸‹é™å°±æ˜¯æ²¿ç€ $-\nabla L$ æ–¹å‘èµ°ä¸€æ­¥ $\eta$ï¼Œç›®æ ‡æ˜¯æ‰¾åˆ°å±€éƒ¨æœ€å°å€¼ã€‚
>
> **å…³é”®è®¡ç®—**ï¼šé“¾å¼æ³•åˆ™ $\frac{\partial L}{\partial W^{(l)}} = \frac{\partial L}{\partial z^{(L)}} \cdot \frac{\partial z^{(L)}}{\partial z^{(L-1)}} \cdots \frac{\partial z^{(l+1)}}{\partial z^{(l)}} \cdot \frac{\partial z^{(l)}}{\partial W^{(l)}}$
>
> æ¯ä¸€æ­¥éƒ½åŒ…å«æ¿€æ´»å‡½æ•°å¯¼æ•°ï¼Œè¿™æ˜¯å¯¼è‡´**æ¢¯åº¦æ¶ˆå¤±**çš„æºå¤´ï¼ˆReLU è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼‰ã€‚

### 4. å¸¸ç”¨æ¿€æ´»å‡½æ•°

| æ¿€æ´»å‡½æ•° | å…¬å¼ | å¯¼æ•° | ä¼˜ç¼ºç‚¹ |
|---|---|---|---|
| **Sigmoid** | $\sigma(z) = \frac{1}{1+e^{-z}}$ | $\sigma'(z) = \sigma(z)(1-\sigma(z))$ | è¾“å‡ºèŒƒå›´ [0,1]ï¼Œä½†å¯¼æ•°æœ€å¤§ 0.25ï¼Œæ˜“æ¢¯åº¦æ¶ˆå¤± |
| **Tanh** | $\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$ | $\tanh'(z) = 1 - \tanh^2(z)$ | è¾“å‡ºèŒƒå›´ [-1,1]ï¼Œå¯¼æ•°æœ€å¤§ 1ï¼Œæ¯” sigmoid å¥½ |
| **ReLU** | $\text{ReLU}(z) = \max(0, z)$ | $\text{ReLU}'(z) = \begin{cases}1 & z>0 \\ 0 & z\leq0 \end{cases}$ | è®¡ç®—å¿«ï¼Œå¯¼æ•°æ’ä¸º 0 æˆ– 1ï¼Œè§£å†³æ¢¯åº¦æ¶ˆå¤±ã€‚ä½†æœ‰ dead ReLU é—®é¢˜ |
| **Leaky ReLU** | $\text{LReLU}(z) = \max(\alpha z, z)$ | $\text{LReLU}'(z) = \begin{cases}1 & z>0 \\ \alpha & z\leq0 \end{cases}$ | æ”¹è¿› ReLUï¼Œè´Ÿæ•°æ®µæœ‰å°æ¢¯åº¦ï¼Œé¿å…å®Œå…¨æ­»äº¡ |
| **Softmax** | $\text{softmax}_i(z) = \frac{e^{z_i}}{\sum_j e^{z_j}}$ | - | ä»…ç”¨äºè¾“å‡ºå±‚ï¼ˆå¤šåˆ†ç±»ï¼‰ï¼Œè¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ |

---

## ğŸ’» ç®—æ³•å®ç°

### PyTorch å®Œæ•´å®ç°ï¼ˆ2å±‚ç½‘ç»œï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np

class SimpleNeuralNetwork(nn.Module):
    """PyTorch å®ç°çš„ 2 å±‚ç¥ç»ç½‘ç»œ"""

    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNeuralNetwork, self).__init__()

        # å®šä¹‰ç½‘ç»œå±‚
        self.fc1 = nn.Linear(input_size, hidden_size)  # éšå±‚ï¼ˆè¾“å…¥ â†’ éšå±‚ï¼‰
        self.relu = nn.ReLU()                           # ReLU æ¿€æ´»
        self.fc2 = nn.Linear(hidden_size, output_size) # è¾“å‡ºå±‚

        # He åˆå§‹åŒ–ï¼ˆé€‚é… ReLUï¼‰
        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')
        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu')
        nn.init.zeros_(self.fc1.bias)
        nn.init.zeros_(self.fc2.bias)

    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        z1 = self.fc1(x)           # çº¿æ€§å˜æ¢
        a1 = self.relu(z1)         # éšå±‚æ¿€æ´»
        z2 = self.fc2(a1)          # è¾“å‡ºå±‚çº¿æ€§
        return z2  # è¿”å› logitsï¼Œç”± loss å‡½æ•°å†…éƒ¨åº”ç”¨ softmax

class NeuralNetworkTrainer:
    """è®­ç»ƒå™¨ï¼Œå°è£…è®­ç»ƒå¾ªç¯"""

    def __init__(self, model, learning_rate=0.01, device='cpu'):
        self.model = model.to(device)
        self.device = device
        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        # CrossEntropyLoss å†…éƒ¨åŒ…å« softmax + äº¤å‰ç†µ
        self.criterion = nn.CrossEntropyLoss()

    def train(self, X_train, y_train, epochs=100, batch_size=32, validation_split=0.2):
        """è®­ç»ƒç½‘ç»œ"""
        # è½¬æ¢ä¸º PyTorch å¼ é‡
        X_tensor = torch.FloatTensor(X_train).to(self.device)
        y_tensor = torch.LongTensor(y_train).to(self.device)

        # åˆ›å»ºéªŒè¯é›†
        val_size = int(len(X_train) * validation_split)
        indices = torch.randperm(len(X_train))
        train_indices = indices[val_size:]
        val_indices = indices[:val_size]

        X_train_split, y_train_split = X_tensor[train_indices], y_tensor[train_indices]
        X_val, y_val = X_tensor[val_indices], y_tensor[val_indices]

        # åˆ›å»º DataLoaderï¼ˆè‡ªåŠ¨ shuffle å’Œ batchï¼‰
        train_dataset = TensorDataset(X_train_split, y_train_split)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

        train_losses = []
        val_losses = []

        for epoch in range(epochs):
            # ===== è®­ç»ƒé˜¶æ®µ =====
            self.model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆDropoutã€BatchNorm ä¼šæ¿€æ´»ï¼‰
            epoch_loss = 0
            num_batches = 0

            for X_batch, y_batch in train_loader:
                # å‰å‘ä¼ æ’­
                outputs = self.model(X_batch)

                # è®¡ç®—æŸå¤±
                loss = self.criterion(outputs, y_batch)

                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()  # æ¸…ç©ºä¸Šä¸€æ­¥æ¢¯åº¦
                loss.backward()              # è®¡ç®—æ¢¯åº¦ï¼ˆé“¾å¼æ³•åˆ™è‡ªåŠ¨åŒ–ï¼‰
                self.optimizer.step()        # å‚æ•°æ›´æ–°

                epoch_loss += loss.item()
                num_batches += 1

            avg_train_loss = epoch_loss / num_batches
            train_losses.append(avg_train_loss)

            # ===== éªŒè¯é˜¶æ®µ =====
            self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜
                val_outputs = self.model(X_val)
                val_loss = self.criterion(val_outputs, y_val)
                val_losses.append(val_loss.item())

            if (epoch + 1) % 20 == 0:
                print(f"Epoch {epoch+1}/{epochs}, "
                      f"Train Loss: {avg_train_loss:.4f}, "
                      f"Val Loss: {val_loss.item():.4f}")

        return train_losses, val_losses

    def predict(self, X):
        """é¢„æµ‹"""
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)
            outputs = self.model(X_tensor)
            predictions = torch.argmax(outputs, dim=1)
        return predictions.cpu().numpy()

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import accuracy_score

    # åŠ è½½æ•°æ®
    X, y = load_iris(return_X_y=True)
    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    # åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒå™¨
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = SimpleNeuralNetwork(input_size=4, hidden_size=64, output_size=3)
    trainer = NeuralNetworkTrainer(model, learning_rate=0.01, device=device)

    # è®­ç»ƒ
    train_losses, val_losses = trainer.train(
        X_train, y_train, epochs=100, batch_size=16, validation_split=0.2
    )

    # è¯„ä¼°
    y_pred = trainer.predict(X_test)
    test_acc = accuracy_score(y_test, y_pred)
    print(f"\nTest Accuracy: {test_acc:.3f}")
```

### PyTorch è¿›é˜¶å®ç°ï¼ˆå¸¦ Dropout å’Œ BatchNormï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt

class AdvancedNeuralNetwork(nn.Module):
    """æ›´å¤æ‚çš„ç½‘ç»œï¼ŒåŒ…å« Dropout å’Œ BatchNormalization"""

    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.3):
        super(AdvancedNeuralNetwork, self).__init__()

        layers = []
        prev_size = input_size

        # åŠ¨æ€æ„å»ºéšå±‚
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.BatchNorm1d(hidden_size))  # æ‰¹é‡å½’ä¸€åŒ–
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))     # Dropout æ­£åˆ™åŒ–
            prev_size = hidden_size

        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_size, output_size))

        self.network = nn.Sequential(*layers)

        # He åˆå§‹åŒ–
        for module in self.network:
            if isinstance(module, nn.Linear):
                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
                nn.init.zeros_(module.bias)

    def forward(self, x):
        return self.network(x)

# ===== å®Œæ•´è®­ç»ƒç®¡é“ =====
class AdvancedTrainer:
    """æ”¯æŒ Early Stopping çš„è®­ç»ƒå™¨"""

    def __init__(self, model, learning_rate=0.001, device='cpu', patience=20):
        self.model = model.to(device)
        self.device = device
        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        self.criterion = nn.CrossEntropyLoss()
        self.patience = patience  # Early stopping è€å¿ƒå€¼
        self.best_val_loss = float('inf')
        self.patience_counter = 0

    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ª epoch"""
        self.model.train()
        epoch_loss = 0

        for X_batch, y_batch in train_loader:
            outputs = self.model(X_batch)
            loss = self.criterion(outputs, y_batch)

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            epoch_loss += loss.item()

        return epoch_loss / len(train_loader)

    def validate(self, val_loader):
        """éªŒè¯"""
        self.model.eval()
        val_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                outputs = self.model(X_batch)
                loss = self.criterion(outputs, y_batch)
                val_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                correct += (predicted == y_batch).sum().item()
                total += y_batch.size(0)

        return val_loss / len(val_loader), correct / total

    def train(self, X_train, y_train, epochs=200, batch_size=32, validation_split=0.2):
        """è®­ç»ƒï¼Œæ”¯æŒ Early Stopping"""
        # æ•°æ®å‡†å¤‡
        X_tensor = torch.FloatTensor(X_train).to(self.device)
        y_tensor = torch.LongTensor(y_train).to(self.device)

        val_size = int(len(X_train) * validation_split)
        indices = torch.randperm(len(X_train))
        train_indices = indices[val_size:]
        val_indices = indices[:val_size]

        train_dataset = TensorDataset(X_tensor[train_indices], y_tensor[train_indices])
        val_dataset = TensorDataset(X_tensor[val_indices], y_tensor[val_indices])

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

        train_losses = []
        val_losses = []
        val_accs = []

        for epoch in range(epochs):
            train_loss = self.train_epoch(train_loader)
            val_loss, val_acc = self.validate(val_loader)

            train_losses.append(train_loss)
            val_losses.append(val_loss)
            val_accs.append(val_acc)

            # Early Stopping é€»è¾‘
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(self.model.state_dict(), 'best_model.pth')
            else:
                self.patience_counter += 1
                if self.patience_counter >= self.patience:
                    print(f"Early stopping at epoch {epoch+1}")
                    # åŠ è½½æœ€ä½³æ¨¡å‹
                    self.model.load_state_dict(torch.load('best_model.pth'))
                    break

            if (epoch + 1) % 30 == 0:
                print(f"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, "
                      f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.3f}")

        return train_losses, val_losses, val_accs

    def predict(self, X):
        """é¢„æµ‹"""
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)
            outputs = self.model(X_tensor)
            predictions = torch.argmax(outputs, dim=1)
        return predictions.cpu().numpy()

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import accuracy_score, classification_report

    # åŠ è½½æ•°æ®
    X, y = load_iris(return_X_y=True)
    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    # åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒå™¨
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = AdvancedNeuralNetwork(
        input_size=4,
        hidden_sizes=[64, 32],  # 2 ä¸ªéšå±‚
        output_size=3,
        dropout_rate=0.3
    )
    trainer = AdvancedTrainer(model, learning_rate=0.001, device=device, patience=30)

    # è®­ç»ƒ
    train_losses, val_losses, val_accs = trainer.train(
        X_train, y_train, epochs=200, batch_size=16, validation_split=0.2
    )

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss', linewidth=2)
    plt.plot(val_losses, label='Val Loss', linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Training Curve (Loss)')
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 2, 2)
    plt.plot(val_accs, label='Val Accuracy', linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title('Validation Accuracy')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # è¯„ä¼°
    y_pred = trainer.predict(X_test)
    test_acc = accuracy_score(y_test, y_pred)
    print(f"\nTest Accuracy: {test_acc:.3f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # è·å–ä¸­é—´å±‚ç‰¹å¾ï¼ˆç‰¹å¾æå–ï¼‰
    print("\n===== ç‰¹å¾æå–ç¤ºä¾‹ =====")
    # åˆ é™¤è¾“å‡ºå±‚ï¼Œè·å–å€’æ•°ç¬¬äºŒå±‚ç‰¹å¾
    feature_extractor = nn.Sequential(*list(model.network.children())[:-1])
    feature_extractor.to(device)
    feature_extractor.eval()

    with torch.no_grad():
        X_test_tensor = torch.FloatTensor(X_test).to(device)
        hidden_features = feature_extractor(X_test_tensor)
    print(f"Hidden features shape: {hidden_features.shape}")  # (n_samples, 32)
```

---

## ğŸ”§ è¶…å‚æ•°è°ƒä¼˜

### å…³é”®å‚æ•°è¯¦è§£

| å‚æ•° | å«ä¹‰ | å¯¹å†³ç­–è¾¹ç•Œçš„å½±å“ | æ¨èèŒƒå›´ |
|---|---|---|---|
| **éšå±‚æ•° (Depth)** | ç½‘ç»œçš„æ·±åº¦ | æ›´æ·± â†’ å¯å­¦ä¹ æ›´å¤æ‚çš„åˆ†å±‚ç‰¹å¾ï¼Œä½†è®­ç»ƒéš¾ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰ã€‚æ ¹æ®æ•°æ®å¤æ‚åº¦é€‰æ‹© | 2-5 å±‚ï¼ˆé€šå¸¸ 2-3 è¶³å¤Ÿï¼‰ |
| **éšå±‚å®½åº¦ (Width)** | æ¯å±‚ç¥ç»å…ƒæ•° | æ›´å®½ â†’ æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ï¼ˆæ¯å±‚ç‰¹å¾æ›´ä¸°å¯Œï¼‰ï¼Œä½†å‚æ•°å¢å¤šï¼Œè¿‡æ‹Ÿåˆé£é™©å¤§ | 64-512ï¼ˆæ ¹æ®è¾“å…¥ç»´åº¦è°ƒæ•´ï¼‰ |
| **learning_rate** | æ¢¯åº¦ä¸‹é™çš„æ­¥é•¿ | è¿‡å¤§ â†’ éœ‡è¡ä¸æ”¶æ•›ï¼›è¿‡å° â†’ æ”¶æ•›æ…¢ï¼Œé™·å…¥å±€éƒ¨æœ€å°å€¼ | 0.001-0.1ï¼ˆé€šå¸¸ 0.01ï¼‰ |
| **batch_size** | Mini-batch å¤§å° | å° â†’ æ¢¯åº¦å™ªå£°å¤§ï¼Œéœ‡è¡æ›´æ–°ä½†å¯èƒ½è·³å‡ºå±€éƒ¨æœ€å°å€¼ï¼›å¤§ â†’ å¹³ç¨³ä½†å¯èƒ½é™·å…¥å°–é”æœ€å°å€¼ | 16-128 |
| **activation (éšå±‚)** | æ¿€æ´»å‡½æ•° | **å…³é”®ï¼** ReLU â†’ å¿«é€Ÿæ”¶æ•›ã€é¿å…æ¢¯åº¦æ¶ˆå¤±ï¼›sigmoid/tanh â†’ æ¢¯åº¦æ¶ˆå¤±ã€è®­ç»ƒæ…¢ | ReLU æˆ– Leaky ReLU âœ… |
| **dropout_rate** | Dropout æ¯”ä¾‹ | å¢å¤§ â†’ æ­£åˆ™åŒ–å¼ºï¼Œé˜²è¿‡æ‹Ÿåˆä½†å¯èƒ½æ¬ æ‹Ÿåˆï¼›å‡å° â†’ è¿‡æ‹Ÿåˆé£é™© | 0.2-0.5 |
| **epochs** | è®­ç»ƒè½®æ•° | æ›´å¤š â†’ å¯èƒ½è¿‡æ‹Ÿåˆï¼›å¤ªå°‘ â†’ æ¬ æ‹Ÿåˆ | Early stoppingï¼ˆç›‘æµ‹éªŒè¯æŸå¤±ï¼‰ |

> [!TIP] learning_rate ä¸æ¢¯åº¦ä¸‹é™çš„åŠ¨æ€æ€§
>
> - **å›ºå®š lr**ï¼š$W \leftarrow W - \eta \nabla L$ï¼Œå®¹æ˜“åœ¨å¹³å¦åŒºåŸŸå¡ä½ï¼ˆå°æ¢¯åº¦ï¼‰æˆ–åœ¨é™¡å³­åŒºåŸŸéœ‡è¡
> - **è‡ªé€‚åº” lr**ï¼ˆAdam, RMSpropï¼‰ï¼š
>   - è®°å½•æ¢¯åº¦çš„ä¸€é˜¶çŸ©ï¼ˆåŠ¨é‡ï¼‰å’ŒäºŒé˜¶çŸ©ï¼ˆæ–¹å·®ï¼‰
>   - è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ï¼šå¡é™¡æ—¶é™é€Ÿï¼Œå¡å¹³æ—¶åŠ é€Ÿ
>   - å…¬å¼ï¼š$W \leftarrow W - \frac{\eta}{\sqrt{v + \epsilon}} m$ï¼ˆå…¶ä¸­ $m$ æ˜¯åŠ¨é‡ï¼Œ$v$ æ˜¯æ–¹å·®ï¼‰
> - **æ¨è**ï¼šä½¿ç”¨ Adamï¼ˆé»˜è®¤ $\eta=0.001$ï¼‰æˆ– RMSpropï¼Œè€Œéæœ´ç´  SGD

### è°ƒä¼˜å®è·µï¼ˆæ–¹æ³•1ï¼šç½‘æ ¼æœç´¢ï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import itertools

class HyperparameterGrid:
    """ç½‘æ ¼æœç´¢è¶…å‚æ•°"""

    def __init__(self, X_train, y_train, device='cpu'):
        self.X_train = torch.FloatTensor(X_train).to(device)
        self.y_train = torch.LongTensor(y_train).to(device)
        self.device = device
        self.results = []

    def build_model(self, hidden_size, dropout_rate):
        """æ„å»ºæ¨¡å‹"""
        model = nn.Sequential(
            nn.Linear(4, hidden_size),
            nn.BatchNorm1d(hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size, 3)
        )
        return model

    def train_and_evaluate(self, model, learning_rate, hidden_size, dropout_rate, epochs=50):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹å¹¶è¿”å›éªŒè¯ç²¾åº¦"""
        model = model.to(self.device)
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()

        # éªŒè¯é›†åˆ†å‰²
        val_size = int(len(self.X_train) * 0.2)
        indices = torch.randperm(len(self.X_train))
        train_indices = indices[val_size:]
        val_indices = indices[:val_size]

        X_train_split = self.X_train[train_indices]
        y_train_split = self.y_train[train_indices]
        X_val = self.X_train[val_indices]
        y_val = self.y_train[val_indices]

        train_loader = DataLoader(
            TensorDataset(X_train_split, y_train_split),
            batch_size=16,
            shuffle=True
        )

        best_val_acc = 0

        for epoch in range(epochs):
            model.train()
            for X_batch, y_batch in train_loader:
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            # éªŒè¯
            model.eval()
            with torch.no_grad():
                val_outputs = model(X_val)
                _, val_preds = torch.max(val_outputs, 1)
                val_acc = (val_preds == y_val).float().mean().item()
                best_val_acc = max(best_val_acc, val_acc)

        return best_val_acc

    def grid_search(self, param_grid, epochs=50):
        """æ‰§è¡Œç½‘æ ¼æœç´¢"""
        hidden_sizes = param_grid['hidden_size']
        dropout_rates = param_grid['dropout_rate']
        learning_rates = param_grid['learning_rate']

        total_trials = len(hidden_sizes) * len(dropout_rates) * len(learning_rates)
        trial = 0

        for hidden_size, dropout_rate, learning_rate in itertools.product(
            hidden_sizes, dropout_rates, learning_rates
        ):
            trial += 1
            print(f"Trial {trial}/{total_trials}: "
                  f"hidden={hidden_size}, dropout={dropout_rate:.2f}, lr={learning_rate:.1e}")

            model = self.build_model(hidden_size, dropout_rate)
            val_acc = self.train_and_evaluate(
                model, learning_rate, hidden_size, dropout_rate, epochs
            )

            self.results.append({
                'hidden_size': hidden_size,
                'dropout_rate': dropout_rate,
                'learning_rate': learning_rate,
                'val_accuracy': val_acc
            })

            print(f"  â†’ Val Accuracy: {val_acc:.4f}\n")

        # è¿”å›æœ€ä½³è¶…å‚æ•°
        best_result = max(self.results, key=lambda x: x['val_accuracy'])
        return best_result

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X, y = load_iris(return_X_y=True)
X = StandardScaler().fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# å®šä¹‰å‚æ•°ç½‘æ ¼
param_grid = {
    'hidden_size': [32, 64, 128],
    'dropout_rate': [0.2, 0.3, 0.4],
    'learning_rate': [1e-4, 1e-3, 1e-2]
}

searcher = HyperparameterGrid(X_train, y_train, device=device)
best_params = searcher.grid_search(param_grid, epochs=50)

print("\n===== æœ€ä½³è¶…å‚æ•° =====")
print(f"Hidden Size: {best_params['hidden_size']}")
print(f"Dropout Rate: {best_params['dropout_rate']}")
print(f"Learning Rate: {best_params['learning_rate']:.1e}")
print(f"Validation Accuracy: {best_params['val_accuracy']:.4f}")

# ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
final_model = nn.Sequential(
    nn.Linear(4, best_params['hidden_size']),
    nn.BatchNorm1d(best_params['hidden_size']),
    nn.ReLU(),
    nn.Dropout(best_params['dropout_rate']),
    nn.Linear(best_params['hidden_size'], 3)
)
trainer = AdvancedTrainer(final_model, learning_rate=best_params['learning_rate'], device=device)
```

### è°ƒä¼˜å®è·µï¼ˆæ–¹æ³•2ï¼šéšæœºæœç´¢ - æ›´é«˜æ•ˆï¼‰

```python
import random
from scipy.stats import loguniform

class RandomSearch:
    """éšæœºæœç´¢è¶…å‚æ•°ï¼ˆæ¯”ç½‘æ ¼æœç´¢æ›´é«˜æ•ˆï¼‰"""

    def __init__(self, X_train, y_train, device='cpu'):
        self.X_train = torch.FloatTensor(X_train).to(device)
        self.y_train = torch.LongTensor(y_train).to(device)
        self.device = device
        self.results = []

    def build_model(self, hidden_size, dropout_rate):
        model = nn.Sequential(
            nn.Linear(4, hidden_size),
            nn.BatchNorm1d(hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size, 3)
        )
        return model

    def train_and_evaluate(self, model, learning_rate, epochs=50):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        model = model.to(self.device)
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()

        val_size = int(len(self.X_train) * 0.2)
        indices = torch.randperm(len(self.X_train))
        train_indices = indices[val_size:]
        val_indices = indices[:val_size]

        X_train_split = self.X_train[train_indices]
        y_train_split = self.y_train[train_indices]
        X_val = self.X_train[val_indices]
        y_val = self.y_train[val_indices]

        train_loader = DataLoader(
            TensorDataset(X_train_split, y_train_split),
            batch_size=16,
            shuffle=True
        )

        best_val_acc = 0

        for epoch in range(epochs):
            model.train()
            for X_batch, y_batch in train_loader:
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            model.eval()
            with torch.no_grad():
                val_outputs = model(X_val)
                _, val_preds = torch.max(val_outputs, 1)
                val_acc = (val_preds == y_val).float().mean().item()
                best_val_acc = max(best_val_acc, val_acc)

        return best_val_acc

    def random_search(self, n_trials=20, epochs=50):
        """æ‰§è¡Œéšæœºæœç´¢"""
        best_result = None
        best_acc = 0

        for trial in range(n_trials):
            # éšæœºé‡‡æ ·è¶…å‚æ•°
            hidden_size = random.choice([32, 64, 128, 256, 512])
            dropout_rate = random.uniform(0.1, 0.5)
            learning_rate = float(loguniform.rvs(1e-4, 1e-2))

            print(f"Trial {trial+1}/{n_trials}: "
                  f"hidden={hidden_size}, dropout={dropout_rate:.2f}, lr={learning_rate:.1e}")

            model = self.build_model(hidden_size, dropout_rate)
            val_acc = self.train_and_evaluate(model, learning_rate, epochs)

            result = {
                'hidden_size': hidden_size,
                'dropout_rate': dropout_rate,
                'learning_rate': learning_rate,
                'val_accuracy': val_acc
            }
            self.results.append(result)

            if val_acc > best_acc:
                best_acc = val_acc
                best_result = result

            print(f"  â†’ Val Accuracy: {val_acc:.4f}\n")

        return best_result

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
searcher = RandomSearch(X_train, y_train, device=device)
best_params = searcher.random_search(n_trials=20, epochs=50)

print("\n===== æœ€ä½³è¶…å‚æ•°ï¼ˆéšæœºæœç´¢ï¼‰=====")
print(f"Hidden Size: {best_params['hidden_size']}")
print(f"Dropout Rate: {best_params['dropout_rate']:.4f}")
print(f"Learning Rate: {best_params['learning_rate']:.1e}")
print(f"Validation Accuracy: {best_params['val_accuracy']:.4f}")
```

> [!WARNING] å¸¸è§é™·é˜±
>
> 1. **learning_rate è¿‡å¤§æˆ–è¿‡å°**ï¼šå­¦ä¹ ç‡æ˜¯æœ€æ•æ„Ÿçš„è¶…å‚æ•°ã€‚ç›‘æµ‹è®­ç»ƒæŸå¤±æ›²çº¿ï¼Œåº”å¹³ç¨³ä¸‹é™è€Œééœ‡è¡æˆ–åœæ»
> 2. **éšå±‚æ•°è¿‡æ·±ä½†æœªç”¨ BatchNorm**ï¼šæ·±ç½‘ç»œæ˜“æ¢¯åº¦æ¶ˆå¤±ï¼Œç”¨ BatchNormalization ç¼“è§£ã€‚ä¸ç”¨ä¼šå¡åœ¨æ¬ æ‹Ÿåˆ
> 3. **æ¿€æ´»å‡½æ•°ç”¨ sigmoid**ï¼šåœ¨éšå±‚ä½¿ç”¨ sigmoid ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼ˆå¯¼æ•°æœ€å¤§ 0.25ï¼‰ï¼Œæ”¹ç”¨ ReLU
> 4. **Dropout æ¯”ä¾‹è¿‡é«˜**ï¼šå¤ªé«˜çš„ dropoutï¼ˆå¦‚ 0.8ï¼‰ä¼šè®©ç½‘ç»œå®Œå…¨éšæœºåŒ–ï¼Œæ— æ³•å­¦ä¹ 
> 5. **ä¸ç”¨éªŒè¯é›†ç›‘æµ‹**ï¼šå®¹æ˜“ä¸¥é‡è¿‡æ‹Ÿåˆã€‚éœ€è¦è®¾ `validation_split` æˆ– early stopping

---

## âš–ï¸ ä¼˜ç¼ºç‚¹ä¸åœºæ™¯

### âœ… ä¼˜åŠ¿ (Pros)

1. **é€šç”¨è¿‘ä¼¼å™¨**ï¼šè¶³å¤Ÿå®½çš„å•éšå±‚å¯è¿‘ä¼¼ä»»æ„è¿ç»­å‡½æ•°ï¼ˆUniversal Approximation Theoremï¼‰
2. **è‡ªåŠ¨ç‰¹å¾å­¦ä¹ **ï¼šæ— éœ€æ‰‹å·¥ç‰¹å¾å·¥ç¨‹ï¼Œç½‘ç»œè‡ªåŠ¨å­¦ä¹ åˆ†å±‚ç‰¹å¾
3. **éçº¿æ€§è¡¨è¾¾èƒ½åŠ›å¼º**ï¼šå¯æ•æ‰å¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œè¿œä¼˜äºçº¿æ€§æ¨¡å‹
4. **ç«¯åˆ°ç«¯å¯å¾®**ï¼šåå‘ä¼ æ’­å…è®¸ä¼˜åŒ–ä»»æ„å¯å¾®çš„æŸå¤±å‡½æ•°
5. **å¹¶è¡Œè®¡ç®—å‹å¥½**ï¼šçŸ©é˜µè¿ç®—æ˜“äº GPU åŠ é€Ÿ

### âŒ åŠ£åŠ¿ (Cons)

1. **å¯è§£é‡Šæ€§å·®**ï¼šéšå±‚ç‰¹å¾æ— æ˜ç¡®å«ä¹‰ï¼Œéš¾ä»¥ç†è§£ç½‘ç»œçš„å†³ç­–é€»è¾‘
2. **è®­ç»ƒå¤æ‚**ï¼šè¶…å‚æ•°ä¼—å¤šï¼ˆæ·±åº¦ã€å®½åº¦ã€lrã€batch_size ç­‰ï¼‰ï¼Œè°ƒå‚å›°éš¾ï¼›æ˜“é™·å…¥å±€éƒ¨æœ€å°å€¼
3. **å®¹æ˜“è¿‡æ‹Ÿåˆ**ï¼šå‚æ•°å¤šï¼Œè‹¥ç¼ºä¹æ­£åˆ™åŒ–æ˜“ä¸¥é‡è¿‡æ‹Ÿåˆ
4. **éœ€è¦å¤§é‡æ•°æ®**ï¼šç›¸æ¯”æ ‘æ¨¡å‹ï¼ŒANN éœ€è¦æ›´å¤šæ ·æœ¬æ‰èƒ½æ³›åŒ–
5. **æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸**ï¼šæ·±ç½‘ç»œè®­ç»ƒä¸ç¨³å®šï¼ˆè™½ç„¶ ReLU ç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±ï¼‰
6. **è®­ç»ƒæ—¶é—´é•¿**ï¼šéœ€è¦å¤šä¸ª epochï¼Œå•æ¬¡åå‘ä¼ æ’­è®¡ç®—é‡å¤§

### ğŸ¯ é€‚ç”¨åœºæ™¯

| åœºæ™¯ | é€‚ç”¨åº¦ | åŸå›  |
|---|---|---|
| å›¾åƒè¯†åˆ«ï¼ˆCNNï¼‰ | â­â­â­â­â­ | å·ç§¯åˆ©ç”¨ç©ºé—´å±€éƒ¨æ€§ï¼Œå‚æ•°å…±äº«ï¼›æ·±å±‚å †å æ•æ‰å¤šå°ºåº¦ç‰¹å¾ |
| NLPï¼ˆåºåˆ—æ¨¡å‹ï¼‰ | â­â­â­â­â­ | Transformerã€LSTM æ˜¯åºåˆ—å»ºæ¨¡çš„æ ‡å‡†æ–¹æ¡ˆ |
| éçº¿æ€§åˆ†ç±»ï¼ˆä¸­ç­‰è§„æ¨¡ï¼‰ | â­â­â­â­ | ä¼˜äº SVMï¼Œæ¯”éšæœºæ£®æ—æ›´çµæ´» |
| å›å½’ï¼ˆè¿ç»­å€¼é¢„æµ‹ï¼‰ | â­â­â­â­ | å¯ç”¨ï¼Œæ•ˆæœä¸æ ‘æ¨¡å‹æ¥è¿‘ |
| å°æ•°æ®é›†ï¼ˆ<10Kï¼‰ | â­â­ | æ˜“è¿‡æ‹Ÿåˆï¼Œä¸å¦‚æ ‘æ¨¡å‹ç¨³å®šï¼›éœ€è¦æ­£åˆ™åŒ–æˆ–é¢„è®­ç»ƒ |
| ç»“æ„åŒ–è¡¨æ ¼æ•°æ®ï¼ˆå¤§ï¼‰| â­â­â­ | å¯ç”¨ï¼Œä½†é€šå¸¸ä¸å¦‚ XGBoostï¼›é™¤ééœ€è¦ç‰¹æ®Šçš„ç«¯åˆ°ç«¯å­¦ä¹  |
| å¼‚æ„æ•°æ®èåˆ | â­â­â­â­â­ | ANN è‡ªç„¶å¤„ç†å¤šæ¨¡æ€æ•°æ®ï¼Œå¯èåˆå›¾åƒ+æ–‡æœ¬+ç»“æ„åŒ–ç‰¹å¾ |

---

## ğŸ’¬ é¢è¯•å¿…é—®

> [!question] Q1: æ¨å¯¼åå‘ä¼ æ’­çš„é“¾å¼æ³•åˆ™ï¼Œä¸ºä»€ä¹ˆæ¢¯åº¦æ¶ˆå¤±åœ¨æ·±ç½‘ç»œä¸­æ˜¯å…³é”®é—®é¢˜ï¼Ÿ
>
> **ç­”æ¡ˆæ¡†æ¶**ï¼š
>
> **é“¾å¼æ³•åˆ™æ¨å¯¼**ï¼š
>
> å¯¹äº $L$ å±‚ç½‘ç»œï¼Œè®¡ç®— $\frac{\partial L}{\partial W^{(1)}}$ æ¶‰åŠä»è¾“å‡ºå±‚é€†å‘é“¾å¼ç›¸ä¹˜ï¼š
>
> $$\frac{\partial L}{\partial W^{(1)}} = \frac{\partial L}{\partial z^{(L)}} \cdot \frac{\partial z^{(L)}}{\partial a^{(L-1)}} \cdot \frac{\partial a^{(L-1)}}{\partial z^{(L-1)}} \cdots \frac{\partial z^{(2)}}{\partial a^{(1)}} \cdot \frac{\partial a^{(1)}}{\partial z^{(1)}} \cdot \frac{\partial z^{(1)}}{\partial W^{(1)}}$$
>
> **æ¢¯åº¦æ¶ˆå¤±åˆ†æ**ï¼š
>
> æ¯ä¸€é¡¹åŒ…å«æ¿€æ´»å‡½æ•°å¯¼æ•° $\sigma'(z^{(l)})$ã€‚å¯¹ sigmoidï¼š
> - $\sigma'(z) = \sigma(z)(1-\sigma(z)) \leq 0.25$
> - é“¾å¼ä¹˜ç§¯ï¼š$\prod_{l=1}^{L-1} \sigma'(z^{(l)}) \leq 0.25^{L-1}$
> - å½“ $L$ å¾ˆå¤§æ—¶ï¼Œå¦‚ $L=10$ï¼Œä¹˜ç§¯ $\leq 0.25^9 \approx 10^{-6}$ï¼Œæ¢¯åº¦æ¥è¿‘ 0
> - ç¬¬ 1 å±‚çš„æƒé‡å‡ ä¹æ— æ³•æ›´æ–°ï¼Œç½‘ç»œæ— æ³•å­¦ä¹ æ—©æœŸç‰¹å¾
>
> **ReLU çš„æ•‘èµ**ï¼š
> - ReLU å¯¼æ•°ï¼š$\text{ReLU}'(z) = 1$ï¼ˆå½“ $z > 0$ï¼‰
> - é“¾å¼ä¹˜ç§¯ï¼š$\prod_{l} \text{ReLU}'(z^{(l)}) = 1$ï¼Œæ¢¯åº¦ä¸è¡°å‡
> - è™½ç„¶æœ‰ dead ReLU é—®é¢˜ï¼ˆ$z < 0$ æ—¶å¯¼æ•°ä¸º 0ï¼‰ï¼Œä½† Leaky ReLU æˆ– ELU æ”¹è¿›

> [!question] Q2: å¦‚ä½•åŒºåˆ†è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆï¼ŸANN ä¸­çš„æ­£åˆ™åŒ–æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ
>
> **ç­”æ¡ˆæ ¸å¿ƒ**ï¼š
>
> **è¯Šæ–­æ–¹æ³•**ï¼š
> - **è¿‡æ‹Ÿåˆ**ï¼šè®­ç»ƒæŸå¤± â†’ 0ï¼ŒéªŒè¯æŸå¤± â†’ é«˜ã€‚ç‰¹å¾ï¼šè®­ç»ƒå‡†ç¡®åº¦é«˜ï¼Œæµ‹è¯•ä½
> - **æ¬ æ‹Ÿåˆ**ï¼šè®­ç»ƒæŸå¤±ä»å¾ˆé«˜ï¼ŒéªŒè¯æŸå¤±ä¹Ÿé«˜ï¼Œå·®è·ä¸å¤§
> - **å·¥å…·**ï¼šç»˜åˆ¶ loss æ›²çº¿ï¼ˆepochs vs train_loss/val_lossï¼‰
>
> **æ­£åˆ™åŒ–æŠ€æœ¯å¯¹æ¯”**ï¼š
>
> | æ–¹æ³• | åŸç† | ä½•æ—¶ç”¨ | æ•ˆæœ |
> |---|---|---|---|
> | **L2 æ­£åˆ™åŒ–** | $L' = L + \lambda \sum W^2$ï¼Œæƒ©ç½šå¤§æƒé‡ | æ€»æ˜¯æ¨è | è½»åº¦è¿‡æ‹Ÿåˆï¼Œç¨³å®š |
> | **L1 æ­£åˆ™åŒ–** | $L' = L + \lambda \sum \|W\|$ï¼Œäº§ç”Ÿç¨€ç–æƒé‡ | éœ€è¦ç‰¹å¾é€‰æ‹© | ä¸­åº¦è¿‡æ‹Ÿåˆ |
> | **Dropout** | éšæœºå…³é—­ç¥ç»å…ƒï¼Œå¼ºåˆ¶å†—ä½™å­¦ä¹  | ä¸­å¤§å‹ç½‘ç»œï¼ˆ>100 å‚æ•°ï¼‰ | ä¸­-é‡åº¦è¿‡æ‹Ÿåˆ |
> | **Early Stopping** | ç›‘æµ‹éªŒè¯æŸå¤±ï¼ŒéªŒè¯ä¸é™æ—¶åœæ­¢ | æ€»æ˜¯ç”¨ | ç®€å•æœ‰æ•ˆ |
> | **BatchNormalization** | æ¯å±‚å½’ä¸€åŒ–ï¼Œç¨³å®šè®­ç»ƒ | æ·±ç½‘ç»œï¼ˆ>3 å±‚ï¼‰ | åŠ é€Ÿæ”¶æ•›+æ­£åˆ™åŒ–æ•ˆæœ |
> | **æ•°æ®å¢å¼º** | æ‰©å±•è®­ç»ƒæ•°æ® | å°æ•°æ®é›† | å®è´¨æ€§æ”¹è¿› |
>
> **æ¨èç»„åˆ**ï¼šL2 æ­£åˆ™åŒ– + Dropout + Early Stopping + BatchNorm

> [!question] Q3: è§£é‡Šæ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatchNormalizationï¼‰çš„ä½œç”¨ï¼Œä¸ºä»€ä¹ˆå®ƒæ—¢åŠ é€Ÿè®­ç»ƒåˆé˜²è¿‡æ‹Ÿåˆï¼Ÿ
>
> **ç­”æ¡ˆæ ¸å¿ƒ**ï¼š
>
> **å…¬å¼**ï¼š
>
> $$\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$
> $$y = \gamma \hat{x} + \beta$$
>
> å…¶ä¸­ï¼š
> - $\mu_B, \sigma_B$ï¼šmini-batch çš„å‡å€¼å’Œæ–¹å·®
> - $\gamma, \beta$ï¼šå¯å­¦ä¹ çš„ç¼©æ”¾å’Œåç§»å‚æ•°
>
> **åŠ é€Ÿè®­ç»ƒçš„åŸå› **ï¼š
> 1. **è§£å†³ Internal Covariate Shift**ï¼šæ·±ç½‘ç»œä¸­ï¼Œæ¯å±‚è¾“å…¥åˆ†å¸ƒéšå‚æ•°æ›´æ–°è€Œå˜åŒ–ï¼ˆè¾“å…¥æ—¶ä¸ç¨³å®šï¼‰ï¼ŒBN ç¨³å®šåˆ†å¸ƒ
> 2. **å¢å¤§å­¦ä¹ ç‡ä¸Šç•Œ**ï¼šåˆ†å¸ƒç¨³å®šï¼Œæ¢¯åº¦çˆ†ç‚¸é£é™©é™ä½ï¼Œå¯ç”¨æ›´å¤§çš„ lr
> 3. **å‡å°‘å¯¹åˆå§‹åŒ–çš„æ•æ„Ÿæ€§**ï¼šBN ä½¿ç½‘ç»œå¯¹åˆå§‹åŒ–é²æ£’
>
> **é˜²è¿‡æ‹Ÿåˆçš„åŸå› **ï¼š
> 1. **æ­£åˆ™åŒ–æ•ˆæœ**ï¼šä½¿ç”¨ mini-batch ç»Ÿè®¡è€Œéå…¨å±€ç»Ÿè®¡ï¼Œå¼•å…¥å™ªå£°ï¼ˆç±»ä¼¼ Dropoutï¼‰
> 2. **ç®€åŒ–ä¼˜åŒ–æ™¯è§‚**ï¼šä½¿æŸå¤±æ›²é¢æ›´å¹³æ»‘ï¼Œé™·å…¥å°–é”æœ€å°å€¼ï¼ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰çš„æ¦‚ç‡é™ä½
>
> **å®æˆ˜å»ºè®®**ï¼š
> - æ€»æ˜¯åœ¨éšå±‚åŠ  BNï¼ˆåœ¨æ¿€æ´»å‡½æ•°å‰ï¼‰ï¼š`Dense â†’ BatchNorm â†’ ReLU`
> - æ³¨æ„ï¼šBN çš„å‚æ•°åŒ–æ”¹å˜äº†åç»­å±‚çš„è¾“å…¥ï¼Œå¯èƒ½å½±å“æƒé‡åˆå§‹åŒ–ç­–ç•¥